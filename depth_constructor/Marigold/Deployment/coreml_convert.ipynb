{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "525b27ca",
   "metadata": {},
   "source": [
    "# COREML CONVERT - this is for taking pretrained QAT models/F32 variants and converting them to CoreML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619868c3",
   "metadata": {},
   "source": [
    "## 1. Start with model loading and set the ENV paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "626ecf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export dir: /datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml_192\n",
      "QAT sd: /datasets/abradshaw/output_QAT/train_marigold_depth_qat/checkpoint/latest/unet_qat_state_dict.pt\n",
      "Base (VAE/config): /datasets/abradshaw/mgold_ckpts/stable-diffusion-2\n"
     ]
    }
   ],
   "source": [
    "# 1.1) Environment & Paths\n",
    "\n",
    "import os, torch, warnings\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "\n",
    "# EDIT THESE THREE\n",
    "PATH_RUN = \"/datasets/abradshaw/output_QAT/train_marigold_depth_qat\"\n",
    "PATH_QAT_SD = f\"{PATH_RUN}/checkpoint/latest/unet_qat_state_dict.pt\"  # QAT UNet weights (with 8-ch conv_in)\n",
    "PATH_PRETRAINED_LOCAL = \"/datasets/abradshaw/mgold_ckpts/stable-diffusion-2\"  # local base (for VAE + UNet config)\n",
    "\n",
    "PATH_EXPORT = os.path.join(PATH_RUN, \"export_coreml_192\")\n",
    "os.makedirs(PATH_EXPORT, exist_ok=True)\n",
    "\n",
    "assert os.path.isfile(PATH_QAT_SD), f\"Missing QAT state-dict: {PATH_QAT_SD}\"\n",
    "print(\"Export dir:\", PATH_EXPORT)\n",
    "print(\"QAT sd:\", PATH_QAT_SD)\n",
    "print(\"Base (VAE/config):\", PATH_PRETRAINED_LOCAL)\n",
    "\n",
    "warnings.filterwarnings(\"once\")\n",
    "torch.set_grad_enabled(False)\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7906ba",
   "metadata": {},
   "source": [
    "## 2. Load UNet config only, force 8 input channels like the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923dec41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNet load] missing=0 unexpected=3087\n",
      "  (expected: observer/fake-quant keys show up here and are ignored)\n",
      "UNet forward OK → (1, 4, 64, 64)\n",
      "VAE loaded.\n",
      "fixed_embed loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- 2.1 Load UNet config only, force 8 input channels ---\n",
    "unet_cfg = UNet2DConditionModel.load_config(os.path.join(PATH_PRETRAINED_LOCAL, \"unet\"))\n",
    "unet_cfg[\"in_channels\"] = 8  # we trained on [rgb_latent(4) + target_latent(4)]\n",
    "unet = UNet2DConditionModel.from_config(unet_cfg).to(device)  # empty FP32 skeleton\n",
    "\n",
    "# --- 2.2 Sanitize activations to match QAT training (GELU->ReLU, SiLU->Hardswish) ---\n",
    "_BAD2GOOD = {\n",
    "    nn.GELU: lambda _: nn.ReLU(inplace=False),\n",
    "    nn.SiLU: lambda _: nn.Hardswish(),\n",
    "}\n",
    "_BAD_FUNCS = {F.gelu, F.silu}\n",
    "def _relu(x): return F.relu(x, inplace=False)\n",
    "\n",
    "def sanitize_only(module: nn.Module):\n",
    "    for name, child in list(module.named_children()):\n",
    "        replaced = False\n",
    "        for bad_cls, make_good in _BAD2GOOD.items():\n",
    "            if isinstance(child, bad_cls):\n",
    "                setattr(module, name, make_good(child))\n",
    "                replaced = True\n",
    "                break\n",
    "        sanitize_only(getattr(module, name) if replaced else child)\n",
    "    for attr_name, attr_val in vars(module).items():\n",
    "        if callable(attr_val) and attr_val in _BAD_FUNCS:\n",
    "            setattr(module, attr_name, _relu)\n",
    "\n",
    "sanitize_only(unet)\n",
    "\n",
    "# Disable memory-efficient attention (PyTorch export prefers standard ops)\n",
    "try:\n",
    "    unet.disable_xformers_memory_efficient_attention()\n",
    "except Exception:\n",
    "    pass\n",
    "unet.eval()\n",
    "\n",
    "# --- 2.3 Load QAT state-dict; observers/fake-quant keys are ignored by strict=False ---\n",
    "sd = torch.load(PATH_QAT_SD, map_location=\"cpu\", weights_only=True)\n",
    "missing, unexpected = unet.load_state_dict(sd, strict=False)\n",
    "print(f\"[UNet load] missing={len(missing)} unexpected={len(unexpected)}\")\n",
    "if unexpected:\n",
    "    print(\"  (expected: observer/fake-quant keys show up here and are ignored)\")\n",
    "\n",
    "# quick forward sanity\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(1, 8, 64, 64)\n",
    "    t = torch.tensor([0])\n",
    "    cond = torch.randn(1, 77, 1024)\n",
    "    y = unet(x, t, encoder_hidden_states=cond).sample\n",
    "    print(\"UNet forward OK →\", tuple(y.shape))\n",
    "\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(PATH_PRETRAINED_LOCAL, subfolder=\"vae\").to(device).eval()\n",
    "print(\"VAE loaded.\")\n",
    "fixed_embed = torch.randn(1, 77, 1024, dtype=torch.float32)\n",
    "print(\"fixed_embed loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96034b56",
   "metadata": {},
   "source": [
    "## 3.  Export wrappers (Encoder / UNetStep / Decoder) this way we can pass in F32 inouts ot the model and convert each wrapper separately instead of a monolithic pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d93c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrappers prepared.\n"
     ]
    }
   ],
   "source": [
    "# 3.1) Export wrappers (Encoder / UNetStep / Decoder)\n",
    "import torch.nn as nn\n",
    "\n",
    "LATENT_SF = 0.18215\n",
    "\n",
    "class EncoderWrapper(nn.Module):\n",
    "    def __init__(self, vae: AutoencoderKL):\n",
    "        super().__init__(); self.vae = vae\n",
    "    def forward(self, rgb_norm: torch.Tensor) -> torch.Tensor:\n",
    "        # [-1,1]-normalized RGB → latent(4,H/8,W/8)\n",
    "        h = self.vae.encoder(rgb_norm)\n",
    "        mean, logvar = torch.chunk(self.vae.quant_conv(h), 2, dim=1)\n",
    "        return mean * LATENT_SF\n",
    "\n",
    "class UNetStepWrapper(nn.Module):\n",
    "    def __init__(self, unet: UNet2DConditionModel, fixed_embed: torch.Tensor):\n",
    "        super().__init__(); self.unet = unet\n",
    "        self.register_buffer(\"fixed_embed\", fixed_embed, persistent=False)\n",
    "    def forward(self, rgb_latent: torch.Tensor, target_latent: torch.Tensor, t_f32: torch.Tensor) -> torch.Tensor:\n",
    "        # concat latents; convert float timestep → int64\n",
    "        x = torch.cat([rgb_latent, target_latent], dim=1)\n",
    "        t = t_f32.to(torch.int64).reshape(-1)\n",
    "        cond = self.fixed_embed.expand(x.shape[0], -1, -1)\n",
    "        return self.unet(x, t, encoder_hidden_states=cond).sample\n",
    "\n",
    "class DecoderWrapper(nn.Module):\n",
    "    def __init__(self, vae: AutoencoderKL):\n",
    "        super().__init__(); self.vae = vae\n",
    "    def forward(self, depth_latent: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.vae.post_quant_conv(depth_latent / LATENT_SF)\n",
    "        stacked = self.vae.decoder(z)\n",
    "        return stacked.mean(dim=1, keepdim=True)  # [B,1,H,W]\n",
    "\n",
    "enc = EncoderWrapper(vae).eval()\n",
    "step = UNetStepWrapper(unet, fixed_embed).eval()\n",
    "dec  = DecoderWrapper(vae).eval()\n",
    "print(\"Wrappers prepared.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdf541e",
   "metadata": {},
   "source": [
    "## 4. Save each torch script, fall back to a trace if there is conditional logic that scirpt deos not support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27938204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TS] script failed for /datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml_192/Encoder.ts: function definitions aren't supported:\n",
      "  File \"/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py\", line 181\n",
      "        processors = {}\n",
      "    \n",
      "        def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):\n",
      "        ~~~ <--- HERE\n",
      "            if hasattr(module, \"get_processor\"):\n",
      "                processors[f\"{name}.processor\"] = module.get_processor()\n",
      "\n",
      "→ tracing instead.\n",
      "[TS] saved: /datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml_192/Encoder.ts\n",
      "[TS] script failed for /datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml_192/UNetStep.ts: keyword-arg expansion is not supported:\n",
      "  File \"/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py\", line 1173\n",
      "            cross_attention_kwargs = cross_attention_kwargs.copy()\n",
      "            gligen_args = cross_attention_kwargs.pop(\"gligen\")\n",
      "            cross_attention_kwargs[\"gligen\"] = {\"objs\": self.position_net(**gligen_args)}\n",
      "                                                                            ~~~~~~~~~~~ <--- HERE\n",
      "    \n",
      "        # 3. down\n",
      "\n",
      "→ tracing instead.\n",
      "[TS] saved: /datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml_192/UNetStep.ts\n",
      "[TS] script failed for /datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml_192/Decoder.ts: function definitions aren't supported:\n",
      "  File \"/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py\", line 181\n",
      "        processors = {}\n",
      "    \n",
      "        def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):\n",
      "        ~~~ <--- HERE\n",
      "            if hasattr(module, \"get_processor\"):\n",
      "                processors[f\"{name}.processor\"] = module.get_processor()\n",
      "\n",
      "→ tracing instead.\n",
      "[TS] saved: /datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml_192/Decoder.ts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# ablations\n",
    "#B, H, W = 1, 512, 512\n",
    "#B, H, W = 1, 320, 320\n",
    "#B, H, W = 1, 256, 256\n",
    "B, H, W = 1, 256, 192\n",
    "h, w = H//8, W//8\n",
    "\n",
    "ex_rgb = torch.randn(B,3,H,W, dtype=torch.float32)\n",
    "ex_lat = torch.randn(B,4,h,w, dtype=torch.float32)\n",
    "ex_t   = torch.tensor([0.0], dtype=torch.float32)\n",
    "\n",
    "def save_ts(m, ex, path):\n",
    "    try:\n",
    "        ts = torch.jit.script(m)\n",
    "    except Exception as e:\n",
    "        print(f\"[TS] script failed for {path}: {e}\\n→ tracing instead.\")\n",
    "        ts = torch.jit.trace(m, ex)\n",
    "    ts.save(path)\n",
    "    print(\"[TS] saved:\", path)\n",
    "    return ts\n",
    "\n",
    "enc_ts  = save_ts(enc,  (ex_rgb,),              os.path.join(PATH_EXPORT, \"Encoder.ts\"))\n",
    "step_ts = save_ts(step, (ex_lat,ex_lat,ex_t),   os.path.join(PATH_EXPORT, \"UNetStep.ts\"))\n",
    "dec_ts  = save_ts(dec,  (ex_lat,),              os.path.join(PATH_EXPORT, \"Decoder.ts\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbad225",
   "metadata": {},
   "source": [
    "## 5. Convert the torch scripts into CoreML packages straight from pytorch scurpts to corenk (ONNX supporrted later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c1f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/coremltools/_deps/__init__.py:65: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return _StrictVersion(version)\n",
      "scikit-learn version 1.7.0 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
      "Torch version 2.4.1+cu121 has not been tested with coremltools. You may run into unexpected errors. Torch 2.2.0 is the most recent version that has been tested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
      "Converting PyTorch Frontend ==> MIL Ops:   0%|          | 0/301 [00:00<?, ? ops/s]/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/coremltools/converters/mil/frontend/torch/ops.py:2185: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  res = mb.const(val=dtype(x.val), name=node.name)\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 300/301 [00:00<00:00, 4281.72 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 187.53 passes/s]\n",
      "Running MIL default pipeline:   0%|          | 0/78 [00:00<?, ? passes/s]/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:266: UserWarning: Output, '506', of the source model, has been renamed to 'var_506' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 78/78 [00:03<00:00, 24.07 passes/s] \n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 240.03 passes/s]\n",
      "/usr/lib/python3.10/tempfile.py:1008: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpi7k0jnrs'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n",
      "Converting PyTorch Frontend ==> MIL Ops:   0%|          | 0/2412 [00:00<?, ? ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 2411/2412 [00:00<00:00, 2994.41 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 32.94 passes/s]\n",
      "Running MIL default pipeline:  10%|█         | 8/78 [00:00<00:02, 31.32 passes/s]/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:266: UserWarning: Output, '3787', of the source model, has been renamed to 'var_3787' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 78/78 [00:30<00:00,  2.57 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 40.73 passes/s]\n",
      "/usr/lib/python3.10/tempfile.py:1008: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp47qyy99k'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 377/378 [00:00<00:00, 4019.52 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 128.62 passes/s]\n",
      "Running MIL default pipeline:   0%|          | 0/78 [00:00<?, ? passes/s]/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:266: UserWarning: Output, '638', of the source model, has been renamed to 'var_638' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 78/78 [00:02<00:00, 33.74 passes/s] \n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 158.98 passes/s]\n",
      "/usr/lib/python3.10/tempfile.py:1008: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpwr42_3e9'>\n",
      "  _warnings.warn(warn_message, ResourceWarning)\n"
     ]
    }
   ],
   "source": [
    "# TorchScript → Core ML only (no ONNX fallback)\n",
    "\n",
    "import coremltools as ct\n",
    "import numpy as np \n",
    "\n",
    "enc_ml = ct.convert(\n",
    "    enc_ts,\n",
    "    convert_to=\"mlprogram\",\n",
    "    inputs=[ct.TensorType(name=\"rgb_norm\", shape=ex_rgb.shape, dtype=np.float16)], # dtype 16 is only supported for iOS 16+\n",
    "    compute_precision=ct.precision.FLOAT16, # lowest precision supported in coreML execp if you do post traingn qunaitixaiotn which is not a targert precisons supported for int8\n",
    "    minimum_deployment_target=ct.target.iOS16,\n",
    ")\n",
    "enc_ml.save(os.path.join(PATH_EXPORT, \"Encoder.mlpackage\"))\n",
    "\n",
    "step_ml = ct.convert(\n",
    "    step_ts,\n",
    "    convert_to=\"mlprogram\",\n",
    "    inputs=[\n",
    "        ct.TensorType(name=\"rgb_latent\",    shape=ex_lat.shape, dtype=np.float16),\n",
    "        ct.TensorType(name=\"target_latent\", shape=ex_lat.shape, dtype=np.float16),\n",
    "        ct.TensorType(name=\"t_f32\",         shape=(1,), dtype=np.float32),\n",
    "    ],\n",
    "    compute_precision=ct.precision.FLOAT16,\n",
    "    minimum_deployment_target=ct.target.iOS16,\n",
    ")\n",
    "step_ml.save(os.path.join(PATH_EXPORT, \"UNetStep.mlpackage\"))\n",
    "\n",
    "dec_ml = ct.convert(\n",
    "    dec_ts,\n",
    "    convert_to=\"mlprogram\",\n",
    "    inputs=[ct.TensorType(name=\"depth_latent\", shape=ex_lat.shape, dtype=np.float16)],\n",
    "    compute_precision=ct.precision.FLOAT16,\n",
    "    minimum_deployment_target=ct.target.iOS16,\n",
    ")\n",
    "dec_ml.save(os.path.join(PATH_EXPORT, \"Decoder.mlpackage\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8add4",
   "metadata": {},
   "source": [
    "## 5. Convert to CoreML with ONNX catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core ML conversion (ML Program, FP16) + ONNX fallback\n",
    "import coremltools as ct\n",
    "\n",
    "def to_coreml_from_ts(ts_module, inputs, out_name):\n",
    "    mlmodel = ct.convert(\n",
    "        ts_module,\n",
    "        convert_to=\"mlprogram\",\n",
    "        inputs=inputs,\n",
    "        compute_precision=ct.precision.FLOAT16,\n",
    "        minimum_deployment_target=ct.target.iOS15,\n",
    "    )\n",
    "    out_path = os.path.join(PATH_EXPORT, out_name)\n",
    "    mlmodel.save(out_path)\n",
    "    print(\"[CoreML] saved:\", out_path)\n",
    "    return out_path\n",
    "\n",
    "def to_coreml_from_onnx(onnx_path, inputs, out_name):\n",
    "    mlmodel = ct.convert(\n",
    "        onnx_path,\n",
    "        convert_to=\"mlprogram\",\n",
    "        inputs=inputs,\n",
    "        compute_precision=ct.precision.FLOAT16,\n",
    "        minimum_deployment_target=ct.target.iOS15,\n",
    "    )\n",
    "    out_path = os.path.join(PATH_EXPORT, out_name)\n",
    "    mlmodel.save(out_path)\n",
    "    print(\"[CoreML][ONNX] saved:\", out_path)\n",
    "    return out_path\n",
    "\n",
    "# Encoder\n",
    "try:\n",
    "    enc_ml = to_coreml_from_ts(\n",
    "        enc_ts,\n",
    "        [ct.TensorType(name=\"rgb_norm\", shape=ex_rgb.shape)],\n",
    "        \"Encoder.mlpackage\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"[CoreML] Encoder TS failed; fallback to ONNX:\", repr(e))\n",
    "    onnx_path = os.path.join(PATH_EXPORT, \"Encoder.onnx\")\n",
    "    torch.onnx.export(enc, (ex_rgb,), onnx_path, opset_version=17,\n",
    "                      input_names=[\"rgb_norm\"], output_names=[\"rgb_latent\"], dynamic_axes=None)\n",
    "    enc_ml = to_coreml_from_onnx(\n",
    "        onnx_path, [ct.TensorType(name=\"rgb_norm\", shape=ex_rgb.shape)], \"Encoder.mlpackage\"\n",
    "    )\n",
    "\n",
    "# UNetStep\n",
    "try:\n",
    "    step_ml = to_coreml_from_ts(\n",
    "        step_ts,\n",
    "        [\n",
    "            ct.TensorType(name=\"rgb_latent\",    shape=ex_lat.shape),\n",
    "            ct.TensorType(name=\"target_latent\", shape=ex_lat.shape),\n",
    "            ct.TensorType(name=\"t_f32\",         shape=(1,)),\n",
    "        ],\n",
    "        \"UNetStep.mlpackage\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"[CoreML] UNetStep TS failed; fallback to ONNX:\", repr(e))\n",
    "    onnx_path = os.path.join(PATH_EXPORT, \"UNetStep.onnx\")\n",
    "    torch.onnx.export(step, (ex_lat, ex_lat, ex_t), onnx_path, opset_version=17,\n",
    "                      input_names=[\"rgb_latent\",\"target_latent\",\"t_f32\"], output_names=[\"noise_pred\"], dynamic_axes=None)\n",
    "    step_ml = to_coreml_from_onnx(\n",
    "        onnx_path,\n",
    "        [\n",
    "            ct.TensorType(name=\"rgb_latent\",    shape=ex_lat.shape),\n",
    "            ct.TensorType(name=\"target_latent\", shape=ex_lat.shape),\n",
    "            ct.TensorType(name=\"t_f32\",         shape=(1,)),\n",
    "        ],\n",
    "        \"UNetStep.mlpackage\",\n",
    "    )\n",
    "\n",
    "# Decoder\n",
    "try:\n",
    "    dec_ml = to_coreml_from_ts(\n",
    "        dec_ts,\n",
    "        [ct.TensorType(name=\"depth_latent\", shape=ex_lat.shape)],\n",
    "        \"Decoder.mlpackage\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"[CoreML] Decoder TS failed; fallback to ONNX:\", repr(e))\n",
    "    onnx_path = os.path.join(PATH_EXPORT, \"Decoder.onnx\")\n",
    "    torch.onnx.export(dec, (ex_lat,), onnx_path, opset_version=17,\n",
    "                      input_names=[\"depth_latent\"], output_names=[\"depth\"], dynamic_axes=None)\n",
    "    dec_ml = to_coreml_from_onnx(\n",
    "        onnx_path, [ct.TensorType(name=\"depth_latent\", shape=ex_lat.shape)], \"Decoder.mlpackage\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1ed06",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e9cbf",
   "metadata": {},
   "source": [
    "## Optional: Post training quantization (QAT) is required for CoreML conversion. If you first convert to CoreML without QAT, the CoreML package (not yet supported) will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca11bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ Quantizing weights to int8: /datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml/Encoder.mlpackage\n",
      "   Saved: /datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml/Encoder-INT8.mlpackage\n",
      "\n",
      "→ Quantizing weights to int8: /datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml/UNetStep.mlpackage\n",
      "   Saved: /datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml/UNetStep-INT8.mlpackage\n",
      "\n",
      "→ Quantizing weights to int8: /datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml/Decoder.mlpackage\n",
      "   Saved: /datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml/Decoder-INT8.mlpackage\n",
      "\n",
      "Done. Open the *-INT8.mlpackage in Xcode → Performance to benchmark.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import coremltools as ct\n",
    "from coremltools.models.neural_network import quantization_utils\n",
    "from coremltools.models.utils import load_spec\n",
    "\n",
    "BASE = \"/datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml\"\n",
    "MODELS = [\"Encoder\", \"UNetStep\", \"Decoder\"]\n",
    "\n",
    "def quantize_pkg(pkg_path: str):\n",
    "    coreml_dir  = os.path.join(pkg_path, \"Data\", \"com.apple.CoreML\")\n",
    "    spec_path   = os.path.join(coreml_dir, \"model.mlmodel\")\n",
    "    weights_dir = os.path.join(coreml_dir, \"weights\")\n",
    "    assert os.path.isfile(spec_path), f\"missing {spec_path}\"\n",
    "    assert os.path.isdir(weights_dir), f\"missing {weights_dir}\"\n",
    "\n",
    "    print(f\"\\n→ Quantizing weights to int8: {pkg_path}\")\n",
    "    spec = load_spec(spec_path)\n",
    "\n",
    "    # Quantize weights in the spec (linear int8; no calibration, weights-only)\n",
    "    qspec = quantization_utils._quantize_spec_weights(  # underscore is fine here\n",
    "        spec, nbits=8, quantization_mode=\"linear\"\n",
    "    )\n",
    "\n",
    "    # Re-wrap with weights_dir so it can save as a full .mlpackage\n",
    "    ml_q = ct.models.MLModel(qspec, weights_dir=weights_dir)\n",
    "\n",
    "    out_pkg = pkg_path.replace(\".mlpackage\", \"-INT8.mlpackage\")\n",
    "    ml_q.save(out_pkg)\n",
    "    print(f\"   Saved: {out_pkg}\")\n",
    "    return out_pkg\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for name in MODELS:\n",
    "        pkg = os.path.join(BASE, f\"{name}.mlpackage\")\n",
    "        if os.path.isdir(pkg):\n",
    "            quantize_pkg(pkg)\n",
    "        else:\n",
    "            print(f\"!! Missing {pkg}\")\n",
    "\n",
    "    print(\"\\nDone. Open the *-INT8.mlpackage in Xcode → Performance to benchmark.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ee52b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "from coremltools.models.neural_network.quantization_utils import _quantize_spec_weights\n",
    "from coremltools.models import MLModel\n",
    "\n",
    "# Step 1: Load model spec directly from .mlmodel inside the .mlpackage\n",
    "spec = ct.utils.load_spec(\"/datasets/abradshaw/export_coreml_Full_F16/Decoder.mlpackage/Data/com.apple.CoreML/model.mlmodel\")\n",
    "weights_dir = \"/datasets/abradshaw/export_coreml_Full_F16/Decoder.mlpackage/Data/com.apple.CoreML/weights\"\n",
    "\n",
    "# Step 2: Quantize weights using the *private* function (bypasses automatic wrapping)\n",
    "qspec = _quantize_spec_weights(\n",
    "    spec,\n",
    "    nbits=8,\n",
    "    quantization_mode=\"linear\"\n",
    ")\n",
    "\n",
    "# Step 3: Manually wrap with original weights directory\n",
    "qmodel = MLModel(qspec, weights_dir=weights_dir)\n",
    "\n",
    "# Step 4: Save as a new quantized model package\n",
    "qmodel.save(\"Decoder_fp8.mlpackage\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5801f7",
   "metadata": {},
   "source": [
    "## Weight precision checker for the optional conversion using PTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714bdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Encoder.mlpackage ===\n",
      "weight.bin                                                      65.17 MB\n",
      "TOTAL weights: 65.17 MB\n",
      "\n",
      "=== UNetStep.mlpackage ===\n",
      "weight.bin                                                    1606.60 MB\n",
      "TOTAL weights: 1,606.60 MB\n",
      "\n",
      "=== Decoder.mlpackage ===\n",
      "weight.bin                                                      94.41 MB\n",
      "TOTAL weights: 94.41 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "BASE = \"/datasets/abradshaw/output_QAT/train_marigold_depth_qat/export_coreml\"\n",
    "NAMES = [\"Encoder\",\"UNetStep\",\"Decoder\"]\n",
    "\n",
    "def weight_size(pkg):\n",
    "    w = os.path.join(pkg, \"Data\",\"com.apple.CoreML\",\"weights\",\"weight.bin\")\n",
    "    return os.path.getsize(w) if os.path.isfile(w) else 0\n",
    "\n",
    "print(\"Model         FP16 (MB)   INT8 (MB)   shrink\")\n",
    "for n in NAMES:\n",
    "    p16 = os.path.join(BASE, f\"{n}.mlpackage\")\n",
    "    p8  = os.path.join(BASE, f\"{n}-INT8.mlpackage\")\n",
    "    s16 = weight_size(p16) / (1024**2)\n",
    "    s8  = weight_size(p8)  / (1024**2)\n",
    "    if s16 and s8:\n",
    "        r = (1 - s8/s16) * 100\n",
    "        print(f\"{n:<12} {s16:9.2f}   {s8:9.2f}   {r:6.1f}%\")\n",
    "    else:\n",
    "        print(f\"{n:<12} (missing weight.bin)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ff17e2",
   "metadata": {},
   "source": [
    "# DEBUG - some helper functions for debugging and sanity checks on the conversion script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eacf080",
   "metadata": {},
   "source": [
    "## For cell 1.2 for debug checks on the snaitzer and funcitnal ops remval ttaht are not supperotedd in the sciriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- 2.1 Load UNet config only, force 8 input channels ---\n",
    "unet_cfg = UNet2DConditionModel.load_config(os.path.join(PATH_PRETRAINED_LOCAL, \"unet\"))\n",
    "unet_cfg[\"in_channels\"] = 8  # we trained on [rgb_latent(4) + target_latent(4)]\n",
    "unet = UNet2DConditionModel.from_config(unet_cfg).to(device)  # empty FP32 skeleton\n",
    "\n",
    "\n",
    "\n",
    "def count_bad_activations(model):\n",
    "    n_gelu_mod = n_silu_mod = n_gelu_fun = n_silu_fun = 0\n",
    "    for m in model.modules():                        # module instances\n",
    "        if isinstance(m, nn.GELU): n_gelu_mod += 1\n",
    "        if isinstance(m, nn.SiLU): n_silu_mod += 1\n",
    "    for m in model.modules():                        # cached functional refs\n",
    "        for _, attr in vars(m).items():\n",
    "            if attr is F.gelu: n_gelu_fun += 1\n",
    "            if attr is F.silu: n_silu_fun += 1\n",
    "    return (n_gelu_mod + n_gelu_fun, n_silu_mod + n_silu_fun)\n",
    "\n",
    "gelu_cnt, silu_cnt = count_bad_activations(unet)\n",
    "print(f\"[before sanitize] GELU-like: {gelu_cnt} | SiLU-like: {silu_cnt}\")\n",
    "# (optional) warn if nothing to replace\n",
    "if gelu_cnt == 0 and silu_cnt == 0:\n",
    "    print(\" No GELU/SiLU found; sanitize_only may be redundant.\")\n",
    "\n",
    "\n",
    "# --- 2.2 Sanitize activations to match QAT training (GELU->ReLU, SiLU->Hardswish) ---\n",
    "_BAD2GOOD = {\n",
    "    nn.GELU: lambda _: nn.ReLU(inplace=False),\n",
    "    nn.SiLU: lambda _: nn.Hardswish(),\n",
    "}\n",
    "_BAD_FUNCS = {F.gelu, F.silu}\n",
    "def _relu(x): return F.relu(x, inplace=False)\n",
    "\n",
    "def sanitize_only(module: nn.Module):\n",
    "    for name, child in list(module.named_children()):\n",
    "        replaced = False\n",
    "        for bad_cls, make_good in _BAD2GOOD.items():\n",
    "            if isinstance(child, bad_cls):\n",
    "                setattr(module, name, make_good(child))\n",
    "                replaced = True\n",
    "                break\n",
    "        sanitize_only(getattr(module, name) if replaced else child)\n",
    "    for attr_name, attr_val in vars(module).items():\n",
    "        if callable(attr_val) and attr_val in _BAD_FUNCS:\n",
    "            setattr(module, attr_name, _relu)\n",
    "\n",
    "sanitize_only(unet)\n",
    "\n",
    "# Disable memory-efficient attention (PyTorch export prefers standard ops)\n",
    "try:\n",
    "    unet.disable_xformers_memory_efficient_attention()\n",
    "except Exception:\n",
    "    pass\n",
    "unet.eval()\n",
    "\n",
    "# --- 2.3 Load QAT state-dict; observers/fake-quant keys are ignored by strict=False ---\n",
    "sd = torch.load(PATH_QAT_SD, map_location=\"cpu\", weights_only=True)\n",
    "missing, unexpected = unet.load_state_dict(sd, strict=False)\n",
    "print(f\"[UNet load] missing={len(missing)} unexpected={len(unexpected)}\")\n",
    "if unexpected:\n",
    "    print(\"  (expected: observer/fake-quant keys show up here and are ignored)\")\n",
    "\n",
    "# quick forward sanity\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(1, 8, 64, 64)\n",
    "    t = torch.tensor([0])\n",
    "    cond = torch.randn(1, 77, 1024)\n",
    "    y = unet(x, t, encoder_hidden_states=cond).sample\n",
    "    print(\"UNet forward OK →\", tuple(y.shape))\n",
    "\n",
    "\n",
    "# 2.4 — Activation parity check (matches what you trained)\n",
    "import torch.nn as nn\n",
    "from torch.export import export  # PyTorch 2.1+; if missing, see note below.\n",
    "\n",
    "# Count Hardswish modules (should be >0) and verify no nn.SiLU left.\n",
    "hswish_count = sum(isinstance(m, nn.Hardswish) for m in unet.modules())\n",
    "silu_count   = sum(isinstance(m, nn.SiLU) for m in unet.modules())\n",
    "print(f\"Hardswish modules: {hswish_count} | SiLU modules: {silu_count}\")\n",
    "\n",
    "# Exported graph still uses inline GELU in transformer MLPs (expected).\n",
    "gm = export(unet, (x, t, cond))\n",
    "has_gelu = any(\"gelu\" in (getattr(n.target, \"__name__\", \"\") or str(n.target))\n",
    "               for n in gm.graph.nodes)\n",
    "print(\"GELU present in exported graph:\", has_gelu)  # should be True\n",
    "\n",
    "# Optional: inspect a compact op table\n",
    "try:\n",
    "    gm.graph.print_tabular()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(PATH_PRETRAINED_LOCAL, subfolder=\"vae\").to(device).eval()\n",
    "print(\"VAE loaded.\")\n",
    "fixed_embed = torch.randn(1, 77, 1024, dtype=torch.float32)\n",
    "print(\"fixed_embed loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78f35aa",
   "metadata": {},
   "source": [
    "## For running after cell 4, to check if the ts scirpoted models are well behaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, torch\n",
    "from collections import Counter\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# --- reload TS ---\n",
    "enc_ts  = torch.jit.load(os.path.join(PATH_EXPORT, \"Encoder.ts\")).eval()\n",
    "step_ts = torch.jit.load(os.path.join(PATH_EXPORT, \"UNetStep.ts\")).eval()\n",
    "dec_ts  = torch.jit.load(os.path.join(PATH_EXPORT, \"Decoder.ts\")).eval()\n",
    "\n",
    "# --- helpers ---\n",
    "def _shape(t): return tuple(t.shape)\n",
    "\n",
    "def check_graph(name, m):\n",
    "    g = m.inlined_graph if hasattr(m, \"inlined_graph\") else m.graph\n",
    "    kinds = [n.kind() for n in g.nodes()]\n",
    "    c = Counter(kinds)\n",
    "    bad_signatures = [\n",
    "        \"quantize_per_tensor\", \"aten::fake_quantize_per_tensor_affine\",\n",
    "        \"xformers\", \"aten::_scaled_dot_product_efficient_attention\"\n",
    "    ]\n",
    "    has_bad = [k for k in kinds if any(b in k for b in bad_signatures)]\n",
    "    print(f\"[graph] {name}: {len(kinds)} nodes | top ops: {c.most_common(8)}\")\n",
    "    if has_bad:\n",
    "        print(f\"  bad unexpected ops present: {sorted(set(has_bad))}\")\n",
    "    else:\n",
    "        print(\"  good no obvious bad ops found\")\n",
    "\n",
    "def assert_close(a, b, name, rtol=1e-3, atol=1e-3):\n",
    "    try:\n",
    "        torch.testing.assert_close(a, b, rtol=rtol, atol=atol)\n",
    "        print(f\"  good {name} close (rtol={rtol}, atol={atol})\")\n",
    "    except AssertionError as e:\n",
    "        max_abs = (a - b).abs().max().item()\n",
    "        print(f\"  bad {name} mismatch (max_abs={max_abs:.4e})\")\n",
    "\n",
    "def no_nans(t, name):\n",
    "    if torch.isnan(t).any() or torch.isinf(t).any():\n",
    "        raise RuntimeError(f\"{name} has NaN/Inf\")\n",
    "    else:\n",
    "        print(f\" good {name} finite\")\n",
    "\n",
    "# --- run N randomized trials with static shapes ---\n",
    "N = 3\n",
    "B, H, W = 1, 512, 512\n",
    "h, w = H//8, W//8\n",
    "\n",
    "for trial in range(1, N+1):\n",
    "    torch.manual_seed(1234 + trial)\n",
    "    print(f\"\\n=== Sanity trial {trial}/{N} ===\")\n",
    "\n",
    "    rgb = torch.randn(B,3,H,W, dtype=torch.float32)\n",
    "    lat = torch.randn(B,4,h,w, dtype=torch.float32)\n",
    "    t_f = torch.tensor([float(random.randrange(0, 50))], dtype=torch.float32)\n",
    "\n",
    "    # Eager refs (wrappers you built earlier)\n",
    "    rgb_latent_ref = enc(rgb)\n",
    "    noise_ref      = step(lat, lat, t_f)\n",
    "    depth_ref      = dec(lat)\n",
    "\n",
    "    # TS runs\n",
    "    rgb_latent_ts = enc_ts(rgb)\n",
    "    noise_ts      = step_ts(lat, lat, t_f)\n",
    "    depth_ts      = dec_ts(lat)\n",
    "\n",
    "    # Shape checks\n",
    "    print(\"  shapes:\",\n",
    "          \"enc\", _shape(rgb_latent_ts),\n",
    "          \"| step\", _shape(noise_ts),\n",
    "          \"| dec\", _shape(depth_ts))\n",
    "    assert _shape(rgb_latent_ts) == (B,4,h,w)\n",
    "    assert _shape(noise_ts)      == (B,4,h,w)\n",
    "    assert _shape(depth_ts)      == (B,1,H,W) or _shape(depth_ts) == (B,1,h,w)\n",
    "\n",
    "    # Finite checks\n",
    "    no_nans(rgb_latent_ts, \"Encoder.ts out\")\n",
    "    no_nans(noise_ts,      \"UNetStep.ts out\")\n",
    "    no_nans(depth_ts,      \"Decoder.ts out\")\n",
    "\n",
    "    # Numeric closeness (script/trace vs eager)\n",
    "    # Tolerances are loose because tracing can fold constants & reorder ops.\n",
    "    assert_close(rgb_latent_ts, rgb_latent_ref, \"Encoder TS≈eager\", rtol=1e-3, atol=2e-3)\n",
    "    assert_close(noise_ts,      noise_ref,      \"UNetStep TS≈eager\", rtol=2e-3, atol=3e-3)\n",
    "    assert_close(depth_ts,      depth_ref,      \"Decoder TS≈eager\",  rtol=1e-3, atol=2e-3)\n",
    "\n",
    "# Graph hygiene\n",
    "check_graph(\"Encoder.ts\",  enc_ts)\n",
    "check_graph(\"UNetStep.ts\", step_ts)\n",
    "check_graph(\"Decoder.ts\",  dec_ts)\n",
    "\n",
    "print(\"\\nTS sanity pass completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875d9a5",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427b364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder → 2.179 G-FLOPs, 1.090 G-MACs\n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total KFLOPs  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             aten::mm         0.61%       1.936ms         0.61%       1.937ms     645.544us             3   1610612.736  \n",
      "                                          aten::addmm         0.19%     603.935us         0.22%     705.560us     705.560us             1    536870.912  \n",
      "                                            aten::add         4.26%      13.554ms         4.26%      13.554ms       1.232ms            11     31981.568  \n",
      "                                            aten::mul         0.00%      15.269us         0.01%      19.751us       9.875us             2         4.096  \n",
      "                                          aten::equal         0.01%      28.219us         0.01%      29.977us       2.998us            10            --  \n",
      "                                   aten::is_same_size         0.00%       1.758us         0.00%       1.758us       0.176us            10            --  \n",
      "                                           aten::item         0.00%      11.983us         0.01%      18.814us       1.176us            16            --  \n",
      "                            aten::_local_scalar_dense         0.00%       6.831us         0.00%       6.831us       0.427us            16            --  \n",
      "                                              forward         0.35%       1.126ms        99.99%     318.020ms     318.020ms             1            --  \n",
      "                                   aten::_convolution         0.13%     403.607us        78.92%     251.013ms       8.965ms            28            --  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 318.061ms\n",
      "\n",
      "\n",
      "Decoder → 2.195 G-FLOPs, 1.097 G-MACs\n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total MFLOPs  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             aten::mm         0.26%       1.906ms         0.26%       1.908ms     635.844us             3      1610.613  \n",
      "                                          aten::addmm         0.08%     611.741us         0.10%     708.831us     708.831us             1       536.871  \n",
      "                                            aten::add         2.77%      20.649ms         2.77%      20.649ms       1.377ms            15        47.186  \n",
      "                                          aten::equal         0.00%      29.770us         0.00%      31.626us       2.259us            14            --  \n",
      "                                   aten::is_same_size         0.00%       1.856us         0.00%       1.856us       0.133us            14            --  \n",
      "                                           aten::item         0.00%      12.610us         0.00%      18.675us       0.934us            20            --  \n",
      "                            aten::_local_scalar_dense         0.00%       6.065us         0.00%       6.065us       0.303us            20            --  \n",
      "                                              forward         0.31%       2.337ms        99.99%     745.229ms     745.229ms             1            --  \n",
      "                                            aten::div         1.36%      10.106ms         1.40%      10.456ms     653.488us            16            --  \n",
      "                                             aten::to         0.01%      61.246us         0.05%     373.591us      19.663us            19            --  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 745.273ms\n",
      "\n",
      "\n",
      "UNetStep → 67.984 G-FLOPs, 33.992 G-MACs\n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  Total MFLOPs  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          aten::addmm        14.27%      75.982ms        15.14%      80.633ms     775.315us           104     48028.058  \n",
      "                                             aten::mm         7.01%      37.341ms         7.02%      37.363ms     333.600us           112     19926.876  \n",
      "                                            aten::add         0.87%       4.637ms         0.87%       4.637ms      42.933us           108        17.572  \n",
      "                                            aten::mul         0.42%       2.252ms         0.43%       2.267ms      34.354us            66        11.551  \n",
      "                                          aten::equal         0.03%     137.777us         0.03%     146.059us       1.098us           133            --  \n",
      "                                   aten::is_same_size         0.00%       8.282us         0.00%       8.282us       0.062us           133            --  \n",
      "                                           aten::item         0.04%     238.159us         0.06%     315.340us       1.866us           169            --  \n",
      "                            aten::_local_scalar_dense         0.01%      77.181us         0.01%      77.181us       0.457us           169            --  \n",
      "                                         aten::arange         0.01%      27.353us         0.01%      54.543us      27.272us             2            --  \n",
      "                                          aten::empty         0.34%       1.814ms         0.34%       1.814ms       2.633us           689            --  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 532.578ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flops counter\n",
    "import torch\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "def profile_model(name, path, input_tensor):\n",
    "    model = torch.jit.load(path).eval().to(\"cpu\")\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU],\n",
    "        record_shapes=True,\n",
    "        with_flops=True\n",
    "    ) as prof:\n",
    "        if isinstance(input_tensor, tuple):\n",
    "            model(*input_tensor)\n",
    "        else:\n",
    "            model(input_tensor)\n",
    "\n",
    "    total_flops = sum(\n",
    "        evt.flops for evt in prof.key_averages()\n",
    "        if getattr(evt, \"flops\", None) is not None\n",
    "    )\n",
    "    total_macs = total_flops / 2\n",
    "    flops_G = total_flops / 1e9\n",
    "    macs_G  = total_macs  / 1e9\n",
    "\n",
    "    print(f\"\\n{name} → {flops_G:.3f} G-FLOPs, {macs_G:.3f} G-MACs\")\n",
    "    print(prof.key_averages().table(sort_by=\"flops\", row_limit=10))\n",
    "\n",
    "\n",
    "\n",
    "rgb_latent     = torch.randn(1, 4, 32, 24)  # Output of encode_rgb\n",
    "target_latent  = torch.randn(1, 4, 32, 24)  # Noisy or clean latent\n",
    "timesteps_f32  = torch.tensor([500.0])     # Single timestep as float32\n",
    "\n",
    "# Update paths to point to your actual .ts files\n",
    "profile_model(\"Encoder\",   \"/datasets/abradshaw/export_coreml_Full_F16/Encoder.ts\",   torch.randn(1, 3, 256, 256))\n",
    "profile_model(\"Decoder\",   \"/datasets/abradshaw/export_coreml_Full_F16/Decoder.ts\",   torch.randn(1, 4, 32, 32))\n",
    "profile_model(\"UNetStep\", \"/datasets/abradshaw/export_coreml_Full_F16/UNetStep.ts\", (rgb_latent, target_latent, timesteps_f32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f17fb38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input shape: (1, 3, 256, 192) → output shape: (1, 4, 32, 24)\n"
     ]
    }
   ],
   "source": [
    "enc = torch.jit.load(\"/datasets/abradshaw/export_coreml_Full_F16/Encoder.ts\").eval()\n",
    "x = torch.randn(1,3,256,192)\n",
    "print(\"Encoder input shape:\", tuple(x.shape), \"→ output shape:\", tuple(enc(x).shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc194eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "register_forward_hook is not supported on ScriptModules",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m encoder \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datasets/abradshaw/export_coreml_Full_F16/Encoder.ts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m encoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 9\u001b[0m macs_enc, params_enc \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_complexity_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_per_layer_stat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m flops_enc \u001b[38;5;241m=\u001b[39m macs_enc \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoder.ts → MACs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmacs_enc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, FLOPs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflops_enc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams_enc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/ptflops/flops_counter.py:94\u001b[0m, in \u001b[0;36mget_model_complexity_info\u001b[0;34m(model, input_res, print_per_layer_stat, as_strings, input_constructor, ost, verbose, ignore_modules, custom_modules_hooks, backend, flops_units, param_units, output_precision, backend_specific_config)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, nn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m FLOPS_BACKEND(backend) \u001b[38;5;241m==\u001b[39m FLOPS_BACKEND\u001b[38;5;241m.\u001b[39mPYTORCH:\n\u001b[1;32m     93\u001b[0m     flops_count, params_count \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 94\u001b[0m         \u001b[43mget_flops_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_res\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mprint_per_layer_stat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                          \u001b[49m\u001b[43minput_constructor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43most\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcustom_modules_hooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                          \u001b[49m\u001b[43moutput_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mflops_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflops_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mparam_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mextra_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_specific_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m FLOPS_BACKEND(backend) \u001b[38;5;241m==\u001b[39m FLOPS_BACKEND\u001b[38;5;241m.\u001b[39mATEN:\n\u001b[1;32m    104\u001b[0m     flops_count, params_count \u001b[38;5;241m=\u001b[39m get_flops_aten(model, input_res,\n\u001b[1;32m    105\u001b[0m                                                print_per_layer_stat,\n\u001b[1;32m    106\u001b[0m                                                input_constructor, ost,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m                                                param_units\u001b[38;5;241m=\u001b[39mparam_units,\n\u001b[1;32m    112\u001b[0m                                                extra_config\u001b[38;5;241m=\u001b[39mbackend_specific_config)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/ptflops/pytorch_engine.py:37\u001b[0m, in \u001b[0;36mget_flops_pytorch\u001b[0;34m(model, input_res, print_per_layer_stat, input_constructor, ost, verbose, ignore_modules, custom_modules_hooks, output_precision, flops_units, param_units, extra_config)\u001b[0m\n\u001b[1;32m     35\u001b[0m flops_model \u001b[38;5;241m=\u001b[39m add_flops_counting_methods(model)\n\u001b[1;32m     36\u001b[0m flops_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 37\u001b[0m \u001b[43mflops_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_flops_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43most\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43most\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mignore_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_modules\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_constructor:\n\u001b[1;32m     40\u001b[0m     batch \u001b[38;5;241m=\u001b[39m input_constructor(input_res)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/ptflops/pytorch_engine.py:211\u001b[0m, in \u001b[0;36mstart_flops_count\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart_flops_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    A method that will be available after add_flops_counting_methods() is called\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    on a desired net object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     \u001b[43madd_batch_counter_hook_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     seen_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madd_flops_counter_hook_function\u001b[39m(module, ost, verbose, ignore_list):\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/ptflops/pytorch_engine.py:289\u001b[0m, in \u001b[0;36madd_batch_counter_hook_function\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__batch_counter_handle__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_forward_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_counter_hook\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m module\u001b[38;5;241m.\u001b[39m__batch_counter_handle__ \u001b[38;5;241m=\u001b[39m handle\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/jit/_script.py:983\u001b[0m, in \u001b[0;36m_make_fail.<locals>.fail\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfail\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not supported on ScriptModules\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: register_forward_hook is not supported on ScriptModules"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ptflops import get_model_complexity_info\n",
    "import torch\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "# Profile Encoder (.ts expects input [1,3,256,256])\n",
    "encoder = torch.jit.load(\"/datasets/abradshaw/export_coreml_Full_F16/Encoder.ts\")\n",
    "encoder.eval()\n",
    "macs_enc, params_enc = get_model_complexity_info(\n",
    "    encoder,\n",
    "    (3, 256, 256),\n",
    "    as_strings=False,\n",
    "    print_per_layer_stat=False,\n",
    "    verbose=False\n",
    ")\n",
    "flops_enc = macs_enc * 2\n",
    "print(f\"Encoder.ts → MACs: {macs_enc:,}, FLOPs: {flops_enc:,}, Params: {params_enc:,}\")\n",
    "\n",
    "# %%\n",
    "# Profile Decoder (.ts expects input [1,4,32,32])\n",
    "decoder = torch.jit.load(\"/datasets/abradshaw/export_coreml_Full_F16/Decoder.ts\")\n",
    "decoder.eval()\n",
    "macs_dec, params_dec = get_model_complexity_info(\n",
    "    decoder,\n",
    "    (4, 32, 32),\n",
    "    as_strings=False,\n",
    "    print_per_layer_stat=False,\n",
    "    verbose=False\n",
    ")\n",
    "flops_dec = macs_dec * 2\n",
    "print(f\"Decoder.ts → MACs: {macs_dec:,}, FLOPs: {flops_dec:,}, Params: {params_dec:,}\")\n",
    "\n",
    "# %%\n",
    "# Profile UNetStep by combining two latent inputs along channels\n",
    "# UNetStep.ts expects (rgb_latent, target_latent, t), we approximate by merging latents\n",
    "\n",
    "# Load module\n",
    "unet_step = torch.jit.load(\"/datasets/abradshaw/export_coreml_Full_F16/UNetStep.ts\")\n",
    "unet_step.eval()\n",
    "\n",
    "# Define wrapper to accept merged latent\n",
    "def unet_step_wrapper(x):\n",
    "    # x: Tensor of shape (1,8,32,32) => split into two latents\n",
    "    rgb_latent, target_latent = torch.chunk(x, 2, dim=1)\n",
    "    t = torch.tensor([0.0])\n",
    "    return unet_step(rgb_latent, target_latent, t)\n",
    "\n",
    "macs_step, params_step = get_model_complexity_info(\n",
    "    unet_step_wrapper,\n",
    "    (8, 32, 32),\n",
    "    as_strings=False,\n",
    "    print_per_layer_stat=False,\n",
    "    verbose=False\n",
    ")\n",
    "flops_step = macs_step * 2\n",
    "print(f\"UNetStep.ts → MACs: {macs_step:,}, FLOPs: {flops_step:,}, Params: {params_step:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7b61f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m relu_like \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m hswish_like \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, m \u001b[38;5;129;01min\u001b[39;00m \u001b[43munet\u001b[49m\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, nn\u001b[38;5;241m.\u001b[39mGELU):   gelu_like\u001b[38;5;241m.\u001b[39mappend(n)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, nn\u001b[38;5;241m.\u001b[39mSiLU):   silu_like\u001b[38;5;241m.\u001b[39mappend(n)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unet' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "gelu_like = []\n",
    "silu_like = []\n",
    "relu_like = []\n",
    "hswish_like = []\n",
    "\n",
    "for n, m in unet.named_modules():\n",
    "    if isinstance(m, nn.GELU):   gelu_like.append(n)\n",
    "    if isinstance(m, nn.SiLU):   silu_like.append(n)\n",
    "    if isinstance(m, nn.ReLU):   relu_like.append(n)\n",
    "    if isinstance(m, nn.Hardswish): hswish_like.append(n)\n",
    "\n",
    "print(f\"GELU left: {len(gelu_like)} | SiLU left: {len(silu_like)}\")\n",
    "print(f\"ReLU found: {len(relu_like)} | Hardswish found: {len(hswish_like)}\")\n",
    "assert len(gelu_like) == 0 and len(silu_like) == 0, \"Found GELU/SiLU; sanitize_only didn't fully apply.\"\n",
    "\n",
    "print(\"✅ Sanitized activations in place (ReLU/Hardswish); no GELU/SiLU remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04110032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QAT ckpt] fake-quant/observer-like keys in file: 3087 (show 10)\n",
      "['conv_in.weight_fake_quant.fake_quant_enabled', 'conv_in.weight_fake_quant.observer_enabled', 'conv_in.weight_fake_quant.scale', 'conv_in.weight_fake_quant.zero_point', 'conv_in.weight_fake_quant.activation_post_process.eps', 'conv_in.weight_fake_quant.activation_post_process.min_val', 'conv_in.weight_fake_quant.activation_post_process.max_val', 'conv_in.activation_post_process.fake_quant_enabled', 'conv_in.activation_post_process.observer_enabled', 'conv_in.activation_post_process.scale']\n",
      "[model] modules still carrying observer/fake-quant attributes: 0\n",
      "conv_in.weight shape: (320, 8, 3, 3)\n",
      "✅ Observers stripped from runtime graph; conv_in is 8ch; QAT weights loaded.\n"
     ]
    }
   ],
   "source": [
    "# 7A.1 — List the \"unexpected\" keys we ignored when loading strict=False\n",
    "qat_sd = torch.load(PATH_QAT_SD, map_location=\"cpu\", weights_only=True)\n",
    "unexpected_like = [k for k in qat_sd.keys()\n",
    "                   if (\"activation_post_process\" in k) or (\"weight_fake_quant\" in k) or (\"_fake_quant\" in k)]\n",
    "print(f\"[QAT ckpt] fake-quant/observer-like keys in file: {len(unexpected_like)} (show 10)\")\n",
    "print(unexpected_like[:10])\n",
    "\n",
    "# 7A.2 — Make sure the *model* has none of these tensors after load\n",
    "has_obs_attr = []\n",
    "for n, m in unet.named_modules():\n",
    "    ap = getattr(m, \"activation_post_process\", None)\n",
    "    wfq = getattr(m, \"weight_fake_quant\", None)\n",
    "    if ap is not None or wfq is not None:\n",
    "        has_obs_attr.append((n, type(m).__name__, ap is not None, wfq is not None))\n",
    "\n",
    "print(f\"[model] modules still carrying observer/fake-quant attributes: {len(has_obs_attr)}\")\n",
    "assert len(has_obs_attr) == 0, \"Some modules still have observer/fake-quant attributes.\"\n",
    "\n",
    "# 7A.3 — Conv-in audit (8 channels)\n",
    "w = dict(unet.named_parameters()).get(\"conv_in.weight\", None)\n",
    "assert w is not None, \"conv_in.weight not found\"\n",
    "print(\"conv_in.weight shape:\", tuple(w.shape))\n",
    "assert w.shape[1] == 8, \"conv_in is NOT 8-channel — something is off.\"\n",
    "\n",
    "print(\"✅ Observers stripped from runtime graph; conv_in is 8ch; QAT weights loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32cecf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functional SiLU/GELU refs left: []\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "bad_funcs = {F.silu, F.gelu}\n",
    "bad_refs = []\n",
    "for n, m in unet.named_modules():\n",
    "    for attr, val in vars(m).items():\n",
    "        if callable(val) and val in bad_funcs:\n",
    "            bad_refs.append((n, attr, val.__name__))\n",
    "\n",
    "print(\"functional SiLU/GELU refs left:\", bad_refs)\n",
    "assert not bad_refs, \"Found stored functional SiLU/GELU references.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marigold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
