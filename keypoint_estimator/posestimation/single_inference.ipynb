{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a108bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.patches import Circle\n",
    "\n",
    "# # --- PATCH torch.load SAFELY FOR LEGACY CHECKPOINTS ---\n",
    "# import numpy.core.multiarray\n",
    "# import functools\n",
    "\n",
    "# PROJECT_ROOT = '/home/abradshaw/posestimation/elif_model'\n",
    "\n",
    "# if not hasattr(torch.load, \"_is_patched\"):\n",
    "#     _original_torch_load = torch.load\n",
    "\n",
    "#     @functools.wraps(_original_torch_load)\n",
    "#     def patched_torch_load(*args, **kwargs):\n",
    "#         if 'weights_only' not in kwargs:\n",
    "#             kwargs['weights_only'] = False\n",
    "#         return _original_torch_load(*args, **kwargs)\n",
    "\n",
    "#     patched_torch_load._is_patched = True\n",
    "#     torch.load = patched_torch_load\n",
    "\n",
    "# torch.serialization.add_safe_globals([numpy.core.multiarray._reconstruct])\n",
    "\n",
    "# # --- IMPORTS FROM YOUR PIPELINE ---\n",
    "# from elif_model.models.PoseEstimator.elif_pose import ElifPose\n",
    "# from elif_model.codecs.simcc_label import SimCCLabel\n",
    "# from elif_model.datasets.data_augmentation.formatting import PackPoseInputs\n",
    "\n",
    "# # --- CONFIG ---\n",
    "# input_size = (192, 256)  # (width, height)\n",
    "# model_ckpt = os.path.join(PROJECT_ROOT, 'training', 'elif_pose_model.pth')\n",
    "# image_path = '/home/abradshaw/posestimation/HICO_test2015_00000854.jpg'\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# codec_cfg = dict(\n",
    "#     input_size=input_size,\n",
    "#     sigma=(4.9, 5.66),\n",
    "#     simcc_split_ratio=2.0,\n",
    "#     normalize=False,\n",
    "#     use_dark=False\n",
    "# )\n",
    "\n",
    "# # --- LOAD MODEL ---\n",
    "# model = ElifPose()\n",
    "# model.load_state_dict(torch.load(model_ckpt, map_location=device, weights_only=False))\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # --- LOAD IMAGE ---\n",
    "# img = cv2.imread(image_path)\n",
    "# if img is None:\n",
    "#     raise FileNotFoundError(f'Could not load image: {image_path}')\n",
    "# h, w = img.shape[:2]\n",
    "\n",
    "# # --- COMPUTE AFFINE TRANSFORM MANUALLY ---\n",
    "# src_center = np.array([w / 2, h / 2], dtype=np.float32)\n",
    "# src_size = np.array([w, h], dtype=np.float32)\n",
    "# dst_size = np.array(input_size, dtype=np.float32)\n",
    "\n",
    "# src_points = np.array([\n",
    "#     src_center,\n",
    "#     src_center + [0, src_size[1] * -0.5],\n",
    "#     src_center + [src_size[0] * 0.5, 0]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# dst_center = dst_size / 2\n",
    "# dst_points = np.array([\n",
    "#     dst_center,\n",
    "#     dst_center + [0, dst_size[1] * -0.5],\n",
    "#     dst_center + [dst_size[0] * 0.5, 0]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# warp_mat = cv2.getAffineTransform(src_points, dst_points)\n",
    "# img_resized = cv2.warpAffine(img, warp_mat, input_size, flags=cv2.INTER_LINEAR)\n",
    "\n",
    "# # --- PACK INPUT ---\n",
    "# packed = {\n",
    "#     'img': img_resized,\n",
    "#     'img_shape': img_resized.shape,\n",
    "#     'input_size': input_size\n",
    "# }\n",
    "# packed = PackPoseInputs(packed)\n",
    "# input_tensor = packed['inputs'].unsqueeze(0).float().to(device)\n",
    "\n",
    "# # --- INFERENCE ---\n",
    "# with torch.no_grad():\n",
    "#     simcc_x, simcc_y = model(input_tensor)\n",
    "#     simcc_x = simcc_x.cpu().numpy()\n",
    "#     simcc_y = simcc_y.cpu().numpy()\n",
    "#     decoder = SimCCLabel(**codec_cfg)\n",
    "#     keypoints, scores = decoder.decode(simcc_x, simcc_y)\n",
    "\n",
    "# # --- BACK-TRANSFORM TO ORIGINAL IMAGE ---\n",
    "# warp_mat_inv = cv2.invertAffineTransform(warp_mat)\n",
    "# keypoints_2d = keypoints[0].astype(np.float32).reshape(-1, 1, 2)\n",
    "# keypoints_orig = cv2.transform(keypoints_2d, warp_mat_inv).reshape(-1, 2)\n",
    "# print(\"Decoded keypoints (x, y):\")\n",
    "# print(keypoints)\n",
    "# print(\"Scores:\")\n",
    "# print(scores)\n",
    "# print(f\"Total keypoints: {len(keypoints)}\")\n",
    "# print(\"Final transformed keypoints shape:\", keypoints_orig.shape)\n",
    "\n",
    "\n",
    "\n",
    "# # --- VISUALIZE ---\n",
    "# plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "# for (x, y) in keypoints_orig:\n",
    "#     plt.gca().add_patch(Circle((x, y), 3, color='red'))\n",
    "# plt.title(\"Predicted Keypoints\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.patches import Circle\n",
    "# import functools\n",
    "# import numpy.core.multiarray\n",
    "\n",
    "# # --- Inline fix_aspect_ratio ---\n",
    "# def fix_aspect_ratio(scale, aspect_ratio):\n",
    "#     \"\"\"Adjust the scale to match the given aspect ratio.\"\"\"\n",
    "#     w, h = scale[0]\n",
    "#     expected_w = h * aspect_ratio\n",
    "#     scale[0][0] = expected_w\n",
    "#     return scale\n",
    "\n",
    "# # --- PATCH torch.load for legacy checkpoints ---\n",
    "# if not hasattr(torch.load, \"_is_patched\"):\n",
    "#     _original_torch_load = torch.load\n",
    "#     @functools.wraps(_original_torch_load)\n",
    "#     def patched_torch_load(*args, **kwargs):\n",
    "#         if 'weights_only' not in kwargs:\n",
    "#             kwargs['weights_only'] = False\n",
    "#         return _original_torch_load(*args, **kwargs)\n",
    "#     patched_torch_load._is_patched = True\n",
    "#     torch.load = patched_torch_load\n",
    "\n",
    "# torch.serialization.add_safe_globals([numpy.core.multiarray._reconstruct])\n",
    "\n",
    "# # --- IMPORTS ---\n",
    "# from elif_model.models.PoseEstimator.elif_pose import ElifPose\n",
    "# from elif_model.codecs.simcc_label import SimCCLabel\n",
    "# from elif_model.datasets.data_augmentation.data_augmentation import GetBBoxCenterScale, TopDownAffine\n",
    "# from elif_model.datasets.data_augmentation.formatting import PackPoseInputs\n",
    "# from elif_model.datasets.transforms import bbox_xyxy2cs, get_warp_matrix\n",
    "\n",
    "\n",
    "# # --- CONFIG ---\n",
    "# PROJECT_ROOT = '/home/abradshaw/posestimation/elif_model'\n",
    "# input_size = (192, 256)\n",
    "# images_dir = '/home/abradshaw/posestimation/input_images'  # Folder of input images\n",
    "# model_ckpt = os.path.join(PROJECT_ROOT, 'training', 'elif_pose_model.pth')\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Codec config\n",
    "# codec_cfg = dict(\n",
    "#     input_size=input_size,\n",
    "#     sigma=(4.9, 5.66),\n",
    "#     simcc_split_ratio=2.0,\n",
    "#     normalize=False,\n",
    "#     use_dark=False\n",
    "# )\n",
    "\n",
    "# # --- LOAD MODEL ---\n",
    "# model = ElifPose()\n",
    "# model.load_state_dict(torch.load(model_ckpt, map_location=device))\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # --- Loop through all images in folder ---\n",
    "# valid_exts = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "# image_files = sorted([f for f in os.listdir(images_dir) if os.path.splitext(f)[1].lower() in valid_exts])\n",
    "\n",
    "# for idx, filename in enumerate(image_files):\n",
    "#     print(f\"\\n=== Processing Image {idx+1}/{len(image_files)}: {filename} ===\")\n",
    "#     image_path = os.path.join(images_dir, filename)\n",
    "#     img = cv2.imread(image_path)\n",
    "#     if img is None:\n",
    "#         print(f\" Skipping {filename}: image not found or unreadable.\")\n",
    "#         continue\n",
    "#     h, w = img.shape[:2]\n",
    "\n",
    "#     # --- Define bounding box (same manual values for all) ---\n",
    "#     x1, y1 = 250,  200\n",
    "#     x2, y2 = 1700, 2200\n",
    "#     bbox = np.array([[x1, y1, x2, y2]], dtype=np.float32)\n",
    "\n",
    "#     # --- PREPROCESSING ---\n",
    "#     sample = {\n",
    "#         'img': img,\n",
    "#         'img_shape': img.shape,\n",
    "#         'bbox': bbox,\n",
    "#         'bbox_scores': np.array([[1.0]], dtype=np.float32)\n",
    "#     }\n",
    "#     sample = GetBBoxCenterScale(sample)\n",
    "#     sample['bbox_scale'] = fix_aspect_ratio(sample['bbox_scale'], input_size[0] / input_size[1])\n",
    "#     center = sample['bbox_center'][0]\n",
    "#     scale = sample['bbox_scale'][0]\n",
    "#     rot = sample.get('bbox_rotation', np.zeros((1,), dtype=np.float32))[0]\n",
    "#     warp_mat = get_warp_matrix(center, scale, rot, output_size=input_size)\n",
    "#     sample['warp_mat'] = warp_mat\n",
    "#     sample = TopDownAffine(sample, input_size)\n",
    "\n",
    "#     # --- Prepare model input ---\n",
    "#     packed = PackPoseInputs(sample)\n",
    "#     input_tensor = packed['inputs'].unsqueeze(0).float().to(device)\n",
    "#     data_sample = packed['data_samples']\n",
    "\n",
    "#     # --- INFERENCE ---\n",
    "#     with torch.no_grad():\n",
    "#         simcc_x, simcc_y = model(input_tensor)\n",
    "#         decoder = SimCCLabel(**codec_cfg)\n",
    "#         keypoints, scores = decoder.decode(simcc_x.cpu().numpy(), simcc_y.cpu().numpy())\n",
    "\n",
    "#     # --- Back-project keypoints ---\n",
    "#      # --- Back-project keypoints ---\n",
    "#     keypoints_xy = keypoints[0, :, :2].astype(np.float32).reshape(-1, 1, 2)\n",
    "#     keypoints_transformed = cv2.transform(keypoints_xy, cv2.invertAffineTransform(warp_mat)).reshape(-1, 2)\n",
    "#     keypoints_px = np.round(keypoints_transformed).astype(int)\n",
    "#     keypoints_px[:, 0] = np.clip(keypoints_px[:, 0], 0, w - 1)\n",
    "#     keypoints_px[:, 1] = np.clip(keypoints_px[:, 1], 0, h - 1)\n",
    "\n",
    "#     # --- Print keypoints ---\n",
    "#     print(\"keypoints = np.array([\")\n",
    "#     for x, y in keypoints_px:\n",
    "#         print(f\"    [{x}, {y}],\")\n",
    "#     print(\"])\")\n",
    "\n",
    "#     # --- Summary Statistics ---\n",
    "#     print(\"\\nSummary statistics for keypoints:\")\n",
    "#     print(f\"  min x: {keypoints_px[:, 0].min()}  max x: {keypoints_px[:, 0].max()}\")\n",
    "#     print(f\"  min y: {keypoints_px[:, 1].min()}  max y: {keypoints_px[:, 1].max()}\")\n",
    "#     print(f\"  image dims: width={w}, height={h}\")\n",
    "#     print(f\"  total valid keypoints: {len(keypoints_px)}\")\n",
    "#     print(\"Scores:\")\n",
    "#     print(scores)\n",
    "#     print(\"Final transformed keypoints shape:\", keypoints_transformed.shape)\n",
    "#     print(f\"Total keypoints: {len(keypoints)} | Unique: {np.unique(keypoints_px, axis=0).shape[0]}\")\n",
    "\n",
    "#     # --- VISUALIZE ---\n",
    "#     plt.figure(figsize=(8, 10))\n",
    "#     plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "#     for i, (x, y) in enumerate(keypoints_transformed):\n",
    "#         plt.gca().add_patch(Circle((x, y), 7, color='red', alpha=0.8))\n",
    "#         plt.text(x + 5, y, str(i), fontsize=9, color='yellow', weight='bold')\n",
    "#     bbox_width, bbox_height = x2 - x1, y2 - y1\n",
    "#     plt.gca().add_patch(plt.Rectangle((x1, y1), bbox_width, bbox_height, linewidth=2, edgecolor='blue', facecolor='none'))\n",
    "#     plt.title(f\"Keypoints: {filename}\")\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34df353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workgin scior for keypints dectionss\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "# --- PATCH torch.load SAFELY FOR LEGACY CHECKPOINTS ---\n",
    "import numpy.core.multiarray\n",
    "import functools\n",
    "\n",
    "PROJECT_ROOT = '/home/abradshaw/posestimation/elif_model'\n",
    "\n",
    "if not hasattr(torch.load, \"_is_patched\"):\n",
    "    _original_torch_load = torch.load\n",
    "\n",
    "    @functools.wraps(_original_torch_load)\n",
    "    def patched_torch_load(*args, **kwargs):\n",
    "        if 'weights_only' not in kwargs:\n",
    "            kwargs['weights_only'] = False\n",
    "        return _original_torch_load(*args, **kwargs)\n",
    "\n",
    "    patched_torch_load._is_patched = True\n",
    "    torch.load = patched_torch_load\n",
    "\n",
    "torch.serialization.add_safe_globals([numpy.core.multiarray._reconstruct])\n",
    "\n",
    "# --- IMPORTS FROM YOUR PIPELINE ---\n",
    "from elif_model.models.PoseEstimator.elif_pose import ElifPose\n",
    "from elif_model.codecs.simcc_label import SimCCLabel\n",
    "from elif_model.datasets.data_augmentation.formatting import PackPoseInputs\n",
    "\n",
    "# --- CONFIG ---\n",
    "input_size = (192, 256)  # (width, height)\n",
    "model_ckpt = os.path.join(PROJECT_ROOT, 'training', 'elif_pose_model.pth')\n",
    "image_path = '/home/abradshaw/posestimation/input_images/frame_000228.png'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "codec_cfg = dict(\n",
    "    input_size=input_size,\n",
    "    sigma=(4.9, 5.66),\n",
    "    simcc_split_ratio=2.0,\n",
    "    normalize=False,\n",
    "    use_dark=False\n",
    ")\n",
    "\n",
    "# --- HALPE-26 names (26 joints) ---\n",
    "HALPE26_NAME = {\n",
    "  0:\"neck\", 1:\"head_top_mid\", 2:\"head_left\", 3:\"head_right\",\n",
    "  4:\"l_shoulder_tip\", 5:\"r_shoulder_tip\",\n",
    "  6:\"l_upperarm_root\", 7:\"r_upperarm_root\",\n",
    "  8:\"l_wrist\", 9:\"r_wrist\", 10:\"l_elbow\",\n",
    "  11:\"r_hip\", 12:\"l_hip\",\n",
    "  13:\"r_knee\", 14:\"l_knee\",\n",
    "  15:\"r_ankle\", 16:\"l_ankle\",\n",
    "  17:\"head_top\", 18:\"r_clavicle\", 19:\"pelvis\",\n",
    "  20:\"r_heel\", 21:\"l_small_toe\", 22:\"r_big_toe\",\n",
    "  23:\"l_big_toe\", 24:\"r_small_toe\", 25:\"l_heel\",\n",
    "}\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model = ElifPose()\n",
    "model.load_state_dict(torch.load(model_ckpt, map_location=device, weights_only=False))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- LOAD IMAGE ---\n",
    "img = cv2.imread(image_path)\n",
    "if img is None:\n",
    "    raise FileNotFoundError(f'Could not load image: {image_path}')\n",
    "H_img, W_img = img.shape[:2]\n",
    "\n",
    "# --- COMPUTE AFFINE TRANSFORM (center crop/scale to input_size) ---\n",
    "src_center = np.array([W_img / 2, H_img / 2], dtype=np.float32)\n",
    "src_size = np.array([W_img, H_img], dtype=np.float32)\n",
    "dst_size = np.array(input_size, dtype=np.float32)\n",
    "\n",
    "src_points = np.array([\n",
    "    src_center,\n",
    "    src_center + [0, src_size[1] * -0.5],\n",
    "    src_center + [src_size[0] * 0.5, 0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "dst_center = dst_size / 2\n",
    "dst_points = np.array([\n",
    "    dst_center,\n",
    "    dst_center + [0, dst_size[1] * -0.5],\n",
    "    dst_center + [dst_size[0] * 0.5, 0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "warp_mat = cv2.getAffineTransform(src_points, dst_points)         # 2x3\n",
    "img_resized = cv2.warpAffine(img, warp_mat, input_size, flags=cv2.INTER_LINEAR)\n",
    "\n",
    "# --- PACK INPUT ---\n",
    "packed = {\n",
    "    'img': img_resized,\n",
    "    'img_shape': img_resized.shape,\n",
    "    'input_size': input_size\n",
    "}\n",
    "packed = PackPoseInputs(packed)\n",
    "input_tensor = packed['inputs'].unsqueeze(0).float().to(device)\n",
    "\n",
    "# --- INFERENCE ---\n",
    "with torch.no_grad():\n",
    "    simcc_x, simcc_y = model(input_tensor)\n",
    "    simcc_x = simcc_x.cpu().numpy()\n",
    "    simcc_y = simcc_y.cpu().numpy()\n",
    "    decoder = SimCCLabel(**codec_cfg)\n",
    "    keypoints, scores = decoder.decode(simcc_x, simcc_y)  # <- fixed (removed stray 'a')\n",
    "\n",
    "# --- BACK-TRANSFORM TO ORIGINAL IMAGE ---\n",
    "warp_mat_inv = cv2.invertAffineTransform(warp_mat)\n",
    "keypoints_2d = keypoints[0].astype(np.float32).reshape(-1, 1, 2)\n",
    "keypoints_orig = cv2.transform(keypoints_2d, warp_mat_inv).reshape(-1, 2)  # (26,2)\n",
    "scores_vec = scores[0] if scores is not None else np.full((keypoints_orig.shape[0],), np.nan, dtype=np.float32)\n",
    "\n",
    "# --- PRINT META ---\n",
    "print(\"\\n=== Pose export (HALPE-26) ===\")\n",
    "print(f\"image_path        : {image_path}\")\n",
    "print(f\"rgb_size (HxW)    : {H_img} x {W_img}\")\n",
    "print(f\"pose_input_size   : {input_size[1]} x {input_size[0]} (HxW)\")\n",
    "print(\"affine (2x3)      :\")\n",
    "print(np.array_str(warp_mat, precision=4, suppress_small=True))\n",
    "print(\"affine_inv (2x3)  :\")\n",
    "print(np.array_str(warp_mat_inv, precision=4, suppress_small=True))\n",
    "\n",
    "# --- PRINT TABLE (idx, name, x, y, score) ---\n",
    "print(\"\\nidx  name               x_rgb      y_rgb      score\")\n",
    "print(\"---- -----------------  ---------  ---------  ------\")\n",
    "for i, ((x,y), s) in enumerate(zip(keypoints_orig, scores_vec)):\n",
    "    nm = HALPE26_NAME.get(i, str(i))\n",
    "    print(f\"{i:>2d}   {nm:17s}  {x:9.2f}  {y:9.2f}  {float(s):6.3f}\")\n",
    "\n",
    "# --- (optional) save CSV next to the image ---\n",
    "csv_path = os.path.splitext(image_path)[0] + \"_halpe26_xy.csv\"\n",
    "try:\n",
    "    with open(csv_path, \"w\") as f:\n",
    "        f.write(\"idx,name,x_rgb,y_rgb,score\\n\")\n",
    "        for i, ((x,y), s) in enumerate(zip(keypoints_orig, scores_vec)):\n",
    "            nm = HALPE26_NAME.get(i, str(i))\n",
    "            f.write(f\"{i},{nm},{float(x):.3f},{float(y):.3f},{float(s):.4f}\\n\")\n",
    "    print(f\"\\n[âœ“] wrote {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[!] could not write CSV: {e}\")\n",
    "\n",
    "# --- VISUALIZE (with labels) ---\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "for i, (x, y) in enumerate(keypoints_orig):\n",
    "    plt.gca().add_patch(Circle((x, y), 3, color='red'))\n",
    "    nm = HALPE26_NAME.get(i, str(i))\n",
    "    plt.text(x+3, y-3, f\"{i}:{nm}\", color='yellow', fontsize=7, weight='bold')\n",
    "plt.title(\"Predicted Keypoints (HALPE-26, original RGB frame)\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
