{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7cdf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import UNet2DConditionModel\n",
    "# import torch, torch.fx as fx\n",
    "\n",
    "# CKPT = \"prs-eth/marigold-depth-v1-1\"\n",
    "\n",
    "# # 1️⃣  Disable every fused/compiled attention path\n",
    "# import torch.backends.cuda as bk\n",
    "# bk.enable_flash_sdp(False); bk.enable_mem_efficient_sdp(False)\n",
    "\n",
    "# unet = UNet2DConditionModel.from_pretrained(\n",
    "#           CKPT, subfolder=\"unet\",\n",
    "#           low_cpu_mem_usage=True).cpu()\n",
    "\n",
    "# # diffusers >=0.23 has an explicit helper too\n",
    "# unet.disable_xformers_memory_efficient_attention()   # no-op if xformers absent :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "# # 2️⃣  Optional: treat Transformer blocks as leafs so the graph stays small\n",
    "# class DiffusersTracer(fx.Tracer):\n",
    "#     def is_leaf_module(self, m, qualname):\n",
    "#         if \"Transformer2DModel\" in m.__class__.__name__:\n",
    "#             return True\n",
    "#         return super().is_leaf_module(m, qualname)\n",
    "\n",
    "# tracer = DiffusersTracer()\n",
    "# gm = tracer.trace(unet)\n",
    "\n",
    "# gm.graph.print_tabular()          # quick visual check\n",
    "# gm.recompile()                    # be sure the new GraphModule runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ecb20",
   "metadata": {},
   "source": [
    "## this uses the new p2e prepare and wunatize kep this code for usage but it actualy doesnt server nay ppurpose for now, it jsut shows they full progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import UNet2DConditionModel\n",
    "from torch.export import export\n",
    "from torch.ao.quantization.qconfig_mapping import (\n",
    "    get_default_qat_qconfig_mapping)\n",
    "from torch.ao.quantization.quantize_pt2e import (\n",
    "    prepare_qat_pt2e, convert_pt2e)\n",
    "import torch, inspect\n",
    "from torch.ao.quantization.quantize_pt2e import prepare_qat_pt2e\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(inspect.signature(prepare_qat_pt2e))\n",
    "\n",
    "CKPT = \"prs-eth/marigold-depth-v1-1\"\n",
    "unet = UNet2DConditionModel.from_pretrained(CKPT, subfolder=\"unet\").cpu()\n",
    "unet.disable_xformers_memory_efficient_attention()\n",
    "\n",
    "example = (torch.randn(1, 8, 64, 64),     # latent\n",
    "           torch.tensor([0]),             # timestep\n",
    "           torch.randn(1, 77, 1024))      # text enc\n",
    "\n",
    "gm = export(unet, example)                # records the loop as guards\n",
    "qmap = get_default_qat_qconfig_mapping(\"x86\")\n",
    "gm_qat = prepare_qat_pt2e(gm, qmap)\n",
    "# fine-tune ...\n",
    "gm_int8 = convert_pt2e(gm_qat)\n",
    "gm_int8.save(\"unet_int8.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a728be5",
   "metadata": {},
   "source": [
    "## Torch export for unet graph tracing and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd240e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                                                                     target                                                                   args                                                                                                                                                           kwargs\n",
      "-------------  -----------------------------------------------------------------------  -----------------------------------------------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------  ---------------------------------------------------------------------------\n",
      "placeholder    p_time_embedding_linear_1_weight                                         p_time_embedding_linear_1_weight                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_time_embedding_linear_1_bias                                           p_time_embedding_linear_1_bias                                           ()                                                                                                                                                             {}\n",
      "placeholder    p_time_embedding_linear_2_weight                                         p_time_embedding_linear_2_weight                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_time_embedding_linear_2_bias                                           p_time_embedding_linear_2_bias                                           ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_in_weight                                                         p_conv_in_weight                                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_in_bias                                                           p_conv_in_bias                                                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_norm1_weight                                   p_down_blocks_0_resnets_0_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_norm1_bias                                     p_down_blocks_0_resnets_0_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_conv1_weight                                   p_down_blocks_0_resnets_0_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_conv1_bias                                     p_down_blocks_0_resnets_0_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_time_emb_proj_weight                           p_down_blocks_0_resnets_0_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_time_emb_proj_bias                             p_down_blocks_0_resnets_0_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_norm2_weight                                   p_down_blocks_0_resnets_0_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_norm2_bias                                     p_down_blocks_0_resnets_0_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_conv2_weight                                   p_down_blocks_0_resnets_0_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_conv2_bias                                     p_down_blocks_0_resnets_0_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_norm_weight                                 p_down_blocks_0_attentions_0_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_norm_bias                                   p_down_blocks_0_attentions_0_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_proj_in_weight                              p_down_blocks_0_attentions_0_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_proj_in_bias                                p_down_blocks_0_attentions_0_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight           p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias             p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight           p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias             p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight           p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias             p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight        p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias          p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_proj_out_weight                             p_down_blocks_0_attentions_0_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_proj_out_bias                               p_down_blocks_0_attentions_0_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_norm1_weight                                   p_down_blocks_0_resnets_1_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_norm1_bias                                     p_down_blocks_0_resnets_1_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_conv1_weight                                   p_down_blocks_0_resnets_1_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_conv1_bias                                     p_down_blocks_0_resnets_1_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_time_emb_proj_weight                           p_down_blocks_0_resnets_1_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_time_emb_proj_bias                             p_down_blocks_0_resnets_1_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_norm2_weight                                   p_down_blocks_0_resnets_1_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_norm2_bias                                     p_down_blocks_0_resnets_1_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_conv2_weight                                   p_down_blocks_0_resnets_1_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_conv2_bias                                     p_down_blocks_0_resnets_1_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_norm_weight                                 p_down_blocks_0_attentions_1_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_norm_bias                                   p_down_blocks_0_attentions_1_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_proj_in_weight                              p_down_blocks_0_attentions_1_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_proj_in_bias                                p_down_blocks_0_attentions_1_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight           p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias             p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight           p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias             p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight           p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias             p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight        p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias          p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_proj_out_weight                             p_down_blocks_0_attentions_1_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_proj_out_bias                               p_down_blocks_0_attentions_1_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_downsamplers_0_conv_weight                               p_down_blocks_0_downsamplers_0_conv_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_downsamplers_0_conv_bias                                 p_down_blocks_0_downsamplers_0_conv_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_norm1_weight                                   p_down_blocks_1_resnets_0_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_norm1_bias                                     p_down_blocks_1_resnets_0_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv1_weight                                   p_down_blocks_1_resnets_0_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv1_bias                                     p_down_blocks_1_resnets_0_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_time_emb_proj_weight                           p_down_blocks_1_resnets_0_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_time_emb_proj_bias                             p_down_blocks_1_resnets_0_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_norm2_weight                                   p_down_blocks_1_resnets_0_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_norm2_bias                                     p_down_blocks_1_resnets_0_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv2_weight                                   p_down_blocks_1_resnets_0_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv2_bias                                     p_down_blocks_1_resnets_0_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv_shortcut_weight                           p_down_blocks_1_resnets_0_conv_shortcut_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv_shortcut_bias                             p_down_blocks_1_resnets_0_conv_shortcut_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_norm_weight                                 p_down_blocks_1_attentions_0_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_norm_bias                                   p_down_blocks_1_attentions_0_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_proj_in_weight                              p_down_blocks_1_attentions_0_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_proj_in_bias                                p_down_blocks_1_attentions_0_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight           p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias             p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight           p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias             p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight           p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias             p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight        p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias          p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_proj_out_weight                             p_down_blocks_1_attentions_0_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_proj_out_bias                               p_down_blocks_1_attentions_0_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_norm1_weight                                   p_down_blocks_1_resnets_1_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_norm1_bias                                     p_down_blocks_1_resnets_1_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_conv1_weight                                   p_down_blocks_1_resnets_1_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_conv1_bias                                     p_down_blocks_1_resnets_1_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_time_emb_proj_weight                           p_down_blocks_1_resnets_1_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_time_emb_proj_bias                             p_down_blocks_1_resnets_1_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_norm2_weight                                   p_down_blocks_1_resnets_1_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_norm2_bias                                     p_down_blocks_1_resnets_1_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_conv2_weight                                   p_down_blocks_1_resnets_1_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_conv2_bias                                     p_down_blocks_1_resnets_1_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_norm_weight                                 p_down_blocks_1_attentions_1_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_norm_bias                                   p_down_blocks_1_attentions_1_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_proj_in_weight                              p_down_blocks_1_attentions_1_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_proj_in_bias                                p_down_blocks_1_attentions_1_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight           p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias             p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight           p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias             p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight           p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias             p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight        p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias          p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_proj_out_weight                             p_down_blocks_1_attentions_1_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_proj_out_bias                               p_down_blocks_1_attentions_1_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_downsamplers_0_conv_weight                               p_down_blocks_1_downsamplers_0_conv_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_downsamplers_0_conv_bias                                 p_down_blocks_1_downsamplers_0_conv_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_norm1_weight                                   p_down_blocks_2_resnets_0_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_norm1_bias                                     p_down_blocks_2_resnets_0_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv1_weight                                   p_down_blocks_2_resnets_0_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv1_bias                                     p_down_blocks_2_resnets_0_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_time_emb_proj_weight                           p_down_blocks_2_resnets_0_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_time_emb_proj_bias                             p_down_blocks_2_resnets_0_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_norm2_weight                                   p_down_blocks_2_resnets_0_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_norm2_bias                                     p_down_blocks_2_resnets_0_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv2_weight                                   p_down_blocks_2_resnets_0_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv2_bias                                     p_down_blocks_2_resnets_0_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv_shortcut_weight                           p_down_blocks_2_resnets_0_conv_shortcut_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv_shortcut_bias                             p_down_blocks_2_resnets_0_conv_shortcut_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_norm_weight                                 p_down_blocks_2_attentions_0_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_norm_bias                                   p_down_blocks_2_attentions_0_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_proj_in_weight                              p_down_blocks_2_attentions_0_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_proj_in_bias                                p_down_blocks_2_attentions_0_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight           p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias             p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight           p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias             p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight           p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias             p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight        p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias          p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_proj_out_weight                             p_down_blocks_2_attentions_0_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_proj_out_bias                               p_down_blocks_2_attentions_0_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_norm1_weight                                   p_down_blocks_2_resnets_1_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_norm1_bias                                     p_down_blocks_2_resnets_1_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_conv1_weight                                   p_down_blocks_2_resnets_1_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_conv1_bias                                     p_down_blocks_2_resnets_1_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_time_emb_proj_weight                           p_down_blocks_2_resnets_1_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_time_emb_proj_bias                             p_down_blocks_2_resnets_1_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_norm2_weight                                   p_down_blocks_2_resnets_1_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_norm2_bias                                     p_down_blocks_2_resnets_1_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_conv2_weight                                   p_down_blocks_2_resnets_1_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_conv2_bias                                     p_down_blocks_2_resnets_1_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_norm_weight                                 p_down_blocks_2_attentions_1_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_norm_bias                                   p_down_blocks_2_attentions_1_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_proj_in_weight                              p_down_blocks_2_attentions_1_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_proj_in_bias                                p_down_blocks_2_attentions_1_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight           p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias             p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight           p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias             p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight           p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias             p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight        p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias          p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_proj_out_weight                             p_down_blocks_2_attentions_1_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_proj_out_bias                               p_down_blocks_2_attentions_1_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_downsamplers_0_conv_weight                               p_down_blocks_2_downsamplers_0_conv_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_downsamplers_0_conv_bias                                 p_down_blocks_2_downsamplers_0_conv_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_norm1_weight                                   p_down_blocks_3_resnets_0_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_norm1_bias                                     p_down_blocks_3_resnets_0_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_conv1_weight                                   p_down_blocks_3_resnets_0_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_conv1_bias                                     p_down_blocks_3_resnets_0_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_time_emb_proj_weight                           p_down_blocks_3_resnets_0_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_time_emb_proj_bias                             p_down_blocks_3_resnets_0_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_norm2_weight                                   p_down_blocks_3_resnets_0_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_norm2_bias                                     p_down_blocks_3_resnets_0_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_conv2_weight                                   p_down_blocks_3_resnets_0_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_conv2_bias                                     p_down_blocks_3_resnets_0_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_norm1_weight                                   p_down_blocks_3_resnets_1_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_norm1_bias                                     p_down_blocks_3_resnets_1_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_conv1_weight                                   p_down_blocks_3_resnets_1_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_conv1_bias                                     p_down_blocks_3_resnets_1_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_time_emb_proj_weight                           p_down_blocks_3_resnets_1_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_time_emb_proj_bias                             p_down_blocks_3_resnets_1_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_norm2_weight                                   p_down_blocks_3_resnets_1_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_norm2_bias                                     p_down_blocks_3_resnets_1_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_conv2_weight                                   p_down_blocks_3_resnets_1_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_conv2_bias                                     p_down_blocks_3_resnets_1_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_norm1_weight                                       p_mid_block_resnets_0_norm1_weight                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_norm1_bias                                         p_mid_block_resnets_0_norm1_bias                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_conv1_weight                                       p_mid_block_resnets_0_conv1_weight                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_conv1_bias                                         p_mid_block_resnets_0_conv1_bias                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_time_emb_proj_weight                               p_mid_block_resnets_0_time_emb_proj_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_time_emb_proj_bias                                 p_mid_block_resnets_0_time_emb_proj_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_norm2_weight                                       p_mid_block_resnets_0_norm2_weight                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_norm2_bias                                         p_mid_block_resnets_0_norm2_bias                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_conv2_weight                                       p_mid_block_resnets_0_conv2_weight                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_conv2_bias                                         p_mid_block_resnets_0_conv2_bias                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_norm_weight                                     p_mid_block_attentions_0_norm_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_norm_bias                                       p_mid_block_attentions_0_norm_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_proj_in_weight                                  p_mid_block_attentions_0_proj_in_weight                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_proj_in_bias                                    p_mid_block_attentions_0_proj_in_bias                                    ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm1_weight               p_mid_block_attentions_0_transformer_blocks_0_norm1_weight               ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm1_bias                 p_mid_block_attentions_0_transformer_blocks_0_norm1_bias                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight          p_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight          p_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight          p_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight      p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias        p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias        ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm2_weight               p_mid_block_attentions_0_transformer_blocks_0_norm2_weight               ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm2_bias                 p_mid_block_attentions_0_transformer_blocks_0_norm2_bias                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight          p_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight          p_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight          p_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight      p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias        p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias        ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm3_weight               p_mid_block_attentions_0_transformer_blocks_0_norm3_weight               ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm3_bias                 p_mid_block_attentions_0_transformer_blocks_0_norm3_bias                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight       p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias         p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight            p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight            ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias              p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias              ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_proj_out_weight                                 p_mid_block_attentions_0_proj_out_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_proj_out_bias                                   p_mid_block_attentions_0_proj_out_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_norm1_weight                 p_mid_block_resnets_slice_1__none__none___0_norm1_weight                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_norm1_bias                   p_mid_block_resnets_slice_1__none__none___0_norm1_bias                   ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_conv1_weight                 p_mid_block_resnets_slice_1__none__none___0_conv1_weight                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_conv1_bias                   p_mid_block_resnets_slice_1__none__none___0_conv1_bias                   ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_weight         p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_weight         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_bias           p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_bias           ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_norm2_weight                 p_mid_block_resnets_slice_1__none__none___0_norm2_weight                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_norm2_bias                   p_mid_block_resnets_slice_1__none__none___0_norm2_bias                   ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_conv2_weight                 p_mid_block_resnets_slice_1__none__none___0_conv2_weight                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_conv2_bias                   p_mid_block_resnets_slice_1__none__none___0_conv2_bias                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_norm1_weight                                     p_up_blocks_0_resnets_0_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_norm1_bias                                       p_up_blocks_0_resnets_0_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv1_weight                                     p_up_blocks_0_resnets_0_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv1_bias                                       p_up_blocks_0_resnets_0_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_time_emb_proj_weight                             p_up_blocks_0_resnets_0_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_time_emb_proj_bias                               p_up_blocks_0_resnets_0_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_norm2_weight                                     p_up_blocks_0_resnets_0_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_norm2_bias                                       p_up_blocks_0_resnets_0_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv2_weight                                     p_up_blocks_0_resnets_0_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv2_bias                                       p_up_blocks_0_resnets_0_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv_shortcut_weight                             p_up_blocks_0_resnets_0_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv_shortcut_bias                               p_up_blocks_0_resnets_0_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_norm1_weight                                     p_up_blocks_0_resnets_1_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_norm1_bias                                       p_up_blocks_0_resnets_1_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv1_weight                                     p_up_blocks_0_resnets_1_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv1_bias                                       p_up_blocks_0_resnets_1_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_time_emb_proj_weight                             p_up_blocks_0_resnets_1_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_time_emb_proj_bias                               p_up_blocks_0_resnets_1_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_norm2_weight                                     p_up_blocks_0_resnets_1_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_norm2_bias                                       p_up_blocks_0_resnets_1_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv2_weight                                     p_up_blocks_0_resnets_1_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv2_bias                                       p_up_blocks_0_resnets_1_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv_shortcut_weight                             p_up_blocks_0_resnets_1_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv_shortcut_bias                               p_up_blocks_0_resnets_1_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_norm1_weight                                     p_up_blocks_0_resnets_2_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_norm1_bias                                       p_up_blocks_0_resnets_2_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv1_weight                                     p_up_blocks_0_resnets_2_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv1_bias                                       p_up_blocks_0_resnets_2_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_time_emb_proj_weight                             p_up_blocks_0_resnets_2_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_time_emb_proj_bias                               p_up_blocks_0_resnets_2_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_norm2_weight                                     p_up_blocks_0_resnets_2_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_norm2_bias                                       p_up_blocks_0_resnets_2_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv2_weight                                     p_up_blocks_0_resnets_2_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv2_bias                                       p_up_blocks_0_resnets_2_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv_shortcut_weight                             p_up_blocks_0_resnets_2_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv_shortcut_bias                               p_up_blocks_0_resnets_2_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_upsamplers_0_conv_weight                                   p_up_blocks_0_upsamplers_0_conv_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_upsamplers_0_conv_bias                                     p_up_blocks_0_upsamplers_0_conv_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_norm1_weight                                     p_up_blocks_1_resnets_0_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_norm1_bias                                       p_up_blocks_1_resnets_0_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv1_weight                                     p_up_blocks_1_resnets_0_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv1_bias                                       p_up_blocks_1_resnets_0_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_time_emb_proj_weight                             p_up_blocks_1_resnets_0_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_time_emb_proj_bias                               p_up_blocks_1_resnets_0_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_norm2_weight                                     p_up_blocks_1_resnets_0_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_norm2_bias                                       p_up_blocks_1_resnets_0_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv2_weight                                     p_up_blocks_1_resnets_0_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv2_bias                                       p_up_blocks_1_resnets_0_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv_shortcut_weight                             p_up_blocks_1_resnets_0_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv_shortcut_bias                               p_up_blocks_1_resnets_0_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_norm_weight                                   p_up_blocks_1_attentions_0_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_norm_bias                                     p_up_blocks_1_attentions_0_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_proj_in_weight                                p_up_blocks_1_attentions_0_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_proj_in_bias                                  p_up_blocks_1_attentions_0_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight             p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias               p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight             p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias               p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight             p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias               p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight          p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias            p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_proj_out_weight                               p_up_blocks_1_attentions_0_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_proj_out_bias                                 p_up_blocks_1_attentions_0_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_norm1_weight                                     p_up_blocks_1_resnets_1_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_norm1_bias                                       p_up_blocks_1_resnets_1_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv1_weight                                     p_up_blocks_1_resnets_1_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv1_bias                                       p_up_blocks_1_resnets_1_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_time_emb_proj_weight                             p_up_blocks_1_resnets_1_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_time_emb_proj_bias                               p_up_blocks_1_resnets_1_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_norm2_weight                                     p_up_blocks_1_resnets_1_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_norm2_bias                                       p_up_blocks_1_resnets_1_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv2_weight                                     p_up_blocks_1_resnets_1_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv2_bias                                       p_up_blocks_1_resnets_1_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv_shortcut_weight                             p_up_blocks_1_resnets_1_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv_shortcut_bias                               p_up_blocks_1_resnets_1_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_norm_weight                                   p_up_blocks_1_attentions_1_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_norm_bias                                     p_up_blocks_1_attentions_1_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_proj_in_weight                                p_up_blocks_1_attentions_1_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_proj_in_bias                                  p_up_blocks_1_attentions_1_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight             p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias               p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight             p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias               p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight             p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias               p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight          p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias            p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_proj_out_weight                               p_up_blocks_1_attentions_1_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_proj_out_bias                                 p_up_blocks_1_attentions_1_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_norm1_weight                                     p_up_blocks_1_resnets_2_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_norm1_bias                                       p_up_blocks_1_resnets_2_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv1_weight                                     p_up_blocks_1_resnets_2_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv1_bias                                       p_up_blocks_1_resnets_2_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_time_emb_proj_weight                             p_up_blocks_1_resnets_2_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_time_emb_proj_bias                               p_up_blocks_1_resnets_2_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_norm2_weight                                     p_up_blocks_1_resnets_2_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_norm2_bias                                       p_up_blocks_1_resnets_2_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv2_weight                                     p_up_blocks_1_resnets_2_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv2_bias                                       p_up_blocks_1_resnets_2_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv_shortcut_weight                             p_up_blocks_1_resnets_2_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv_shortcut_bias                               p_up_blocks_1_resnets_2_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_norm_weight                                   p_up_blocks_1_attentions_2_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_norm_bias                                     p_up_blocks_1_attentions_2_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_proj_in_weight                                p_up_blocks_1_attentions_2_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_proj_in_bias                                  p_up_blocks_1_attentions_2_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight             p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias               p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight             p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias               p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight             p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias               p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight          p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias            p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_proj_out_weight                               p_up_blocks_1_attentions_2_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_proj_out_bias                                 p_up_blocks_1_attentions_2_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_upsamplers_0_conv_weight                                   p_up_blocks_1_upsamplers_0_conv_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_upsamplers_0_conv_bias                                     p_up_blocks_1_upsamplers_0_conv_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_norm1_weight                                     p_up_blocks_2_resnets_0_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_norm1_bias                                       p_up_blocks_2_resnets_0_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv1_weight                                     p_up_blocks_2_resnets_0_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv1_bias                                       p_up_blocks_2_resnets_0_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_time_emb_proj_weight                             p_up_blocks_2_resnets_0_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_time_emb_proj_bias                               p_up_blocks_2_resnets_0_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_norm2_weight                                     p_up_blocks_2_resnets_0_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_norm2_bias                                       p_up_blocks_2_resnets_0_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv2_weight                                     p_up_blocks_2_resnets_0_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv2_bias                                       p_up_blocks_2_resnets_0_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv_shortcut_weight                             p_up_blocks_2_resnets_0_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv_shortcut_bias                               p_up_blocks_2_resnets_0_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_norm_weight                                   p_up_blocks_2_attentions_0_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_norm_bias                                     p_up_blocks_2_attentions_0_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_proj_in_weight                                p_up_blocks_2_attentions_0_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_proj_in_bias                                  p_up_blocks_2_attentions_0_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight             p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias               p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight             p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias               p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight             p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias               p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight          p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias            p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_proj_out_weight                               p_up_blocks_2_attentions_0_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_proj_out_bias                                 p_up_blocks_2_attentions_0_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_norm1_weight                                     p_up_blocks_2_resnets_1_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_norm1_bias                                       p_up_blocks_2_resnets_1_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv1_weight                                     p_up_blocks_2_resnets_1_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv1_bias                                       p_up_blocks_2_resnets_1_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_time_emb_proj_weight                             p_up_blocks_2_resnets_1_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_time_emb_proj_bias                               p_up_blocks_2_resnets_1_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_norm2_weight                                     p_up_blocks_2_resnets_1_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_norm2_bias                                       p_up_blocks_2_resnets_1_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv2_weight                                     p_up_blocks_2_resnets_1_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv2_bias                                       p_up_blocks_2_resnets_1_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv_shortcut_weight                             p_up_blocks_2_resnets_1_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv_shortcut_bias                               p_up_blocks_2_resnets_1_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_norm_weight                                   p_up_blocks_2_attentions_1_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_norm_bias                                     p_up_blocks_2_attentions_1_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_proj_in_weight                                p_up_blocks_2_attentions_1_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_proj_in_bias                                  p_up_blocks_2_attentions_1_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight             p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias               p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight             p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias               p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight             p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias               p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight          p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias            p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_proj_out_weight                               p_up_blocks_2_attentions_1_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_proj_out_bias                                 p_up_blocks_2_attentions_1_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_norm1_weight                                     p_up_blocks_2_resnets_2_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_norm1_bias                                       p_up_blocks_2_resnets_2_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv1_weight                                     p_up_blocks_2_resnets_2_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv1_bias                                       p_up_blocks_2_resnets_2_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_time_emb_proj_weight                             p_up_blocks_2_resnets_2_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_time_emb_proj_bias                               p_up_blocks_2_resnets_2_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_norm2_weight                                     p_up_blocks_2_resnets_2_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_norm2_bias                                       p_up_blocks_2_resnets_2_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv2_weight                                     p_up_blocks_2_resnets_2_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv2_bias                                       p_up_blocks_2_resnets_2_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv_shortcut_weight                             p_up_blocks_2_resnets_2_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv_shortcut_bias                               p_up_blocks_2_resnets_2_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_norm_weight                                   p_up_blocks_2_attentions_2_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_norm_bias                                     p_up_blocks_2_attentions_2_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_proj_in_weight                                p_up_blocks_2_attentions_2_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_proj_in_bias                                  p_up_blocks_2_attentions_2_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight             p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias               p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight             p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias               p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight             p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias               p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight          p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias            p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_proj_out_weight                               p_up_blocks_2_attentions_2_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_proj_out_bias                                 p_up_blocks_2_attentions_2_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_upsamplers_0_conv_weight                                   p_up_blocks_2_upsamplers_0_conv_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_upsamplers_0_conv_bias                                     p_up_blocks_2_upsamplers_0_conv_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_norm1_weight                                     p_up_blocks_3_resnets_0_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_norm1_bias                                       p_up_blocks_3_resnets_0_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv1_weight                                     p_up_blocks_3_resnets_0_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv1_bias                                       p_up_blocks_3_resnets_0_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_time_emb_proj_weight                             p_up_blocks_3_resnets_0_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_time_emb_proj_bias                               p_up_blocks_3_resnets_0_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_norm2_weight                                     p_up_blocks_3_resnets_0_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_norm2_bias                                       p_up_blocks_3_resnets_0_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv2_weight                                     p_up_blocks_3_resnets_0_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv2_bias                                       p_up_blocks_3_resnets_0_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv_shortcut_weight                             p_up_blocks_3_resnets_0_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv_shortcut_bias                               p_up_blocks_3_resnets_0_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_norm_weight                                   p_up_blocks_3_attentions_0_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_norm_bias                                     p_up_blocks_3_attentions_0_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_proj_in_weight                                p_up_blocks_3_attentions_0_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_proj_in_bias                                  p_up_blocks_3_attentions_0_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight             p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias               p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight             p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias               p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight             p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias               p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight          p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias            p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_proj_out_weight                               p_up_blocks_3_attentions_0_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_proj_out_bias                                 p_up_blocks_3_attentions_0_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_norm1_weight                                     p_up_blocks_3_resnets_1_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_norm1_bias                                       p_up_blocks_3_resnets_1_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv1_weight                                     p_up_blocks_3_resnets_1_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv1_bias                                       p_up_blocks_3_resnets_1_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_time_emb_proj_weight                             p_up_blocks_3_resnets_1_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_time_emb_proj_bias                               p_up_blocks_3_resnets_1_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_norm2_weight                                     p_up_blocks_3_resnets_1_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_norm2_bias                                       p_up_blocks_3_resnets_1_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv2_weight                                     p_up_blocks_3_resnets_1_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv2_bias                                       p_up_blocks_3_resnets_1_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv_shortcut_weight                             p_up_blocks_3_resnets_1_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv_shortcut_bias                               p_up_blocks_3_resnets_1_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_norm_weight                                   p_up_blocks_3_attentions_1_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_norm_bias                                     p_up_blocks_3_attentions_1_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_proj_in_weight                                p_up_blocks_3_attentions_1_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_proj_in_bias                                  p_up_blocks_3_attentions_1_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight             p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias               p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight             p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias               p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight             p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias               p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight          p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias            p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_proj_out_weight                               p_up_blocks_3_attentions_1_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_proj_out_bias                                 p_up_blocks_3_attentions_1_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_norm1_weight                                     p_up_blocks_3_resnets_2_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_norm1_bias                                       p_up_blocks_3_resnets_2_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv1_weight                                     p_up_blocks_3_resnets_2_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv1_bias                                       p_up_blocks_3_resnets_2_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_time_emb_proj_weight                             p_up_blocks_3_resnets_2_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_time_emb_proj_bias                               p_up_blocks_3_resnets_2_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_norm2_weight                                     p_up_blocks_3_resnets_2_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_norm2_bias                                       p_up_blocks_3_resnets_2_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv2_weight                                     p_up_blocks_3_resnets_2_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv2_bias                                       p_up_blocks_3_resnets_2_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv_shortcut_weight                             p_up_blocks_3_resnets_2_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv_shortcut_bias                               p_up_blocks_3_resnets_2_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_norm_weight                                   p_up_blocks_3_attentions_2_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_norm_bias                                     p_up_blocks_3_attentions_2_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_proj_in_weight                                p_up_blocks_3_attentions_2_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_proj_in_bias                                  p_up_blocks_3_attentions_2_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight             p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias               p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight             p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias               p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight             p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias               p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight          p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias            p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_proj_out_weight                               p_up_blocks_3_attentions_2_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_proj_out_bias                                 p_up_blocks_3_attentions_2_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_norm_out_weight                                                   p_conv_norm_out_weight                                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_norm_out_bias                                                     p_conv_norm_out_bias                                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_out_weight                                                        p_conv_out_weight                                                        ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_out_bias                                                          p_conv_out_bias                                                          ()                                                                                                                                                             {}\n",
      "placeholder    sample                                                                   sample                                                                   ()                                                                                                                                                             {}\n",
      "placeholder    timestep                                                                 timestep                                                                 ()                                                                                                                                                             {}\n",
      "placeholder    encoder_hidden_states                                                    encoder_hidden_states                                                    ()                                                                                                                                                             {}\n",
      "call_function  expand                                                                   aten.expand.default                                                      (timestep, [1])                                                                                                                                                {}\n",
      "call_function  arange                                                                   aten.arange.start                                                        (0, 160)                                                                                                                                                       {'dtype': torch.float32, 'device': device(type='cpu'), 'pin_memory': False}\n",
      "call_function  mul                                                                      aten.mul.Tensor                                                          (arange, -9.210340371976184)                                                                                                                                   {}\n",
      "call_function  div                                                                      aten.div.Tensor                                                          (mul, 160)                                                                                                                                                     {}\n",
      "call_function  exp                                                                      aten.exp.default                                                         (div,)                                                                                                                                                         {}\n",
      "call_function  slice_1                                                                  aten.slice.Tensor                                                        (expand, 0, 0, 9223372036854775807)                                                                                                                            {}\n",
      "call_function  unsqueeze                                                                aten.unsqueeze.default                                                   (slice_1, 1)                                                                                                                                                   {}\n",
      "call_function  _to_copy                                                                 aten._to_copy.default                                                    (unsqueeze,)                                                                                                                                                   {'dtype': torch.float32}\n",
      "call_function  unsqueeze_1                                                              aten.unsqueeze.default                                                   (exp, 0)                                                                                                                                                       {}\n",
      "call_function  slice_2                                                                  aten.slice.Tensor                                                        (unsqueeze_1, 1, 0, 9223372036854775807)                                                                                                                       {}\n",
      "call_function  mul_1                                                                    aten.mul.Tensor                                                          (_to_copy, slice_2)                                                                                                                                            {}\n",
      "call_function  mul_2                                                                    aten.mul.Tensor                                                          (mul_1, 1)                                                                                                                                                     {}\n",
      "call_function  sin                                                                      aten.sin.default                                                         (mul_2,)                                                                                                                                                       {}\n",
      "call_function  cos                                                                      aten.cos.default                                                         (mul_2,)                                                                                                                                                       {}\n",
      "call_function  cat                                                                      aten.cat.default                                                         ([sin, cos], -1)                                                                                                                                               {}\n",
      "call_function  slice_3                                                                  aten.slice.Tensor                                                        (cat, 0, 0, 9223372036854775807)                                                                                                                               {}\n",
      "call_function  slice_4                                                                  aten.slice.Tensor                                                        (slice_3, 1, 160, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_5                                                                  aten.slice.Tensor                                                        (cat, 0, 0, 9223372036854775807)                                                                                                                               {}\n",
      "call_function  slice_6                                                                  aten.slice.Tensor                                                        (slice_5, 1, 0, 160)                                                                                                                                           {}\n",
      "call_function  cat_1                                                                    aten.cat.default                                                         ([slice_4, slice_6], -1)                                                                                                                                       {}\n",
      "call_function  _to_copy_1                                                               aten._to_copy.default                                                    (cat_1,)                                                                                                                                                       {'dtype': torch.float32}\n",
      "call_function  linear                                                                   aten.linear.default                                                      (_to_copy_1, p_time_embedding_linear_1_weight, p_time_embedding_linear_1_bias)                                                                                 {}\n",
      "call_function  silu                                                                     aten.silu.default                                                        (linear,)                                                                                                                                                      {}\n",
      "call_function  linear_1                                                                 aten.linear.default                                                      (silu, p_time_embedding_linear_2_weight, p_time_embedding_linear_2_bias)                                                                                       {}\n",
      "call_function  conv2d                                                                   aten.conv2d.default                                                      (sample, p_conv_in_weight, p_conv_in_bias, [1, 1], [1, 1])                                                                                                     {}\n",
      "call_function  group_norm                                                               aten.group_norm.default                                                  (conv2d, 32, p_down_blocks_0_resnets_0_norm1_weight, p_down_blocks_0_resnets_0_norm1_bias)                                                                     {}\n",
      "call_function  silu_1                                                                   aten.silu.default                                                        (group_norm,)                                                                                                                                                  {}\n",
      "call_function  conv2d_1                                                                 aten.conv2d.default                                                      (silu_1, p_down_blocks_0_resnets_0_conv1_weight, p_down_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  silu_2                                                                   aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_2                                                                 aten.linear.default                                                      (silu_2, p_down_blocks_0_resnets_0_time_emb_proj_weight, p_down_blocks_0_resnets_0_time_emb_proj_bias)                                                         {}\n",
      "call_function  slice_7                                                                  aten.slice.Tensor                                                        (linear_2, 0, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  slice_8                                                                  aten.slice.Tensor                                                        (slice_7, 1, 0, 9223372036854775807)                                                                                                                           {}\n",
      "call_function  unsqueeze_2                                                              aten.unsqueeze.default                                                   (slice_8, 2)                                                                                                                                                   {}\n",
      "call_function  unsqueeze_3                                                              aten.unsqueeze.default                                                   (unsqueeze_2, 3)                                                                                                                                               {}\n",
      "call_function  add                                                                      aten.add.Tensor                                                          (conv2d_1, unsqueeze_3)                                                                                                                                        {}\n",
      "call_function  group_norm_1                                                             aten.group_norm.default                                                  (add, 32, p_down_blocks_0_resnets_0_norm2_weight, p_down_blocks_0_resnets_0_norm2_bias)                                                                        {}\n",
      "call_function  silu_3                                                                   aten.silu.default                                                        (group_norm_1,)                                                                                                                                                {}\n",
      "call_function  dropout                                                                  aten.dropout.default                                                     (silu_3, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_2                                                                 aten.conv2d.default                                                      (dropout, p_down_blocks_0_resnets_0_conv2_weight, p_down_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  add_1                                                                    aten.add.Tensor                                                          (conv2d, conv2d_2)                                                                                                                                             {}\n",
      "call_function  div_1                                                                    aten.div.Tensor                                                          (add_1, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_2                                                             aten.group_norm.default                                                  (div_1, 32, p_down_blocks_0_attentions_0_norm_weight, p_down_blocks_0_attentions_0_norm_bias, 1e-06)                                                           {}\n",
      "call_function  permute                                                                  aten.permute.default                                                     (group_norm_2, [0, 2, 3, 1])                                                                                                                                   {}\n",
      "call_function  view                                                                     aten.view.default                                                        (permute, [1, 4096, 320])                                                                                                                                      {}\n",
      "call_function  linear_3                                                                 aten.linear.default                                                      (view, p_down_blocks_0_attentions_0_proj_in_weight, p_down_blocks_0_attentions_0_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm                                                               aten.layer_norm.default                                                  (linear_3, [320], p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias)                {}\n",
      "call_function  linear_4                                                                 aten.linear.default                                                      (layer_norm, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                              {}\n",
      "call_function  linear_5                                                                 aten.linear.default                                                      (layer_norm, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                              {}\n",
      "call_function  linear_6                                                                 aten.linear.default                                                      (layer_norm, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                              {}\n",
      "call_function  view_1                                                                   aten.view.default                                                        (linear_4, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose                                                                aten.transpose.int                                                       (view_1, 1, 2)                                                                                                                                                 {}\n",
      "call_function  view_2                                                                   aten.view.default                                                        (linear_5, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose_1                                                              aten.transpose.int                                                       (view_2, 1, 2)                                                                                                                                                 {}\n",
      "call_function  view_3                                                                   aten.view.default                                                        (linear_6, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose_2                                                              aten.transpose.int                                                       (view_3, 1, 2)                                                                                                                                                 {}\n",
      "call_function  scaled_dot_product_attention                                             aten.scaled_dot_product_attention.default                                (transpose, transpose_1, transpose_2)                                                                                                                          {}\n",
      "call_function  transpose_3                                                              aten.transpose.int                                                       (scaled_dot_product_attention, 1, 2)                                                                                                                           {}\n",
      "call_function  view_4                                                                   aten.view.default                                                        (transpose_3, [1, -1, 320])                                                                                                                                    {}\n",
      "call_function  _to_copy_2                                                               aten._to_copy.default                                                    (view_4,)                                                                                                                                                      {'dtype': torch.float32}\n",
      "call_function  linear_7                                                                 aten.linear.default                                                      (_to_copy_2, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)   {}\n",
      "call_function  dropout_1                                                                aten.dropout.default                                                     (linear_7, 0.0, False)                                                                                                                                         {}\n",
      "call_function  div_2                                                                    aten.div.Tensor                                                          (dropout_1, 1.0)                                                                                                                                               {}\n",
      "call_function  add_2                                                                    aten.add.Tensor                                                          (div_2, linear_3)                                                                                                                                              {}\n",
      "call_function  layer_norm_1                                                             aten.layer_norm.default                                                  (add_2, [320], p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias)                   {}\n",
      "call_function  linear_8                                                                 aten.linear.default                                                      (layer_norm_1, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                            {}\n",
      "call_function  linear_9                                                                 aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_10                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_5                                                                   aten.view.default                                                        (linear_8, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose_4                                                              aten.transpose.int                                                       (view_5, 1, 2)                                                                                                                                                 {}\n",
      "call_function  view_6                                                                   aten.view.default                                                        (linear_9, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose_5                                                              aten.transpose.int                                                       (view_6, 1, 2)                                                                                                                                                 {}\n",
      "call_function  view_7                                                                   aten.view.default                                                        (linear_10, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_6                                                              aten.transpose.int                                                       (view_7, 1, 2)                                                                                                                                                 {}\n",
      "call_function  scaled_dot_product_attention_1                                           aten.scaled_dot_product_attention.default                                (transpose_4, transpose_5, transpose_6)                                                                                                                        {}\n",
      "call_function  transpose_7                                                              aten.transpose.int                                                       (scaled_dot_product_attention_1, 1, 2)                                                                                                                         {}\n",
      "call_function  view_8                                                                   aten.view.default                                                        (transpose_7, [1, -1, 320])                                                                                                                                    {}\n",
      "call_function  _to_copy_3                                                               aten._to_copy.default                                                    (view_8,)                                                                                                                                                      {'dtype': torch.float32}\n",
      "call_function  linear_11                                                                aten.linear.default                                                      (_to_copy_3, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)   {}\n",
      "call_function  dropout_2                                                                aten.dropout.default                                                     (linear_11, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_3                                                                    aten.div.Tensor                                                          (dropout_2, 1.0)                                                                                                                                               {}\n",
      "call_function  add_3                                                                    aten.add.Tensor                                                          (div_3, add_2)                                                                                                                                                 {}\n",
      "call_function  layer_norm_2                                                             aten.layer_norm.default                                                  (add_3, [320], p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias)                   {}\n",
      "call_function  linear_12                                                                aten.linear.default                                                      (layer_norm_2, p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)   {}\n",
      "call_function  split                                                                    aten.split.Tensor                                                        (linear_12, 1280, -1)                                                                                                                                          {}\n",
      "call_function  getitem                                                                  <built-in function getitem>                                              (split, 0)                                                                                                                                                     {}\n",
      "call_function  getitem_1                                                                <built-in function getitem>                                              (split, 1)                                                                                                                                                     {}\n",
      "call_function  gelu                                                                     aten.gelu.default                                                        (getitem_1,)                                                                                                                                                   {}\n",
      "call_function  mul_3                                                                    aten.mul.Tensor                                                          (getitem, gelu)                                                                                                                                                {}\n",
      "call_function  dropout_3                                                                aten.dropout.default                                                     (mul_3, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_13                                                                aten.linear.default                                                      (dropout_3, p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias)                {}\n",
      "call_function  add_4                                                                    aten.add.Tensor                                                          (linear_13, add_3)                                                                                                                                             {}\n",
      "call_function  linear_14                                                                aten.linear.default                                                      (add_4, p_down_blocks_0_attentions_0_proj_out_weight, p_down_blocks_0_attentions_0_proj_out_bias)                                                              {}\n",
      "call_function  view_9                                                                   aten.view.default                                                        (linear_14, [1, 64, 64, 320])                                                                                                                                  {}\n",
      "call_function  permute_1                                                                aten.permute.default                                                     (view_9, [0, 3, 1, 2])                                                                                                                                         {}\n",
      "call_function  clone                                                                    aten.clone.default                                                       (permute_1,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_5                                                                    aten.add.Tensor                                                          (clone, div_1)                                                                                                                                                 {}\n",
      "call_function  group_norm_3                                                             aten.group_norm.default                                                  (add_5, 32, p_down_blocks_0_resnets_1_norm1_weight, p_down_blocks_0_resnets_1_norm1_bias)                                                                      {}\n",
      "call_function  silu_4                                                                   aten.silu.default                                                        (group_norm_3,)                                                                                                                                                {}\n",
      "call_function  conv2d_3                                                                 aten.conv2d.default                                                      (silu_4, p_down_blocks_0_resnets_1_conv1_weight, p_down_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  silu_5                                                                   aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_15                                                                aten.linear.default                                                      (silu_5, p_down_blocks_0_resnets_1_time_emb_proj_weight, p_down_blocks_0_resnets_1_time_emb_proj_bias)                                                         {}\n",
      "call_function  slice_9                                                                  aten.slice.Tensor                                                        (linear_15, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_10                                                                 aten.slice.Tensor                                                        (slice_9, 1, 0, 9223372036854775807)                                                                                                                           {}\n",
      "call_function  unsqueeze_4                                                              aten.unsqueeze.default                                                   (slice_10, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_5                                                              aten.unsqueeze.default                                                   (unsqueeze_4, 3)                                                                                                                                               {}\n",
      "call_function  add_6                                                                    aten.add.Tensor                                                          (conv2d_3, unsqueeze_5)                                                                                                                                        {}\n",
      "call_function  group_norm_4                                                             aten.group_norm.default                                                  (add_6, 32, p_down_blocks_0_resnets_1_norm2_weight, p_down_blocks_0_resnets_1_norm2_bias)                                                                      {}\n",
      "call_function  silu_6                                                                   aten.silu.default                                                        (group_norm_4,)                                                                                                                                                {}\n",
      "call_function  dropout_4                                                                aten.dropout.default                                                     (silu_6, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_4                                                                 aten.conv2d.default                                                      (dropout_4, p_down_blocks_0_resnets_1_conv2_weight, p_down_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1])                                                      {}\n",
      "call_function  add_7                                                                    aten.add.Tensor                                                          (add_5, conv2d_4)                                                                                                                                              {}\n",
      "call_function  div_4                                                                    aten.div.Tensor                                                          (add_7, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_5                                                             aten.group_norm.default                                                  (div_4, 32, p_down_blocks_0_attentions_1_norm_weight, p_down_blocks_0_attentions_1_norm_bias, 1e-06)                                                           {}\n",
      "call_function  permute_2                                                                aten.permute.default                                                     (group_norm_5, [0, 2, 3, 1])                                                                                                                                   {}\n",
      "call_function  view_10                                                                  aten.view.default                                                        (permute_2, [1, 4096, 320])                                                                                                                                    {}\n",
      "call_function  linear_16                                                                aten.linear.default                                                      (view_10, p_down_blocks_0_attentions_1_proj_in_weight, p_down_blocks_0_attentions_1_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_3                                                             aten.layer_norm.default                                                  (linear_16, [320], p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias)               {}\n",
      "call_function  linear_17                                                                aten.linear.default                                                      (layer_norm_3, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                            {}\n",
      "call_function  linear_18                                                                aten.linear.default                                                      (layer_norm_3, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                            {}\n",
      "call_function  linear_19                                                                aten.linear.default                                                      (layer_norm_3, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                            {}\n",
      "call_function  view_11                                                                  aten.view.default                                                        (linear_17, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_8                                                              aten.transpose.int                                                       (view_11, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_12                                                                  aten.view.default                                                        (linear_18, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_9                                                              aten.transpose.int                                                       (view_12, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_13                                                                  aten.view.default                                                        (linear_19, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_10                                                             aten.transpose.int                                                       (view_13, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_2                                           aten.scaled_dot_product_attention.default                                (transpose_8, transpose_9, transpose_10)                                                                                                                       {}\n",
      "call_function  transpose_11                                                             aten.transpose.int                                                       (scaled_dot_product_attention_2, 1, 2)                                                                                                                         {}\n",
      "call_function  view_14                                                                  aten.view.default                                                        (transpose_11, [1, -1, 320])                                                                                                                                   {}\n",
      "call_function  _to_copy_4                                                               aten._to_copy.default                                                    (view_14,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_20                                                                aten.linear.default                                                      (_to_copy_4, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)   {}\n",
      "call_function  dropout_5                                                                aten.dropout.default                                                     (linear_20, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_5                                                                    aten.div.Tensor                                                          (dropout_5, 1.0)                                                                                                                                               {}\n",
      "call_function  add_8                                                                    aten.add.Tensor                                                          (div_5, linear_16)                                                                                                                                             {}\n",
      "call_function  layer_norm_4                                                             aten.layer_norm.default                                                  (add_8, [320], p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias)                   {}\n",
      "call_function  linear_21                                                                aten.linear.default                                                      (layer_norm_4, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                            {}\n",
      "call_function  linear_22                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_23                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_15                                                                  aten.view.default                                                        (linear_21, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_12                                                             aten.transpose.int                                                       (view_15, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_16                                                                  aten.view.default                                                        (linear_22, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_13                                                             aten.transpose.int                                                       (view_16, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_17                                                                  aten.view.default                                                        (linear_23, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_14                                                             aten.transpose.int                                                       (view_17, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_3                                           aten.scaled_dot_product_attention.default                                (transpose_12, transpose_13, transpose_14)                                                                                                                     {}\n",
      "call_function  transpose_15                                                             aten.transpose.int                                                       (scaled_dot_product_attention_3, 1, 2)                                                                                                                         {}\n",
      "call_function  view_18                                                                  aten.view.default                                                        (transpose_15, [1, -1, 320])                                                                                                                                   {}\n",
      "call_function  _to_copy_5                                                               aten._to_copy.default                                                    (view_18,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_24                                                                aten.linear.default                                                      (_to_copy_5, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)   {}\n",
      "call_function  dropout_6                                                                aten.dropout.default                                                     (linear_24, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_6                                                                    aten.div.Tensor                                                          (dropout_6, 1.0)                                                                                                                                               {}\n",
      "call_function  add_9                                                                    aten.add.Tensor                                                          (div_6, add_8)                                                                                                                                                 {}\n",
      "call_function  layer_norm_5                                                             aten.layer_norm.default                                                  (add_9, [320], p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias)                   {}\n",
      "call_function  linear_25                                                                aten.linear.default                                                      (layer_norm_5, p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)   {}\n",
      "call_function  split_1                                                                  aten.split.Tensor                                                        (linear_25, 1280, -1)                                                                                                                                          {}\n",
      "call_function  getitem_2                                                                <built-in function getitem>                                              (split_1, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_3                                                                <built-in function getitem>                                              (split_1, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_1                                                                   aten.gelu.default                                                        (getitem_3,)                                                                                                                                                   {}\n",
      "call_function  mul_4                                                                    aten.mul.Tensor                                                          (getitem_2, gelu_1)                                                                                                                                            {}\n",
      "call_function  dropout_7                                                                aten.dropout.default                                                     (mul_4, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_26                                                                aten.linear.default                                                      (dropout_7, p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias)                {}\n",
      "call_function  add_10                                                                   aten.add.Tensor                                                          (linear_26, add_9)                                                                                                                                             {}\n",
      "call_function  linear_27                                                                aten.linear.default                                                      (add_10, p_down_blocks_0_attentions_1_proj_out_weight, p_down_blocks_0_attentions_1_proj_out_bias)                                                             {}\n",
      "call_function  view_19                                                                  aten.view.default                                                        (linear_27, [1, 64, 64, 320])                                                                                                                                  {}\n",
      "call_function  permute_3                                                                aten.permute.default                                                     (view_19, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_1                                                                  aten.clone.default                                                       (permute_3,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_11                                                                   aten.add.Tensor                                                          (clone_1, div_4)                                                                                                                                               {}\n",
      "call_function  conv2d_5                                                                 aten.conv2d.default                                                      (add_11, p_down_blocks_0_downsamplers_0_conv_weight, p_down_blocks_0_downsamplers_0_conv_bias, [2, 2], [1, 1])                                                 {}\n",
      "call_function  group_norm_6                                                             aten.group_norm.default                                                  (conv2d_5, 32, p_down_blocks_1_resnets_0_norm1_weight, p_down_blocks_1_resnets_0_norm1_bias)                                                                   {}\n",
      "call_function  silu_7                                                                   aten.silu.default                                                        (group_norm_6,)                                                                                                                                                {}\n",
      "call_function  conv2d_6                                                                 aten.conv2d.default                                                      (silu_7, p_down_blocks_1_resnets_0_conv1_weight, p_down_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  silu_8                                                                   aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_28                                                                aten.linear.default                                                      (silu_8, p_down_blocks_1_resnets_0_time_emb_proj_weight, p_down_blocks_1_resnets_0_time_emb_proj_bias)                                                         {}\n",
      "call_function  slice_11                                                                 aten.slice.Tensor                                                        (linear_28, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_12                                                                 aten.slice.Tensor                                                        (slice_11, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_6                                                              aten.unsqueeze.default                                                   (slice_12, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_7                                                              aten.unsqueeze.default                                                   (unsqueeze_6, 3)                                                                                                                                               {}\n",
      "call_function  add_12                                                                   aten.add.Tensor                                                          (conv2d_6, unsqueeze_7)                                                                                                                                        {}\n",
      "call_function  group_norm_7                                                             aten.group_norm.default                                                  (add_12, 32, p_down_blocks_1_resnets_0_norm2_weight, p_down_blocks_1_resnets_0_norm2_bias)                                                                     {}\n",
      "call_function  silu_9                                                                   aten.silu.default                                                        (group_norm_7,)                                                                                                                                                {}\n",
      "call_function  dropout_8                                                                aten.dropout.default                                                     (silu_9, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_7                                                                 aten.conv2d.default                                                      (dropout_8, p_down_blocks_1_resnets_0_conv2_weight, p_down_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1])                                                      {}\n",
      "call_function  conv2d_8                                                                 aten.conv2d.default                                                      (conv2d_5, p_down_blocks_1_resnets_0_conv_shortcut_weight, p_down_blocks_1_resnets_0_conv_shortcut_bias)                                                       {}\n",
      "call_function  add_13                                                                   aten.add.Tensor                                                          (conv2d_8, conv2d_7)                                                                                                                                           {}\n",
      "call_function  div_7                                                                    aten.div.Tensor                                                          (add_13, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_8                                                             aten.group_norm.default                                                  (div_7, 32, p_down_blocks_1_attentions_0_norm_weight, p_down_blocks_1_attentions_0_norm_bias, 1e-06)                                                           {}\n",
      "call_function  permute_4                                                                aten.permute.default                                                     (group_norm_8, [0, 2, 3, 1])                                                                                                                                   {}\n",
      "call_function  view_20                                                                  aten.view.default                                                        (permute_4, [1, 1024, 640])                                                                                                                                    {}\n",
      "call_function  linear_29                                                                aten.linear.default                                                      (view_20, p_down_blocks_1_attentions_0_proj_in_weight, p_down_blocks_1_attentions_0_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_6                                                             aten.layer_norm.default                                                  (linear_29, [640], p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias)               {}\n",
      "call_function  linear_30                                                                aten.linear.default                                                      (layer_norm_6, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                            {}\n",
      "call_function  linear_31                                                                aten.linear.default                                                      (layer_norm_6, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                            {}\n",
      "call_function  linear_32                                                                aten.linear.default                                                      (layer_norm_6, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                            {}\n",
      "call_function  view_21                                                                  aten.view.default                                                        (linear_30, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_16                                                             aten.transpose.int                                                       (view_21, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_22                                                                  aten.view.default                                                        (linear_31, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_17                                                             aten.transpose.int                                                       (view_22, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_23                                                                  aten.view.default                                                        (linear_32, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_18                                                             aten.transpose.int                                                       (view_23, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_4                                           aten.scaled_dot_product_attention.default                                (transpose_16, transpose_17, transpose_18)                                                                                                                     {}\n",
      "call_function  transpose_19                                                             aten.transpose.int                                                       (scaled_dot_product_attention_4, 1, 2)                                                                                                                         {}\n",
      "call_function  view_24                                                                  aten.view.default                                                        (transpose_19, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_6                                                               aten._to_copy.default                                                    (view_24,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_33                                                                aten.linear.default                                                      (_to_copy_6, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)   {}\n",
      "call_function  dropout_9                                                                aten.dropout.default                                                     (linear_33, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_8                                                                    aten.div.Tensor                                                          (dropout_9, 1.0)                                                                                                                                               {}\n",
      "call_function  add_14                                                                   aten.add.Tensor                                                          (div_8, linear_29)                                                                                                                                             {}\n",
      "call_function  layer_norm_7                                                             aten.layer_norm.default                                                  (add_14, [640], p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias)                  {}\n",
      "call_function  linear_34                                                                aten.linear.default                                                      (layer_norm_7, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                            {}\n",
      "call_function  linear_35                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_36                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_25                                                                  aten.view.default                                                        (linear_34, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_20                                                             aten.transpose.int                                                       (view_25, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_26                                                                  aten.view.default                                                        (linear_35, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_21                                                             aten.transpose.int                                                       (view_26, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_27                                                                  aten.view.default                                                        (linear_36, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_22                                                             aten.transpose.int                                                       (view_27, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_5                                           aten.scaled_dot_product_attention.default                                (transpose_20, transpose_21, transpose_22)                                                                                                                     {}\n",
      "call_function  transpose_23                                                             aten.transpose.int                                                       (scaled_dot_product_attention_5, 1, 2)                                                                                                                         {}\n",
      "call_function  view_28                                                                  aten.view.default                                                        (transpose_23, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_7                                                               aten._to_copy.default                                                    (view_28,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_37                                                                aten.linear.default                                                      (_to_copy_7, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)   {}\n",
      "call_function  dropout_10                                                               aten.dropout.default                                                     (linear_37, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_9                                                                    aten.div.Tensor                                                          (dropout_10, 1.0)                                                                                                                                              {}\n",
      "call_function  add_15                                                                   aten.add.Tensor                                                          (div_9, add_14)                                                                                                                                                {}\n",
      "call_function  layer_norm_8                                                             aten.layer_norm.default                                                  (add_15, [640], p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias)                  {}\n",
      "call_function  linear_38                                                                aten.linear.default                                                      (layer_norm_8, p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)   {}\n",
      "call_function  split_2                                                                  aten.split.Tensor                                                        (linear_38, 2560, -1)                                                                                                                                          {}\n",
      "call_function  getitem_4                                                                <built-in function getitem>                                              (split_2, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_5                                                                <built-in function getitem>                                              (split_2, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_2                                                                   aten.gelu.default                                                        (getitem_5,)                                                                                                                                                   {}\n",
      "call_function  mul_5                                                                    aten.mul.Tensor                                                          (getitem_4, gelu_2)                                                                                                                                            {}\n",
      "call_function  dropout_11                                                               aten.dropout.default                                                     (mul_5, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_39                                                                aten.linear.default                                                      (dropout_11, p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias)               {}\n",
      "call_function  add_16                                                                   aten.add.Tensor                                                          (linear_39, add_15)                                                                                                                                            {}\n",
      "call_function  linear_40                                                                aten.linear.default                                                      (add_16, p_down_blocks_1_attentions_0_proj_out_weight, p_down_blocks_1_attentions_0_proj_out_bias)                                                             {}\n",
      "call_function  view_29                                                                  aten.view.default                                                        (linear_40, [1, 32, 32, 640])                                                                                                                                  {}\n",
      "call_function  permute_5                                                                aten.permute.default                                                     (view_29, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_2                                                                  aten.clone.default                                                       (permute_5,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_17                                                                   aten.add.Tensor                                                          (clone_2, div_7)                                                                                                                                               {}\n",
      "call_function  group_norm_9                                                             aten.group_norm.default                                                  (add_17, 32, p_down_blocks_1_resnets_1_norm1_weight, p_down_blocks_1_resnets_1_norm1_bias)                                                                     {}\n",
      "call_function  silu_10                                                                  aten.silu.default                                                        (group_norm_9,)                                                                                                                                                {}\n",
      "call_function  conv2d_9                                                                 aten.conv2d.default                                                      (silu_10, p_down_blocks_1_resnets_1_conv1_weight, p_down_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_11                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_41                                                                aten.linear.default                                                      (silu_11, p_down_blocks_1_resnets_1_time_emb_proj_weight, p_down_blocks_1_resnets_1_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_13                                                                 aten.slice.Tensor                                                        (linear_41, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_14                                                                 aten.slice.Tensor                                                        (slice_13, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_8                                                              aten.unsqueeze.default                                                   (slice_14, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_9                                                              aten.unsqueeze.default                                                   (unsqueeze_8, 3)                                                                                                                                               {}\n",
      "call_function  add_18                                                                   aten.add.Tensor                                                          (conv2d_9, unsqueeze_9)                                                                                                                                        {}\n",
      "call_function  group_norm_10                                                            aten.group_norm.default                                                  (add_18, 32, p_down_blocks_1_resnets_1_norm2_weight, p_down_blocks_1_resnets_1_norm2_bias)                                                                     {}\n",
      "call_function  silu_12                                                                  aten.silu.default                                                        (group_norm_10,)                                                                                                                                               {}\n",
      "call_function  dropout_12                                                               aten.dropout.default                                                     (silu_12, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_10                                                                aten.conv2d.default                                                      (dropout_12, p_down_blocks_1_resnets_1_conv2_weight, p_down_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  add_19                                                                   aten.add.Tensor                                                          (add_17, conv2d_10)                                                                                                                                            {}\n",
      "call_function  div_10                                                                   aten.div.Tensor                                                          (add_19, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_11                                                            aten.group_norm.default                                                  (div_10, 32, p_down_blocks_1_attentions_1_norm_weight, p_down_blocks_1_attentions_1_norm_bias, 1e-06)                                                          {}\n",
      "call_function  permute_6                                                                aten.permute.default                                                     (group_norm_11, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_30                                                                  aten.view.default                                                        (permute_6, [1, 1024, 640])                                                                                                                                    {}\n",
      "call_function  linear_42                                                                aten.linear.default                                                      (view_30, p_down_blocks_1_attentions_1_proj_in_weight, p_down_blocks_1_attentions_1_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_9                                                             aten.layer_norm.default                                                  (linear_42, [640], p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias)               {}\n",
      "call_function  linear_43                                                                aten.linear.default                                                      (layer_norm_9, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                            {}\n",
      "call_function  linear_44                                                                aten.linear.default                                                      (layer_norm_9, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                            {}\n",
      "call_function  linear_45                                                                aten.linear.default                                                      (layer_norm_9, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                            {}\n",
      "call_function  view_31                                                                  aten.view.default                                                        (linear_43, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_24                                                             aten.transpose.int                                                       (view_31, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_32                                                                  aten.view.default                                                        (linear_44, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_25                                                             aten.transpose.int                                                       (view_32, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_33                                                                  aten.view.default                                                        (linear_45, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_26                                                             aten.transpose.int                                                       (view_33, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_6                                           aten.scaled_dot_product_attention.default                                (transpose_24, transpose_25, transpose_26)                                                                                                                     {}\n",
      "call_function  transpose_27                                                             aten.transpose.int                                                       (scaled_dot_product_attention_6, 1, 2)                                                                                                                         {}\n",
      "call_function  view_34                                                                  aten.view.default                                                        (transpose_27, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_8                                                               aten._to_copy.default                                                    (view_34,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_46                                                                aten.linear.default                                                      (_to_copy_8, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)   {}\n",
      "call_function  dropout_13                                                               aten.dropout.default                                                     (linear_46, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_11                                                                   aten.div.Tensor                                                          (dropout_13, 1.0)                                                                                                                                              {}\n",
      "call_function  add_20                                                                   aten.add.Tensor                                                          (div_11, linear_42)                                                                                                                                            {}\n",
      "call_function  layer_norm_10                                                            aten.layer_norm.default                                                  (add_20, [640], p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias)                  {}\n",
      "call_function  linear_47                                                                aten.linear.default                                                      (layer_norm_10, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                           {}\n",
      "call_function  linear_48                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_49                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_35                                                                  aten.view.default                                                        (linear_47, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_28                                                             aten.transpose.int                                                       (view_35, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_36                                                                  aten.view.default                                                        (linear_48, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_29                                                             aten.transpose.int                                                       (view_36, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_37                                                                  aten.view.default                                                        (linear_49, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_30                                                             aten.transpose.int                                                       (view_37, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_7                                           aten.scaled_dot_product_attention.default                                (transpose_28, transpose_29, transpose_30)                                                                                                                     {}\n",
      "call_function  transpose_31                                                             aten.transpose.int                                                       (scaled_dot_product_attention_7, 1, 2)                                                                                                                         {}\n",
      "call_function  view_38                                                                  aten.view.default                                                        (transpose_31, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_9                                                               aten._to_copy.default                                                    (view_38,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_50                                                                aten.linear.default                                                      (_to_copy_9, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)   {}\n",
      "call_function  dropout_14                                                               aten.dropout.default                                                     (linear_50, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_12                                                                   aten.div.Tensor                                                          (dropout_14, 1.0)                                                                                                                                              {}\n",
      "call_function  add_21                                                                   aten.add.Tensor                                                          (div_12, add_20)                                                                                                                                               {}\n",
      "call_function  layer_norm_11                                                            aten.layer_norm.default                                                  (add_21, [640], p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias)                  {}\n",
      "call_function  linear_51                                                                aten.linear.default                                                      (layer_norm_11, p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)  {}\n",
      "call_function  split_3                                                                  aten.split.Tensor                                                        (linear_51, 2560, -1)                                                                                                                                          {}\n",
      "call_function  getitem_6                                                                <built-in function getitem>                                              (split_3, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_7                                                                <built-in function getitem>                                              (split_3, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_3                                                                   aten.gelu.default                                                        (getitem_7,)                                                                                                                                                   {}\n",
      "call_function  mul_6                                                                    aten.mul.Tensor                                                          (getitem_6, gelu_3)                                                                                                                                            {}\n",
      "call_function  dropout_15                                                               aten.dropout.default                                                     (mul_6, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_52                                                                aten.linear.default                                                      (dropout_15, p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias)               {}\n",
      "call_function  add_22                                                                   aten.add.Tensor                                                          (linear_52, add_21)                                                                                                                                            {}\n",
      "call_function  linear_53                                                                aten.linear.default                                                      (add_22, p_down_blocks_1_attentions_1_proj_out_weight, p_down_blocks_1_attentions_1_proj_out_bias)                                                             {}\n",
      "call_function  view_39                                                                  aten.view.default                                                        (linear_53, [1, 32, 32, 640])                                                                                                                                  {}\n",
      "call_function  permute_7                                                                aten.permute.default                                                     (view_39, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_3                                                                  aten.clone.default                                                       (permute_7,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_23                                                                   aten.add.Tensor                                                          (clone_3, div_10)                                                                                                                                              {}\n",
      "call_function  conv2d_11                                                                aten.conv2d.default                                                      (add_23, p_down_blocks_1_downsamplers_0_conv_weight, p_down_blocks_1_downsamplers_0_conv_bias, [2, 2], [1, 1])                                                 {}\n",
      "call_function  group_norm_12                                                            aten.group_norm.default                                                  (conv2d_11, 32, p_down_blocks_2_resnets_0_norm1_weight, p_down_blocks_2_resnets_0_norm1_bias)                                                                  {}\n",
      "call_function  silu_13                                                                  aten.silu.default                                                        (group_norm_12,)                                                                                                                                               {}\n",
      "call_function  conv2d_12                                                                aten.conv2d.default                                                      (silu_13, p_down_blocks_2_resnets_0_conv1_weight, p_down_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_14                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_54                                                                aten.linear.default                                                      (silu_14, p_down_blocks_2_resnets_0_time_emb_proj_weight, p_down_blocks_2_resnets_0_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_15                                                                 aten.slice.Tensor                                                        (linear_54, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_16                                                                 aten.slice.Tensor                                                        (slice_15, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_10                                                             aten.unsqueeze.default                                                   (slice_16, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_11                                                             aten.unsqueeze.default                                                   (unsqueeze_10, 3)                                                                                                                                              {}\n",
      "call_function  add_24                                                                   aten.add.Tensor                                                          (conv2d_12, unsqueeze_11)                                                                                                                                      {}\n",
      "call_function  group_norm_13                                                            aten.group_norm.default                                                  (add_24, 32, p_down_blocks_2_resnets_0_norm2_weight, p_down_blocks_2_resnets_0_norm2_bias)                                                                     {}\n",
      "call_function  silu_15                                                                  aten.silu.default                                                        (group_norm_13,)                                                                                                                                               {}\n",
      "call_function  dropout_16                                                               aten.dropout.default                                                     (silu_15, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_13                                                                aten.conv2d.default                                                      (dropout_16, p_down_blocks_2_resnets_0_conv2_weight, p_down_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  conv2d_14                                                                aten.conv2d.default                                                      (conv2d_11, p_down_blocks_2_resnets_0_conv_shortcut_weight, p_down_blocks_2_resnets_0_conv_shortcut_bias)                                                      {}\n",
      "call_function  add_25                                                                   aten.add.Tensor                                                          (conv2d_14, conv2d_13)                                                                                                                                         {}\n",
      "call_function  div_13                                                                   aten.div.Tensor                                                          (add_25, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_14                                                            aten.group_norm.default                                                  (div_13, 32, p_down_blocks_2_attentions_0_norm_weight, p_down_blocks_2_attentions_0_norm_bias, 1e-06)                                                          {}\n",
      "call_function  permute_8                                                                aten.permute.default                                                     (group_norm_14, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_40                                                                  aten.view.default                                                        (permute_8, [1, 256, 1280])                                                                                                                                    {}\n",
      "call_function  linear_55                                                                aten.linear.default                                                      (view_40, p_down_blocks_2_attentions_0_proj_in_weight, p_down_blocks_2_attentions_0_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_12                                                            aten.layer_norm.default                                                  (linear_55, [1280], p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias)              {}\n",
      "call_function  linear_56                                                                aten.linear.default                                                      (layer_norm_12, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                           {}\n",
      "call_function  linear_57                                                                aten.linear.default                                                      (layer_norm_12, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                           {}\n",
      "call_function  linear_58                                                                aten.linear.default                                                      (layer_norm_12, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                           {}\n",
      "call_function  view_41                                                                  aten.view.default                                                        (linear_56, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_32                                                             aten.transpose.int                                                       (view_41, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_42                                                                  aten.view.default                                                        (linear_57, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_33                                                             aten.transpose.int                                                       (view_42, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_43                                                                  aten.view.default                                                        (linear_58, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_34                                                             aten.transpose.int                                                       (view_43, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_8                                           aten.scaled_dot_product_attention.default                                (transpose_32, transpose_33, transpose_34)                                                                                                                     {}\n",
      "call_function  transpose_35                                                             aten.transpose.int                                                       (scaled_dot_product_attention_8, 1, 2)                                                                                                                         {}\n",
      "call_function  view_44                                                                  aten.view.default                                                        (transpose_35, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_10                                                              aten._to_copy.default                                                    (view_44,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_59                                                                aten.linear.default                                                      (_to_copy_10, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)  {}\n",
      "call_function  dropout_17                                                               aten.dropout.default                                                     (linear_59, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_14                                                                   aten.div.Tensor                                                          (dropout_17, 1.0)                                                                                                                                              {}\n",
      "call_function  add_26                                                                   aten.add.Tensor                                                          (div_14, linear_55)                                                                                                                                            {}\n",
      "call_function  layer_norm_13                                                            aten.layer_norm.default                                                  (add_26, [1280], p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias)                 {}\n",
      "call_function  linear_60                                                                aten.linear.default                                                      (layer_norm_13, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                           {}\n",
      "call_function  linear_61                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_62                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_45                                                                  aten.view.default                                                        (linear_60, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_36                                                             aten.transpose.int                                                       (view_45, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_46                                                                  aten.view.default                                                        (linear_61, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_37                                                             aten.transpose.int                                                       (view_46, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_47                                                                  aten.view.default                                                        (linear_62, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_38                                                             aten.transpose.int                                                       (view_47, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_9                                           aten.scaled_dot_product_attention.default                                (transpose_36, transpose_37, transpose_38)                                                                                                                     {}\n",
      "call_function  transpose_39                                                             aten.transpose.int                                                       (scaled_dot_product_attention_9, 1, 2)                                                                                                                         {}\n",
      "call_function  view_48                                                                  aten.view.default                                                        (transpose_39, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_11                                                              aten._to_copy.default                                                    (view_48,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_63                                                                aten.linear.default                                                      (_to_copy_11, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)  {}\n",
      "call_function  dropout_18                                                               aten.dropout.default                                                     (linear_63, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_15                                                                   aten.div.Tensor                                                          (dropout_18, 1.0)                                                                                                                                              {}\n",
      "call_function  add_27                                                                   aten.add.Tensor                                                          (div_15, add_26)                                                                                                                                               {}\n",
      "call_function  layer_norm_14                                                            aten.layer_norm.default                                                  (add_27, [1280], p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias)                 {}\n",
      "call_function  linear_64                                                                aten.linear.default                                                      (layer_norm_14, p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)  {}\n",
      "call_function  split_4                                                                  aten.split.Tensor                                                        (linear_64, 5120, -1)                                                                                                                                          {}\n",
      "call_function  getitem_8                                                                <built-in function getitem>                                              (split_4, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_9                                                                <built-in function getitem>                                              (split_4, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_4                                                                   aten.gelu.default                                                        (getitem_9,)                                                                                                                                                   {}\n",
      "call_function  mul_7                                                                    aten.mul.Tensor                                                          (getitem_8, gelu_4)                                                                                                                                            {}\n",
      "call_function  dropout_19                                                               aten.dropout.default                                                     (mul_7, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_65                                                                aten.linear.default                                                      (dropout_19, p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias)               {}\n",
      "call_function  add_28                                                                   aten.add.Tensor                                                          (linear_65, add_27)                                                                                                                                            {}\n",
      "call_function  linear_66                                                                aten.linear.default                                                      (add_28, p_down_blocks_2_attentions_0_proj_out_weight, p_down_blocks_2_attentions_0_proj_out_bias)                                                             {}\n",
      "call_function  view_49                                                                  aten.view.default                                                        (linear_66, [1, 16, 16, 1280])                                                                                                                                 {}\n",
      "call_function  permute_9                                                                aten.permute.default                                                     (view_49, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_4                                                                  aten.clone.default                                                       (permute_9,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_29                                                                   aten.add.Tensor                                                          (clone_4, div_13)                                                                                                                                              {}\n",
      "call_function  group_norm_15                                                            aten.group_norm.default                                                  (add_29, 32, p_down_blocks_2_resnets_1_norm1_weight, p_down_blocks_2_resnets_1_norm1_bias)                                                                     {}\n",
      "call_function  silu_16                                                                  aten.silu.default                                                        (group_norm_15,)                                                                                                                                               {}\n",
      "call_function  conv2d_15                                                                aten.conv2d.default                                                      (silu_16, p_down_blocks_2_resnets_1_conv1_weight, p_down_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_17                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_67                                                                aten.linear.default                                                      (silu_17, p_down_blocks_2_resnets_1_time_emb_proj_weight, p_down_blocks_2_resnets_1_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_17                                                                 aten.slice.Tensor                                                        (linear_67, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_18                                                                 aten.slice.Tensor                                                        (slice_17, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_12                                                             aten.unsqueeze.default                                                   (slice_18, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_13                                                             aten.unsqueeze.default                                                   (unsqueeze_12, 3)                                                                                                                                              {}\n",
      "call_function  add_30                                                                   aten.add.Tensor                                                          (conv2d_15, unsqueeze_13)                                                                                                                                      {}\n",
      "call_function  group_norm_16                                                            aten.group_norm.default                                                  (add_30, 32, p_down_blocks_2_resnets_1_norm2_weight, p_down_blocks_2_resnets_1_norm2_bias)                                                                     {}\n",
      "call_function  silu_18                                                                  aten.silu.default                                                        (group_norm_16,)                                                                                                                                               {}\n",
      "call_function  dropout_20                                                               aten.dropout.default                                                     (silu_18, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_16                                                                aten.conv2d.default                                                      (dropout_20, p_down_blocks_2_resnets_1_conv2_weight, p_down_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  add_31                                                                   aten.add.Tensor                                                          (add_29, conv2d_16)                                                                                                                                            {}\n",
      "call_function  div_16                                                                   aten.div.Tensor                                                          (add_31, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_17                                                            aten.group_norm.default                                                  (div_16, 32, p_down_blocks_2_attentions_1_norm_weight, p_down_blocks_2_attentions_1_norm_bias, 1e-06)                                                          {}\n",
      "call_function  permute_10                                                               aten.permute.default                                                     (group_norm_17, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_50                                                                  aten.view.default                                                        (permute_10, [1, 256, 1280])                                                                                                                                   {}\n",
      "call_function  linear_68                                                                aten.linear.default                                                      (view_50, p_down_blocks_2_attentions_1_proj_in_weight, p_down_blocks_2_attentions_1_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_15                                                            aten.layer_norm.default                                                  (linear_68, [1280], p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias)              {}\n",
      "call_function  linear_69                                                                aten.linear.default                                                      (layer_norm_15, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                           {}\n",
      "call_function  linear_70                                                                aten.linear.default                                                      (layer_norm_15, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                           {}\n",
      "call_function  linear_71                                                                aten.linear.default                                                      (layer_norm_15, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                           {}\n",
      "call_function  view_51                                                                  aten.view.default                                                        (linear_69, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_40                                                             aten.transpose.int                                                       (view_51, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_52                                                                  aten.view.default                                                        (linear_70, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_41                                                             aten.transpose.int                                                       (view_52, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_53                                                                  aten.view.default                                                        (linear_71, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_42                                                             aten.transpose.int                                                       (view_53, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_10                                          aten.scaled_dot_product_attention.default                                (transpose_40, transpose_41, transpose_42)                                                                                                                     {}\n",
      "call_function  transpose_43                                                             aten.transpose.int                                                       (scaled_dot_product_attention_10, 1, 2)                                                                                                                        {}\n",
      "call_function  view_54                                                                  aten.view.default                                                        (transpose_43, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_12                                                              aten._to_copy.default                                                    (view_54,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_72                                                                aten.linear.default                                                      (_to_copy_12, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)  {}\n",
      "call_function  dropout_21                                                               aten.dropout.default                                                     (linear_72, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_17                                                                   aten.div.Tensor                                                          (dropout_21, 1.0)                                                                                                                                              {}\n",
      "call_function  add_32                                                                   aten.add.Tensor                                                          (div_17, linear_68)                                                                                                                                            {}\n",
      "call_function  layer_norm_16                                                            aten.layer_norm.default                                                  (add_32, [1280], p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias)                 {}\n",
      "call_function  linear_73                                                                aten.linear.default                                                      (layer_norm_16, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                           {}\n",
      "call_function  linear_74                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_75                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_55                                                                  aten.view.default                                                        (linear_73, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_44                                                             aten.transpose.int                                                       (view_55, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_56                                                                  aten.view.default                                                        (linear_74, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_45                                                             aten.transpose.int                                                       (view_56, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_57                                                                  aten.view.default                                                        (linear_75, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_46                                                             aten.transpose.int                                                       (view_57, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_11                                          aten.scaled_dot_product_attention.default                                (transpose_44, transpose_45, transpose_46)                                                                                                                     {}\n",
      "call_function  transpose_47                                                             aten.transpose.int                                                       (scaled_dot_product_attention_11, 1, 2)                                                                                                                        {}\n",
      "call_function  view_58                                                                  aten.view.default                                                        (transpose_47, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_13                                                              aten._to_copy.default                                                    (view_58,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_76                                                                aten.linear.default                                                      (_to_copy_13, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)  {}\n",
      "call_function  dropout_22                                                               aten.dropout.default                                                     (linear_76, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_18                                                                   aten.div.Tensor                                                          (dropout_22, 1.0)                                                                                                                                              {}\n",
      "call_function  add_33                                                                   aten.add.Tensor                                                          (div_18, add_32)                                                                                                                                               {}\n",
      "call_function  layer_norm_17                                                            aten.layer_norm.default                                                  (add_33, [1280], p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias)                 {}\n",
      "call_function  linear_77                                                                aten.linear.default                                                      (layer_norm_17, p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)  {}\n",
      "call_function  split_5                                                                  aten.split.Tensor                                                        (linear_77, 5120, -1)                                                                                                                                          {}\n",
      "call_function  getitem_10                                                               <built-in function getitem>                                              (split_5, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_11                                                               <built-in function getitem>                                              (split_5, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_5                                                                   aten.gelu.default                                                        (getitem_11,)                                                                                                                                                  {}\n",
      "call_function  mul_8                                                                    aten.mul.Tensor                                                          (getitem_10, gelu_5)                                                                                                                                           {}\n",
      "call_function  dropout_23                                                               aten.dropout.default                                                     (mul_8, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_78                                                                aten.linear.default                                                      (dropout_23, p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias)               {}\n",
      "call_function  add_34                                                                   aten.add.Tensor                                                          (linear_78, add_33)                                                                                                                                            {}\n",
      "call_function  linear_79                                                                aten.linear.default                                                      (add_34, p_down_blocks_2_attentions_1_proj_out_weight, p_down_blocks_2_attentions_1_proj_out_bias)                                                             {}\n",
      "call_function  view_59                                                                  aten.view.default                                                        (linear_79, [1, 16, 16, 1280])                                                                                                                                 {}\n",
      "call_function  permute_11                                                               aten.permute.default                                                     (view_59, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_5                                                                  aten.clone.default                                                       (permute_11,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_35                                                                   aten.add.Tensor                                                          (clone_5, div_16)                                                                                                                                              {}\n",
      "call_function  conv2d_17                                                                aten.conv2d.default                                                      (add_35, p_down_blocks_2_downsamplers_0_conv_weight, p_down_blocks_2_downsamplers_0_conv_bias, [2, 2], [1, 1])                                                 {}\n",
      "call_function  group_norm_18                                                            aten.group_norm.default                                                  (conv2d_17, 32, p_down_blocks_3_resnets_0_norm1_weight, p_down_blocks_3_resnets_0_norm1_bias)                                                                  {}\n",
      "call_function  silu_19                                                                  aten.silu.default                                                        (group_norm_18,)                                                                                                                                               {}\n",
      "call_function  conv2d_18                                                                aten.conv2d.default                                                      (silu_19, p_down_blocks_3_resnets_0_conv1_weight, p_down_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_20                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_80                                                                aten.linear.default                                                      (silu_20, p_down_blocks_3_resnets_0_time_emb_proj_weight, p_down_blocks_3_resnets_0_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_19                                                                 aten.slice.Tensor                                                        (linear_80, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_20                                                                 aten.slice.Tensor                                                        (slice_19, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_14                                                             aten.unsqueeze.default                                                   (slice_20, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_15                                                             aten.unsqueeze.default                                                   (unsqueeze_14, 3)                                                                                                                                              {}\n",
      "call_function  add_36                                                                   aten.add.Tensor                                                          (conv2d_18, unsqueeze_15)                                                                                                                                      {}\n",
      "call_function  group_norm_19                                                            aten.group_norm.default                                                  (add_36, 32, p_down_blocks_3_resnets_0_norm2_weight, p_down_blocks_3_resnets_0_norm2_bias)                                                                     {}\n",
      "call_function  silu_21                                                                  aten.silu.default                                                        (group_norm_19,)                                                                                                                                               {}\n",
      "call_function  dropout_24                                                               aten.dropout.default                                                     (silu_21, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_19                                                                aten.conv2d.default                                                      (dropout_24, p_down_blocks_3_resnets_0_conv2_weight, p_down_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  add_37                                                                   aten.add.Tensor                                                          (conv2d_17, conv2d_19)                                                                                                                                         {}\n",
      "call_function  div_19                                                                   aten.div.Tensor                                                          (add_37, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_20                                                            aten.group_norm.default                                                  (div_19, 32, p_down_blocks_3_resnets_1_norm1_weight, p_down_blocks_3_resnets_1_norm1_bias)                                                                     {}\n",
      "call_function  silu_22                                                                  aten.silu.default                                                        (group_norm_20,)                                                                                                                                               {}\n",
      "call_function  conv2d_20                                                                aten.conv2d.default                                                      (silu_22, p_down_blocks_3_resnets_1_conv1_weight, p_down_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_23                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_81                                                                aten.linear.default                                                      (silu_23, p_down_blocks_3_resnets_1_time_emb_proj_weight, p_down_blocks_3_resnets_1_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_21                                                                 aten.slice.Tensor                                                        (linear_81, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_22                                                                 aten.slice.Tensor                                                        (slice_21, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_16                                                             aten.unsqueeze.default                                                   (slice_22, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_17                                                             aten.unsqueeze.default                                                   (unsqueeze_16, 3)                                                                                                                                              {}\n",
      "call_function  add_38                                                                   aten.add.Tensor                                                          (conv2d_20, unsqueeze_17)                                                                                                                                      {}\n",
      "call_function  group_norm_21                                                            aten.group_norm.default                                                  (add_38, 32, p_down_blocks_3_resnets_1_norm2_weight, p_down_blocks_3_resnets_1_norm2_bias)                                                                     {}\n",
      "call_function  silu_24                                                                  aten.silu.default                                                        (group_norm_21,)                                                                                                                                               {}\n",
      "call_function  dropout_25                                                               aten.dropout.default                                                     (silu_24, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_21                                                                aten.conv2d.default                                                      (dropout_25, p_down_blocks_3_resnets_1_conv2_weight, p_down_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  add_39                                                                   aten.add.Tensor                                                          (div_19, conv2d_21)                                                                                                                                            {}\n",
      "call_function  div_20                                                                   aten.div.Tensor                                                          (add_39, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_22                                                            aten.group_norm.default                                                  (div_20, 32, p_mid_block_resnets_0_norm1_weight, p_mid_block_resnets_0_norm1_bias)                                                                             {}\n",
      "call_function  silu_25                                                                  aten.silu.default                                                        (group_norm_22,)                                                                                                                                               {}\n",
      "call_function  conv2d_22                                                                aten.conv2d.default                                                      (silu_25, p_mid_block_resnets_0_conv1_weight, p_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1])                                                                {}\n",
      "call_function  silu_26                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_82                                                                aten.linear.default                                                      (silu_26, p_mid_block_resnets_0_time_emb_proj_weight, p_mid_block_resnets_0_time_emb_proj_bias)                                                                {}\n",
      "call_function  slice_23                                                                 aten.slice.Tensor                                                        (linear_82, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_24                                                                 aten.slice.Tensor                                                        (slice_23, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_18                                                             aten.unsqueeze.default                                                   (slice_24, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_19                                                             aten.unsqueeze.default                                                   (unsqueeze_18, 3)                                                                                                                                              {}\n",
      "call_function  add_40                                                                   aten.add.Tensor                                                          (conv2d_22, unsqueeze_19)                                                                                                                                      {}\n",
      "call_function  group_norm_23                                                            aten.group_norm.default                                                  (add_40, 32, p_mid_block_resnets_0_norm2_weight, p_mid_block_resnets_0_norm2_bias)                                                                             {}\n",
      "call_function  silu_27                                                                  aten.silu.default                                                        (group_norm_23,)                                                                                                                                               {}\n",
      "call_function  dropout_26                                                               aten.dropout.default                                                     (silu_27, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_23                                                                aten.conv2d.default                                                      (dropout_26, p_mid_block_resnets_0_conv2_weight, p_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1])                                                             {}\n",
      "call_function  add_41                                                                   aten.add.Tensor                                                          (div_20, conv2d_23)                                                                                                                                            {}\n",
      "call_function  div_21                                                                   aten.div.Tensor                                                          (add_41, 1)                                                                                                                                                    {}\n",
      "call_function  group_norm_24                                                            aten.group_norm.default                                                  (div_21, 32, p_mid_block_attentions_0_norm_weight, p_mid_block_attentions_0_norm_bias, 1e-06)                                                                  {}\n",
      "call_function  permute_12                                                               aten.permute.default                                                     (group_norm_24, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_60                                                                  aten.view.default                                                        (permute_12, [1, 64, 1280])                                                                                                                                    {}\n",
      "call_function  linear_83                                                                aten.linear.default                                                      (view_60, p_mid_block_attentions_0_proj_in_weight, p_mid_block_attentions_0_proj_in_bias)                                                                      {}\n",
      "call_function  layer_norm_18                                                            aten.layer_norm.default                                                  (linear_83, [1280], p_mid_block_attentions_0_transformer_blocks_0_norm1_weight, p_mid_block_attentions_0_transformer_blocks_0_norm1_bias)                      {}\n",
      "call_function  linear_84                                                                aten.linear.default                                                      (layer_norm_18, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                               {}\n",
      "call_function  linear_85                                                                aten.linear.default                                                      (layer_norm_18, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                               {}\n",
      "call_function  linear_86                                                                aten.linear.default                                                      (layer_norm_18, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                               {}\n",
      "call_function  view_61                                                                  aten.view.default                                                        (linear_84, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_48                                                             aten.transpose.int                                                       (view_61, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_62                                                                  aten.view.default                                                        (linear_85, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_49                                                             aten.transpose.int                                                       (view_62, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_63                                                                  aten.view.default                                                        (linear_86, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_50                                                             aten.transpose.int                                                       (view_63, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_12                                          aten.scaled_dot_product_attention.default                                (transpose_48, transpose_49, transpose_50)                                                                                                                     {}\n",
      "call_function  transpose_51                                                             aten.transpose.int                                                       (scaled_dot_product_attention_12, 1, 2)                                                                                                                        {}\n",
      "call_function  view_64                                                                  aten.view.default                                                        (transpose_51, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_14                                                              aten._to_copy.default                                                    (view_64,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_87                                                                aten.linear.default                                                      (_to_copy_14, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)          {}\n",
      "call_function  dropout_27                                                               aten.dropout.default                                                     (linear_87, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_22                                                                   aten.div.Tensor                                                          (dropout_27, 1.0)                                                                                                                                              {}\n",
      "call_function  add_42                                                                   aten.add.Tensor                                                          (div_22, linear_83)                                                                                                                                            {}\n",
      "call_function  layer_norm_19                                                            aten.layer_norm.default                                                  (add_42, [1280], p_mid_block_attentions_0_transformer_blocks_0_norm2_weight, p_mid_block_attentions_0_transformer_blocks_0_norm2_bias)                         {}\n",
      "call_function  linear_88                                                                aten.linear.default                                                      (layer_norm_19, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                               {}\n",
      "call_function  linear_89                                                                aten.linear.default                                                      (encoder_hidden_states, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                       {}\n",
      "call_function  linear_90                                                                aten.linear.default                                                      (encoder_hidden_states, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                       {}\n",
      "call_function  view_65                                                                  aten.view.default                                                        (linear_88, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_52                                                             aten.transpose.int                                                       (view_65, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_66                                                                  aten.view.default                                                        (linear_89, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_53                                                             aten.transpose.int                                                       (view_66, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_67                                                                  aten.view.default                                                        (linear_90, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_54                                                             aten.transpose.int                                                       (view_67, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_13                                          aten.scaled_dot_product_attention.default                                (transpose_52, transpose_53, transpose_54)                                                                                                                     {}\n",
      "call_function  transpose_55                                                             aten.transpose.int                                                       (scaled_dot_product_attention_13, 1, 2)                                                                                                                        {}\n",
      "call_function  view_68                                                                  aten.view.default                                                        (transpose_55, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_15                                                              aten._to_copy.default                                                    (view_68,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_91                                                                aten.linear.default                                                      (_to_copy_15, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)          {}\n",
      "call_function  dropout_28                                                               aten.dropout.default                                                     (linear_91, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_23                                                                   aten.div.Tensor                                                          (dropout_28, 1.0)                                                                                                                                              {}\n",
      "call_function  add_43                                                                   aten.add.Tensor                                                          (div_23, add_42)                                                                                                                                               {}\n",
      "call_function  layer_norm_20                                                            aten.layer_norm.default                                                  (add_43, [1280], p_mid_block_attentions_0_transformer_blocks_0_norm3_weight, p_mid_block_attentions_0_transformer_blocks_0_norm3_bias)                         {}\n",
      "call_function  linear_92                                                                aten.linear.default                                                      (layer_norm_20, p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)          {}\n",
      "call_function  split_6                                                                  aten.split.Tensor                                                        (linear_92, 5120, -1)                                                                                                                                          {}\n",
      "call_function  getitem_12                                                               <built-in function getitem>                                              (split_6, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_13                                                               <built-in function getitem>                                              (split_6, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_6                                                                   aten.gelu.default                                                        (getitem_13,)                                                                                                                                                  {}\n",
      "call_function  mul_9                                                                    aten.mul.Tensor                                                          (getitem_12, gelu_6)                                                                                                                                           {}\n",
      "call_function  dropout_29                                                               aten.dropout.default                                                     (mul_9, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_93                                                                aten.linear.default                                                      (dropout_29, p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight, p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias)                       {}\n",
      "call_function  add_44                                                                   aten.add.Tensor                                                          (linear_93, add_43)                                                                                                                                            {}\n",
      "call_function  linear_94                                                                aten.linear.default                                                      (add_44, p_mid_block_attentions_0_proj_out_weight, p_mid_block_attentions_0_proj_out_bias)                                                                     {}\n",
      "call_function  view_69                                                                  aten.view.default                                                        (linear_94, [1, 8, 8, 1280])                                                                                                                                   {}\n",
      "call_function  permute_13                                                               aten.permute.default                                                     (view_69, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_6                                                                  aten.clone.default                                                       (permute_13,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_45                                                                   aten.add.Tensor                                                          (clone_6, div_21)                                                                                                                                              {}\n",
      "call_function  group_norm_25                                                            aten.group_norm.default                                                  (add_45, 32, p_mid_block_resnets_slice_1__none__none___0_norm1_weight, p_mid_block_resnets_slice_1__none__none___0_norm1_bias)                                 {}\n",
      "call_function  silu_28                                                                  aten.silu.default                                                        (group_norm_25,)                                                                                                                                               {}\n",
      "call_function  conv2d_24                                                                aten.conv2d.default                                                      (silu_28, p_mid_block_resnets_slice_1__none__none___0_conv1_weight, p_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1])                    {}\n",
      "call_function  silu_29                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_95                                                                aten.linear.default                                                      (silu_29, p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_weight, p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_bias)                    {}\n",
      "call_function  slice_25                                                                 aten.slice.Tensor                                                        (linear_95, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_26                                                                 aten.slice.Tensor                                                        (slice_25, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_20                                                             aten.unsqueeze.default                                                   (slice_26, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_21                                                             aten.unsqueeze.default                                                   (unsqueeze_20, 3)                                                                                                                                              {}\n",
      "call_function  add_46                                                                   aten.add.Tensor                                                          (conv2d_24, unsqueeze_21)                                                                                                                                      {}\n",
      "call_function  group_norm_26                                                            aten.group_norm.default                                                  (add_46, 32, p_mid_block_resnets_slice_1__none__none___0_norm2_weight, p_mid_block_resnets_slice_1__none__none___0_norm2_bias)                                 {}\n",
      "call_function  silu_30                                                                  aten.silu.default                                                        (group_norm_26,)                                                                                                                                               {}\n",
      "call_function  dropout_30                                                               aten.dropout.default                                                     (silu_30, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_25                                                                aten.conv2d.default                                                      (dropout_30, p_mid_block_resnets_slice_1__none__none___0_conv2_weight, p_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1])                 {}\n",
      "call_function  add_47                                                                   aten.add.Tensor                                                          (add_45, conv2d_25)                                                                                                                                            {}\n",
      "call_function  div_24                                                                   aten.div.Tensor                                                          (add_47, 1)                                                                                                                                                    {}\n",
      "call_function  cat_2                                                                    aten.cat.default                                                         ([div_24, div_20], 1)                                                                                                                                          {}\n",
      "call_function  group_norm_27                                                            aten.group_norm.default                                                  (cat_2, 32, p_up_blocks_0_resnets_0_norm1_weight, p_up_blocks_0_resnets_0_norm1_bias)                                                                          {}\n",
      "call_function  silu_31                                                                  aten.silu.default                                                        (group_norm_27,)                                                                                                                                               {}\n",
      "call_function  conv2d_26                                                                aten.conv2d.default                                                      (silu_31, p_up_blocks_0_resnets_0_conv1_weight, p_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_32                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_96                                                                aten.linear.default                                                      (silu_32, p_up_blocks_0_resnets_0_time_emb_proj_weight, p_up_blocks_0_resnets_0_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_27                                                                 aten.slice.Tensor                                                        (linear_96, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_28                                                                 aten.slice.Tensor                                                        (slice_27, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_22                                                             aten.unsqueeze.default                                                   (slice_28, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_23                                                             aten.unsqueeze.default                                                   (unsqueeze_22, 3)                                                                                                                                              {}\n",
      "call_function  add_48                                                                   aten.add.Tensor                                                          (conv2d_26, unsqueeze_23)                                                                                                                                      {}\n",
      "call_function  group_norm_28                                                            aten.group_norm.default                                                  (add_48, 32, p_up_blocks_0_resnets_0_norm2_weight, p_up_blocks_0_resnets_0_norm2_bias)                                                                         {}\n",
      "call_function  silu_33                                                                  aten.silu.default                                                        (group_norm_28,)                                                                                                                                               {}\n",
      "call_function  dropout_31                                                               aten.dropout.default                                                     (silu_33, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_27                                                                aten.conv2d.default                                                      (dropout_31, p_up_blocks_0_resnets_0_conv2_weight, p_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_28                                                                aten.conv2d.default                                                      (cat_2, p_up_blocks_0_resnets_0_conv_shortcut_weight, p_up_blocks_0_resnets_0_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_49                                                                   aten.add.Tensor                                                          (conv2d_28, conv2d_27)                                                                                                                                         {}\n",
      "call_function  div_25                                                                   aten.div.Tensor                                                          (add_49, 1.0)                                                                                                                                                  {}\n",
      "call_function  cat_3                                                                    aten.cat.default                                                         ([div_25, div_19], 1)                                                                                                                                          {}\n",
      "call_function  group_norm_29                                                            aten.group_norm.default                                                  (cat_3, 32, p_up_blocks_0_resnets_1_norm1_weight, p_up_blocks_0_resnets_1_norm1_bias)                                                                          {}\n",
      "call_function  silu_34                                                                  aten.silu.default                                                        (group_norm_29,)                                                                                                                                               {}\n",
      "call_function  conv2d_29                                                                aten.conv2d.default                                                      (silu_34, p_up_blocks_0_resnets_1_conv1_weight, p_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_35                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_97                                                                aten.linear.default                                                      (silu_35, p_up_blocks_0_resnets_1_time_emb_proj_weight, p_up_blocks_0_resnets_1_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_29                                                                 aten.slice.Tensor                                                        (linear_97, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_30                                                                 aten.slice.Tensor                                                        (slice_29, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_24                                                             aten.unsqueeze.default                                                   (slice_30, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_25                                                             aten.unsqueeze.default                                                   (unsqueeze_24, 3)                                                                                                                                              {}\n",
      "call_function  add_50                                                                   aten.add.Tensor                                                          (conv2d_29, unsqueeze_25)                                                                                                                                      {}\n",
      "call_function  group_norm_30                                                            aten.group_norm.default                                                  (add_50, 32, p_up_blocks_0_resnets_1_norm2_weight, p_up_blocks_0_resnets_1_norm2_bias)                                                                         {}\n",
      "call_function  silu_36                                                                  aten.silu.default                                                        (group_norm_30,)                                                                                                                                               {}\n",
      "call_function  dropout_32                                                               aten.dropout.default                                                     (silu_36, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_30                                                                aten.conv2d.default                                                      (dropout_32, p_up_blocks_0_resnets_1_conv2_weight, p_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_31                                                                aten.conv2d.default                                                      (cat_3, p_up_blocks_0_resnets_1_conv_shortcut_weight, p_up_blocks_0_resnets_1_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_51                                                                   aten.add.Tensor                                                          (conv2d_31, conv2d_30)                                                                                                                                         {}\n",
      "call_function  div_26                                                                   aten.div.Tensor                                                          (add_51, 1.0)                                                                                                                                                  {}\n",
      "call_function  cat_4                                                                    aten.cat.default                                                         ([div_26, conv2d_17], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_31                                                            aten.group_norm.default                                                  (cat_4, 32, p_up_blocks_0_resnets_2_norm1_weight, p_up_blocks_0_resnets_2_norm1_bias)                                                                          {}\n",
      "call_function  silu_37                                                                  aten.silu.default                                                        (group_norm_31,)                                                                                                                                               {}\n",
      "call_function  conv2d_32                                                                aten.conv2d.default                                                      (silu_37, p_up_blocks_0_resnets_2_conv1_weight, p_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_38                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_98                                                                aten.linear.default                                                      (silu_38, p_up_blocks_0_resnets_2_time_emb_proj_weight, p_up_blocks_0_resnets_2_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_31                                                                 aten.slice.Tensor                                                        (linear_98, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_32                                                                 aten.slice.Tensor                                                        (slice_31, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_26                                                             aten.unsqueeze.default                                                   (slice_32, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_27                                                             aten.unsqueeze.default                                                   (unsqueeze_26, 3)                                                                                                                                              {}\n",
      "call_function  add_52                                                                   aten.add.Tensor                                                          (conv2d_32, unsqueeze_27)                                                                                                                                      {}\n",
      "call_function  group_norm_32                                                            aten.group_norm.default                                                  (add_52, 32, p_up_blocks_0_resnets_2_norm2_weight, p_up_blocks_0_resnets_2_norm2_bias)                                                                         {}\n",
      "call_function  silu_39                                                                  aten.silu.default                                                        (group_norm_32,)                                                                                                                                               {}\n",
      "call_function  dropout_33                                                               aten.dropout.default                                                     (silu_39, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_33                                                                aten.conv2d.default                                                      (dropout_33, p_up_blocks_0_resnets_2_conv2_weight, p_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_34                                                                aten.conv2d.default                                                      (cat_4, p_up_blocks_0_resnets_2_conv_shortcut_weight, p_up_blocks_0_resnets_2_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_53                                                                   aten.add.Tensor                                                          (conv2d_34, conv2d_33)                                                                                                                                         {}\n",
      "call_function  div_27                                                                   aten.div.Tensor                                                          (add_53, 1.0)                                                                                                                                                  {}\n",
      "call_function  upsample_nearest2d                                                       aten.upsample_nearest2d.vec                                              (div_27, None, [2.0, 2.0])                                                                                                                                     {}\n",
      "call_function  conv2d_35                                                                aten.conv2d.default                                                      (upsample_nearest2d, p_up_blocks_0_upsamplers_0_conv_weight, p_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  cat_5                                                                    aten.cat.default                                                         ([conv2d_35, add_35], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_33                                                            aten.group_norm.default                                                  (cat_5, 32, p_up_blocks_1_resnets_0_norm1_weight, p_up_blocks_1_resnets_0_norm1_bias)                                                                          {}\n",
      "call_function  silu_40                                                                  aten.silu.default                                                        (group_norm_33,)                                                                                                                                               {}\n",
      "call_function  conv2d_36                                                                aten.conv2d.default                                                      (silu_40, p_up_blocks_1_resnets_0_conv1_weight, p_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_41                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_99                                                                aten.linear.default                                                      (silu_41, p_up_blocks_1_resnets_0_time_emb_proj_weight, p_up_blocks_1_resnets_0_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_33                                                                 aten.slice.Tensor                                                        (linear_99, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_34                                                                 aten.slice.Tensor                                                        (slice_33, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_28                                                             aten.unsqueeze.default                                                   (slice_34, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_29                                                             aten.unsqueeze.default                                                   (unsqueeze_28, 3)                                                                                                                                              {}\n",
      "call_function  add_54                                                                   aten.add.Tensor                                                          (conv2d_36, unsqueeze_29)                                                                                                                                      {}\n",
      "call_function  group_norm_34                                                            aten.group_norm.default                                                  (add_54, 32, p_up_blocks_1_resnets_0_norm2_weight, p_up_blocks_1_resnets_0_norm2_bias)                                                                         {}\n",
      "call_function  silu_42                                                                  aten.silu.default                                                        (group_norm_34,)                                                                                                                                               {}\n",
      "call_function  dropout_34                                                               aten.dropout.default                                                     (silu_42, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_37                                                                aten.conv2d.default                                                      (dropout_34, p_up_blocks_1_resnets_0_conv2_weight, p_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_38                                                                aten.conv2d.default                                                      (cat_5, p_up_blocks_1_resnets_0_conv_shortcut_weight, p_up_blocks_1_resnets_0_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_55                                                                   aten.add.Tensor                                                          (conv2d_38, conv2d_37)                                                                                                                                         {}\n",
      "call_function  div_28                                                                   aten.div.Tensor                                                          (add_55, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_35                                                            aten.group_norm.default                                                  (div_28, 32, p_up_blocks_1_attentions_0_norm_weight, p_up_blocks_1_attentions_0_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_14                                                               aten.permute.default                                                     (group_norm_35, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_70                                                                  aten.view.default                                                        (permute_14, [1, 256, 1280])                                                                                                                                   {}\n",
      "call_function  linear_100                                                               aten.linear.default                                                      (view_70, p_up_blocks_1_attentions_0_proj_in_weight, p_up_blocks_1_attentions_0_proj_in_bias)                                                                  {}\n",
      "call_function  layer_norm_21                                                            aten.layer_norm.default                                                  (linear_100, [1280], p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias)                 {}\n",
      "call_function  linear_101                                                               aten.linear.default                                                      (layer_norm_21, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_102                                                               aten.linear.default                                                      (layer_norm_21, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_103                                                               aten.linear.default                                                      (layer_norm_21, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_71                                                                  aten.view.default                                                        (linear_101, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_56                                                             aten.transpose.int                                                       (view_71, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_72                                                                  aten.view.default                                                        (linear_102, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_57                                                             aten.transpose.int                                                       (view_72, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_73                                                                  aten.view.default                                                        (linear_103, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_58                                                             aten.transpose.int                                                       (view_73, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_14                                          aten.scaled_dot_product_attention.default                                (transpose_56, transpose_57, transpose_58)                                                                                                                     {}\n",
      "call_function  transpose_59                                                             aten.transpose.int                                                       (scaled_dot_product_attention_14, 1, 2)                                                                                                                        {}\n",
      "call_function  view_74                                                                  aten.view.default                                                        (transpose_59, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_16                                                              aten._to_copy.default                                                    (view_74,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_104                                                               aten.linear.default                                                      (_to_copy_16, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_35                                                               aten.dropout.default                                                     (linear_104, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_29                                                                   aten.div.Tensor                                                          (dropout_35, 1.0)                                                                                                                                              {}\n",
      "call_function  add_56                                                                   aten.add.Tensor                                                          (div_29, linear_100)                                                                                                                                           {}\n",
      "call_function  layer_norm_22                                                            aten.layer_norm.default                                                  (add_56, [1280], p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias)                     {}\n",
      "call_function  linear_105                                                               aten.linear.default                                                      (layer_norm_22, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_106                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_107                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_75                                                                  aten.view.default                                                        (linear_105, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_60                                                             aten.transpose.int                                                       (view_75, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_76                                                                  aten.view.default                                                        (linear_106, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_61                                                             aten.transpose.int                                                       (view_76, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_77                                                                  aten.view.default                                                        (linear_107, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_62                                                             aten.transpose.int                                                       (view_77, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_15                                          aten.scaled_dot_product_attention.default                                (transpose_60, transpose_61, transpose_62)                                                                                                                     {}\n",
      "call_function  transpose_63                                                             aten.transpose.int                                                       (scaled_dot_product_attention_15, 1, 2)                                                                                                                        {}\n",
      "call_function  view_78                                                                  aten.view.default                                                        (transpose_63, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_17                                                              aten._to_copy.default                                                    (view_78,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_108                                                               aten.linear.default                                                      (_to_copy_17, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_36                                                               aten.dropout.default                                                     (linear_108, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_30                                                                   aten.div.Tensor                                                          (dropout_36, 1.0)                                                                                                                                              {}\n",
      "call_function  add_57                                                                   aten.add.Tensor                                                          (div_30, add_56)                                                                                                                                               {}\n",
      "call_function  layer_norm_23                                                            aten.layer_norm.default                                                  (add_57, [1280], p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias)                     {}\n",
      "call_function  linear_109                                                               aten.linear.default                                                      (layer_norm_23, p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_7                                                                  aten.split.Tensor                                                        (linear_109, 5120, -1)                                                                                                                                         {}\n",
      "call_function  getitem_14                                                               <built-in function getitem>                                              (split_7, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_15                                                               <built-in function getitem>                                              (split_7, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_7                                                                   aten.gelu.default                                                        (getitem_15,)                                                                                                                                                  {}\n",
      "call_function  mul_10                                                                   aten.mul.Tensor                                                          (getitem_14, gelu_7)                                                                                                                                           {}\n",
      "call_function  dropout_37                                                               aten.dropout.default                                                     (mul_10, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_110                                                               aten.linear.default                                                      (dropout_37, p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_58                                                                   aten.add.Tensor                                                          (linear_110, add_57)                                                                                                                                           {}\n",
      "call_function  linear_111                                                               aten.linear.default                                                      (add_58, p_up_blocks_1_attentions_0_proj_out_weight, p_up_blocks_1_attentions_0_proj_out_bias)                                                                 {}\n",
      "call_function  view_79                                                                  aten.view.default                                                        (linear_111, [1, 16, 16, 1280])                                                                                                                                {}\n",
      "call_function  permute_15                                                               aten.permute.default                                                     (view_79, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_7                                                                  aten.clone.default                                                       (permute_15,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_59                                                                   aten.add.Tensor                                                          (clone_7, div_28)                                                                                                                                              {}\n",
      "call_function  cat_6                                                                    aten.cat.default                                                         ([add_59, add_29], 1)                                                                                                                                          {}\n",
      "call_function  group_norm_36                                                            aten.group_norm.default                                                  (cat_6, 32, p_up_blocks_1_resnets_1_norm1_weight, p_up_blocks_1_resnets_1_norm1_bias)                                                                          {}\n",
      "call_function  silu_43                                                                  aten.silu.default                                                        (group_norm_36,)                                                                                                                                               {}\n",
      "call_function  conv2d_39                                                                aten.conv2d.default                                                      (silu_43, p_up_blocks_1_resnets_1_conv1_weight, p_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_44                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_112                                                               aten.linear.default                                                      (silu_44, p_up_blocks_1_resnets_1_time_emb_proj_weight, p_up_blocks_1_resnets_1_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_35                                                                 aten.slice.Tensor                                                        (linear_112, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_36                                                                 aten.slice.Tensor                                                        (slice_35, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_30                                                             aten.unsqueeze.default                                                   (slice_36, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_31                                                             aten.unsqueeze.default                                                   (unsqueeze_30, 3)                                                                                                                                              {}\n",
      "call_function  add_60                                                                   aten.add.Tensor                                                          (conv2d_39, unsqueeze_31)                                                                                                                                      {}\n",
      "call_function  group_norm_37                                                            aten.group_norm.default                                                  (add_60, 32, p_up_blocks_1_resnets_1_norm2_weight, p_up_blocks_1_resnets_1_norm2_bias)                                                                         {}\n",
      "call_function  silu_45                                                                  aten.silu.default                                                        (group_norm_37,)                                                                                                                                               {}\n",
      "call_function  dropout_38                                                               aten.dropout.default                                                     (silu_45, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_40                                                                aten.conv2d.default                                                      (dropout_38, p_up_blocks_1_resnets_1_conv2_weight, p_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_41                                                                aten.conv2d.default                                                      (cat_6, p_up_blocks_1_resnets_1_conv_shortcut_weight, p_up_blocks_1_resnets_1_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_61                                                                   aten.add.Tensor                                                          (conv2d_41, conv2d_40)                                                                                                                                         {}\n",
      "call_function  div_31                                                                   aten.div.Tensor                                                          (add_61, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_38                                                            aten.group_norm.default                                                  (div_31, 32, p_up_blocks_1_attentions_1_norm_weight, p_up_blocks_1_attentions_1_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_16                                                               aten.permute.default                                                     (group_norm_38, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_80                                                                  aten.view.default                                                        (permute_16, [1, 256, 1280])                                                                                                                                   {}\n",
      "call_function  linear_113                                                               aten.linear.default                                                      (view_80, p_up_blocks_1_attentions_1_proj_in_weight, p_up_blocks_1_attentions_1_proj_in_bias)                                                                  {}\n",
      "call_function  layer_norm_24                                                            aten.layer_norm.default                                                  (linear_113, [1280], p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias)                 {}\n",
      "call_function  linear_114                                                               aten.linear.default                                                      (layer_norm_24, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_115                                                               aten.linear.default                                                      (layer_norm_24, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_116                                                               aten.linear.default                                                      (layer_norm_24, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_81                                                                  aten.view.default                                                        (linear_114, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_64                                                             aten.transpose.int                                                       (view_81, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_82                                                                  aten.view.default                                                        (linear_115, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_65                                                             aten.transpose.int                                                       (view_82, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_83                                                                  aten.view.default                                                        (linear_116, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_66                                                             aten.transpose.int                                                       (view_83, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_16                                          aten.scaled_dot_product_attention.default                                (transpose_64, transpose_65, transpose_66)                                                                                                                     {}\n",
      "call_function  transpose_67                                                             aten.transpose.int                                                       (scaled_dot_product_attention_16, 1, 2)                                                                                                                        {}\n",
      "call_function  view_84                                                                  aten.view.default                                                        (transpose_67, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_18                                                              aten._to_copy.default                                                    (view_84,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_117                                                               aten.linear.default                                                      (_to_copy_18, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_39                                                               aten.dropout.default                                                     (linear_117, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_32                                                                   aten.div.Tensor                                                          (dropout_39, 1.0)                                                                                                                                              {}\n",
      "call_function  add_62                                                                   aten.add.Tensor                                                          (div_32, linear_113)                                                                                                                                           {}\n",
      "call_function  layer_norm_25                                                            aten.layer_norm.default                                                  (add_62, [1280], p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias)                     {}\n",
      "call_function  linear_118                                                               aten.linear.default                                                      (layer_norm_25, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_119                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_120                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_85                                                                  aten.view.default                                                        (linear_118, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_68                                                             aten.transpose.int                                                       (view_85, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_86                                                                  aten.view.default                                                        (linear_119, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_69                                                             aten.transpose.int                                                       (view_86, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_87                                                                  aten.view.default                                                        (linear_120, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_70                                                             aten.transpose.int                                                       (view_87, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_17                                          aten.scaled_dot_product_attention.default                                (transpose_68, transpose_69, transpose_70)                                                                                                                     {}\n",
      "call_function  transpose_71                                                             aten.transpose.int                                                       (scaled_dot_product_attention_17, 1, 2)                                                                                                                        {}\n",
      "call_function  view_88                                                                  aten.view.default                                                        (transpose_71, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_19                                                              aten._to_copy.default                                                    (view_88,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_121                                                               aten.linear.default                                                      (_to_copy_19, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_40                                                               aten.dropout.default                                                     (linear_121, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_33                                                                   aten.div.Tensor                                                          (dropout_40, 1.0)                                                                                                                                              {}\n",
      "call_function  add_63                                                                   aten.add.Tensor                                                          (div_33, add_62)                                                                                                                                               {}\n",
      "call_function  layer_norm_26                                                            aten.layer_norm.default                                                  (add_63, [1280], p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias)                     {}\n",
      "call_function  linear_122                                                               aten.linear.default                                                      (layer_norm_26, p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_8                                                                  aten.split.Tensor                                                        (linear_122, 5120, -1)                                                                                                                                         {}\n",
      "call_function  getitem_16                                                               <built-in function getitem>                                              (split_8, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_17                                                               <built-in function getitem>                                              (split_8, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_8                                                                   aten.gelu.default                                                        (getitem_17,)                                                                                                                                                  {}\n",
      "call_function  mul_11                                                                   aten.mul.Tensor                                                          (getitem_16, gelu_8)                                                                                                                                           {}\n",
      "call_function  dropout_41                                                               aten.dropout.default                                                     (mul_11, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_123                                                               aten.linear.default                                                      (dropout_41, p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_64                                                                   aten.add.Tensor                                                          (linear_123, add_63)                                                                                                                                           {}\n",
      "call_function  linear_124                                                               aten.linear.default                                                      (add_64, p_up_blocks_1_attentions_1_proj_out_weight, p_up_blocks_1_attentions_1_proj_out_bias)                                                                 {}\n",
      "call_function  view_89                                                                  aten.view.default                                                        (linear_124, [1, 16, 16, 1280])                                                                                                                                {}\n",
      "call_function  permute_17                                                               aten.permute.default                                                     (view_89, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_8                                                                  aten.clone.default                                                       (permute_17,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_65                                                                   aten.add.Tensor                                                          (clone_8, div_31)                                                                                                                                              {}\n",
      "call_function  cat_7                                                                    aten.cat.default                                                         ([add_65, conv2d_11], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_39                                                            aten.group_norm.default                                                  (cat_7, 32, p_up_blocks_1_resnets_2_norm1_weight, p_up_blocks_1_resnets_2_norm1_bias)                                                                          {}\n",
      "call_function  silu_46                                                                  aten.silu.default                                                        (group_norm_39,)                                                                                                                                               {}\n",
      "call_function  conv2d_42                                                                aten.conv2d.default                                                      (silu_46, p_up_blocks_1_resnets_2_conv1_weight, p_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_47                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_125                                                               aten.linear.default                                                      (silu_47, p_up_blocks_1_resnets_2_time_emb_proj_weight, p_up_blocks_1_resnets_2_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_37                                                                 aten.slice.Tensor                                                        (linear_125, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_38                                                                 aten.slice.Tensor                                                        (slice_37, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_32                                                             aten.unsqueeze.default                                                   (slice_38, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_33                                                             aten.unsqueeze.default                                                   (unsqueeze_32, 3)                                                                                                                                              {}\n",
      "call_function  add_66                                                                   aten.add.Tensor                                                          (conv2d_42, unsqueeze_33)                                                                                                                                      {}\n",
      "call_function  group_norm_40                                                            aten.group_norm.default                                                  (add_66, 32, p_up_blocks_1_resnets_2_norm2_weight, p_up_blocks_1_resnets_2_norm2_bias)                                                                         {}\n",
      "call_function  silu_48                                                                  aten.silu.default                                                        (group_norm_40,)                                                                                                                                               {}\n",
      "call_function  dropout_42                                                               aten.dropout.default                                                     (silu_48, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_43                                                                aten.conv2d.default                                                      (dropout_42, p_up_blocks_1_resnets_2_conv2_weight, p_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_44                                                                aten.conv2d.default                                                      (cat_7, p_up_blocks_1_resnets_2_conv_shortcut_weight, p_up_blocks_1_resnets_2_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_67                                                                   aten.add.Tensor                                                          (conv2d_44, conv2d_43)                                                                                                                                         {}\n",
      "call_function  div_34                                                                   aten.div.Tensor                                                          (add_67, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_41                                                            aten.group_norm.default                                                  (div_34, 32, p_up_blocks_1_attentions_2_norm_weight, p_up_blocks_1_attentions_2_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_18                                                               aten.permute.default                                                     (group_norm_41, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_90                                                                  aten.view.default                                                        (permute_18, [1, 256, 1280])                                                                                                                                   {}\n",
      "call_function  linear_126                                                               aten.linear.default                                                      (view_90, p_up_blocks_1_attentions_2_proj_in_weight, p_up_blocks_1_attentions_2_proj_in_bias)                                                                  {}\n",
      "call_function  layer_norm_27                                                            aten.layer_norm.default                                                  (linear_126, [1280], p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias)                 {}\n",
      "call_function  linear_127                                                               aten.linear.default                                                      (layer_norm_27, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_128                                                               aten.linear.default                                                      (layer_norm_27, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_129                                                               aten.linear.default                                                      (layer_norm_27, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_91                                                                  aten.view.default                                                        (linear_127, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_72                                                             aten.transpose.int                                                       (view_91, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_92                                                                  aten.view.default                                                        (linear_128, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_73                                                             aten.transpose.int                                                       (view_92, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_93                                                                  aten.view.default                                                        (linear_129, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_74                                                             aten.transpose.int                                                       (view_93, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_18                                          aten.scaled_dot_product_attention.default                                (transpose_72, transpose_73, transpose_74)                                                                                                                     {}\n",
      "call_function  transpose_75                                                             aten.transpose.int                                                       (scaled_dot_product_attention_18, 1, 2)                                                                                                                        {}\n",
      "call_function  view_94                                                                  aten.view.default                                                        (transpose_75, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_20                                                              aten._to_copy.default                                                    (view_94,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_130                                                               aten.linear.default                                                      (_to_copy_20, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_43                                                               aten.dropout.default                                                     (linear_130, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_35                                                                   aten.div.Tensor                                                          (dropout_43, 1.0)                                                                                                                                              {}\n",
      "call_function  add_68                                                                   aten.add.Tensor                                                          (div_35, linear_126)                                                                                                                                           {}\n",
      "call_function  layer_norm_28                                                            aten.layer_norm.default                                                  (add_68, [1280], p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias)                     {}\n",
      "call_function  linear_131                                                               aten.linear.default                                                      (layer_norm_28, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_132                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_133                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_95                                                                  aten.view.default                                                        (linear_131, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_76                                                             aten.transpose.int                                                       (view_95, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_96                                                                  aten.view.default                                                        (linear_132, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_77                                                             aten.transpose.int                                                       (view_96, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_97                                                                  aten.view.default                                                        (linear_133, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_78                                                             aten.transpose.int                                                       (view_97, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_19                                          aten.scaled_dot_product_attention.default                                (transpose_76, transpose_77, transpose_78)                                                                                                                     {}\n",
      "call_function  transpose_79                                                             aten.transpose.int                                                       (scaled_dot_product_attention_19, 1, 2)                                                                                                                        {}\n",
      "call_function  view_98                                                                  aten.view.default                                                        (transpose_79, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_21                                                              aten._to_copy.default                                                    (view_98,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_134                                                               aten.linear.default                                                      (_to_copy_21, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_44                                                               aten.dropout.default                                                     (linear_134, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_36                                                                   aten.div.Tensor                                                          (dropout_44, 1.0)                                                                                                                                              {}\n",
      "call_function  add_69                                                                   aten.add.Tensor                                                          (div_36, add_68)                                                                                                                                               {}\n",
      "call_function  layer_norm_29                                                            aten.layer_norm.default                                                  (add_69, [1280], p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias)                     {}\n",
      "call_function  linear_135                                                               aten.linear.default                                                      (layer_norm_29, p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_9                                                                  aten.split.Tensor                                                        (linear_135, 5120, -1)                                                                                                                                         {}\n",
      "call_function  getitem_18                                                               <built-in function getitem>                                              (split_9, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_19                                                               <built-in function getitem>                                              (split_9, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_9                                                                   aten.gelu.default                                                        (getitem_19,)                                                                                                                                                  {}\n",
      "call_function  mul_12                                                                   aten.mul.Tensor                                                          (getitem_18, gelu_9)                                                                                                                                           {}\n",
      "call_function  dropout_45                                                               aten.dropout.default                                                     (mul_12, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_136                                                               aten.linear.default                                                      (dropout_45, p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_70                                                                   aten.add.Tensor                                                          (linear_136, add_69)                                                                                                                                           {}\n",
      "call_function  linear_137                                                               aten.linear.default                                                      (add_70, p_up_blocks_1_attentions_2_proj_out_weight, p_up_blocks_1_attentions_2_proj_out_bias)                                                                 {}\n",
      "call_function  view_99                                                                  aten.view.default                                                        (linear_137, [1, 16, 16, 1280])                                                                                                                                {}\n",
      "call_function  permute_19                                                               aten.permute.default                                                     (view_99, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_9                                                                  aten.clone.default                                                       (permute_19,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_71                                                                   aten.add.Tensor                                                          (clone_9, div_34)                                                                                                                                              {}\n",
      "call_function  upsample_nearest2d_1                                                     aten.upsample_nearest2d.vec                                              (add_71, None, [2.0, 2.0])                                                                                                                                     {}\n",
      "call_function  conv2d_45                                                                aten.conv2d.default                                                      (upsample_nearest2d_1, p_up_blocks_1_upsamplers_0_conv_weight, p_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1])                                           {}\n",
      "call_function  cat_8                                                                    aten.cat.default                                                         ([conv2d_45, add_23], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_42                                                            aten.group_norm.default                                                  (cat_8, 32, p_up_blocks_2_resnets_0_norm1_weight, p_up_blocks_2_resnets_0_norm1_bias)                                                                          {}\n",
      "call_function  silu_49                                                                  aten.silu.default                                                        (group_norm_42,)                                                                                                                                               {}\n",
      "call_function  conv2d_46                                                                aten.conv2d.default                                                      (silu_49, p_up_blocks_2_resnets_0_conv1_weight, p_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_50                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_138                                                               aten.linear.default                                                      (silu_50, p_up_blocks_2_resnets_0_time_emb_proj_weight, p_up_blocks_2_resnets_0_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_39                                                                 aten.slice.Tensor                                                        (linear_138, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_40                                                                 aten.slice.Tensor                                                        (slice_39, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_34                                                             aten.unsqueeze.default                                                   (slice_40, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_35                                                             aten.unsqueeze.default                                                   (unsqueeze_34, 3)                                                                                                                                              {}\n",
      "call_function  add_72                                                                   aten.add.Tensor                                                          (conv2d_46, unsqueeze_35)                                                                                                                                      {}\n",
      "call_function  group_norm_43                                                            aten.group_norm.default                                                  (add_72, 32, p_up_blocks_2_resnets_0_norm2_weight, p_up_blocks_2_resnets_0_norm2_bias)                                                                         {}\n",
      "call_function  silu_51                                                                  aten.silu.default                                                        (group_norm_43,)                                                                                                                                               {}\n",
      "call_function  dropout_46                                                               aten.dropout.default                                                     (silu_51, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_47                                                                aten.conv2d.default                                                      (dropout_46, p_up_blocks_2_resnets_0_conv2_weight, p_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_48                                                                aten.conv2d.default                                                      (cat_8, p_up_blocks_2_resnets_0_conv_shortcut_weight, p_up_blocks_2_resnets_0_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_73                                                                   aten.add.Tensor                                                          (conv2d_48, conv2d_47)                                                                                                                                         {}\n",
      "call_function  div_37                                                                   aten.div.Tensor                                                          (add_73, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_44                                                            aten.group_norm.default                                                  (div_37, 32, p_up_blocks_2_attentions_0_norm_weight, p_up_blocks_2_attentions_0_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_20                                                               aten.permute.default                                                     (group_norm_44, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_100                                                                 aten.view.default                                                        (permute_20, [1, 1024, 640])                                                                                                                                   {}\n",
      "call_function  linear_139                                                               aten.linear.default                                                      (view_100, p_up_blocks_2_attentions_0_proj_in_weight, p_up_blocks_2_attentions_0_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_30                                                            aten.layer_norm.default                                                  (linear_139, [640], p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_140                                                               aten.linear.default                                                      (layer_norm_30, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_141                                                               aten.linear.default                                                      (layer_norm_30, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_142                                                               aten.linear.default                                                      (layer_norm_30, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_101                                                                 aten.view.default                                                        (linear_140, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_80                                                             aten.transpose.int                                                       (view_101, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_102                                                                 aten.view.default                                                        (linear_141, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_81                                                             aten.transpose.int                                                       (view_102, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_103                                                                 aten.view.default                                                        (linear_142, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_82                                                             aten.transpose.int                                                       (view_103, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_20                                          aten.scaled_dot_product_attention.default                                (transpose_80, transpose_81, transpose_82)                                                                                                                     {}\n",
      "call_function  transpose_83                                                             aten.transpose.int                                                       (scaled_dot_product_attention_20, 1, 2)                                                                                                                        {}\n",
      "call_function  view_104                                                                 aten.view.default                                                        (transpose_83, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_22                                                              aten._to_copy.default                                                    (view_104,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_143                                                               aten.linear.default                                                      (_to_copy_22, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_47                                                               aten.dropout.default                                                     (linear_143, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_38                                                                   aten.div.Tensor                                                          (dropout_47, 1.0)                                                                                                                                              {}\n",
      "call_function  add_74                                                                   aten.add.Tensor                                                          (div_38, linear_139)                                                                                                                                           {}\n",
      "call_function  layer_norm_31                                                            aten.layer_norm.default                                                  (add_74, [640], p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_144                                                               aten.linear.default                                                      (layer_norm_31, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_145                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_146                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_105                                                                 aten.view.default                                                        (linear_144, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_84                                                             aten.transpose.int                                                       (view_105, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_106                                                                 aten.view.default                                                        (linear_145, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_85                                                             aten.transpose.int                                                       (view_106, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_107                                                                 aten.view.default                                                        (linear_146, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_86                                                             aten.transpose.int                                                       (view_107, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_21                                          aten.scaled_dot_product_attention.default                                (transpose_84, transpose_85, transpose_86)                                                                                                                     {}\n",
      "call_function  transpose_87                                                             aten.transpose.int                                                       (scaled_dot_product_attention_21, 1, 2)                                                                                                                        {}\n",
      "call_function  view_108                                                                 aten.view.default                                                        (transpose_87, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_23                                                              aten._to_copy.default                                                    (view_108,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_147                                                               aten.linear.default                                                      (_to_copy_23, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_48                                                               aten.dropout.default                                                     (linear_147, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_39                                                                   aten.div.Tensor                                                          (dropout_48, 1.0)                                                                                                                                              {}\n",
      "call_function  add_75                                                                   aten.add.Tensor                                                          (div_39, add_74)                                                                                                                                               {}\n",
      "call_function  layer_norm_32                                                            aten.layer_norm.default                                                  (add_75, [640], p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_148                                                               aten.linear.default                                                      (layer_norm_32, p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_10                                                                 aten.split.Tensor                                                        (linear_148, 2560, -1)                                                                                                                                         {}\n",
      "call_function  getitem_20                                                               <built-in function getitem>                                              (split_10, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_21                                                               <built-in function getitem>                                              (split_10, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_10                                                                  aten.gelu.default                                                        (getitem_21,)                                                                                                                                                  {}\n",
      "call_function  mul_13                                                                   aten.mul.Tensor                                                          (getitem_20, gelu_10)                                                                                                                                          {}\n",
      "call_function  dropout_49                                                               aten.dropout.default                                                     (mul_13, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_149                                                               aten.linear.default                                                      (dropout_49, p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_76                                                                   aten.add.Tensor                                                          (linear_149, add_75)                                                                                                                                           {}\n",
      "call_function  linear_150                                                               aten.linear.default                                                      (add_76, p_up_blocks_2_attentions_0_proj_out_weight, p_up_blocks_2_attentions_0_proj_out_bias)                                                                 {}\n",
      "call_function  view_109                                                                 aten.view.default                                                        (linear_150, [1, 32, 32, 640])                                                                                                                                 {}\n",
      "call_function  permute_21                                                               aten.permute.default                                                     (view_109, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_10                                                                 aten.clone.default                                                       (permute_21,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_77                                                                   aten.add.Tensor                                                          (clone_10, div_37)                                                                                                                                             {}\n",
      "call_function  cat_9                                                                    aten.cat.default                                                         ([add_77, add_17], 1)                                                                                                                                          {}\n",
      "call_function  group_norm_45                                                            aten.group_norm.default                                                  (cat_9, 32, p_up_blocks_2_resnets_1_norm1_weight, p_up_blocks_2_resnets_1_norm1_bias)                                                                          {}\n",
      "call_function  silu_52                                                                  aten.silu.default                                                        (group_norm_45,)                                                                                                                                               {}\n",
      "call_function  conv2d_49                                                                aten.conv2d.default                                                      (silu_52, p_up_blocks_2_resnets_1_conv1_weight, p_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_53                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_151                                                               aten.linear.default                                                      (silu_53, p_up_blocks_2_resnets_1_time_emb_proj_weight, p_up_blocks_2_resnets_1_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_41                                                                 aten.slice.Tensor                                                        (linear_151, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_42                                                                 aten.slice.Tensor                                                        (slice_41, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_36                                                             aten.unsqueeze.default                                                   (slice_42, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_37                                                             aten.unsqueeze.default                                                   (unsqueeze_36, 3)                                                                                                                                              {}\n",
      "call_function  add_78                                                                   aten.add.Tensor                                                          (conv2d_49, unsqueeze_37)                                                                                                                                      {}\n",
      "call_function  group_norm_46                                                            aten.group_norm.default                                                  (add_78, 32, p_up_blocks_2_resnets_1_norm2_weight, p_up_blocks_2_resnets_1_norm2_bias)                                                                         {}\n",
      "call_function  silu_54                                                                  aten.silu.default                                                        (group_norm_46,)                                                                                                                                               {}\n",
      "call_function  dropout_50                                                               aten.dropout.default                                                     (silu_54, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_50                                                                aten.conv2d.default                                                      (dropout_50, p_up_blocks_2_resnets_1_conv2_weight, p_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_51                                                                aten.conv2d.default                                                      (cat_9, p_up_blocks_2_resnets_1_conv_shortcut_weight, p_up_blocks_2_resnets_1_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_79                                                                   aten.add.Tensor                                                          (conv2d_51, conv2d_50)                                                                                                                                         {}\n",
      "call_function  div_40                                                                   aten.div.Tensor                                                          (add_79, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_47                                                            aten.group_norm.default                                                  (div_40, 32, p_up_blocks_2_attentions_1_norm_weight, p_up_blocks_2_attentions_1_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_22                                                               aten.permute.default                                                     (group_norm_47, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_110                                                                 aten.view.default                                                        (permute_22, [1, 1024, 640])                                                                                                                                   {}\n",
      "call_function  linear_152                                                               aten.linear.default                                                      (view_110, p_up_blocks_2_attentions_1_proj_in_weight, p_up_blocks_2_attentions_1_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_33                                                            aten.layer_norm.default                                                  (linear_152, [640], p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_153                                                               aten.linear.default                                                      (layer_norm_33, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_154                                                               aten.linear.default                                                      (layer_norm_33, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_155                                                               aten.linear.default                                                      (layer_norm_33, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_111                                                                 aten.view.default                                                        (linear_153, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_88                                                             aten.transpose.int                                                       (view_111, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_112                                                                 aten.view.default                                                        (linear_154, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_89                                                             aten.transpose.int                                                       (view_112, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_113                                                                 aten.view.default                                                        (linear_155, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_90                                                             aten.transpose.int                                                       (view_113, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_22                                          aten.scaled_dot_product_attention.default                                (transpose_88, transpose_89, transpose_90)                                                                                                                     {}\n",
      "call_function  transpose_91                                                             aten.transpose.int                                                       (scaled_dot_product_attention_22, 1, 2)                                                                                                                        {}\n",
      "call_function  view_114                                                                 aten.view.default                                                        (transpose_91, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_24                                                              aten._to_copy.default                                                    (view_114,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_156                                                               aten.linear.default                                                      (_to_copy_24, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_51                                                               aten.dropout.default                                                     (linear_156, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_41                                                                   aten.div.Tensor                                                          (dropout_51, 1.0)                                                                                                                                              {}\n",
      "call_function  add_80                                                                   aten.add.Tensor                                                          (div_41, linear_152)                                                                                                                                           {}\n",
      "call_function  layer_norm_34                                                            aten.layer_norm.default                                                  (add_80, [640], p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_157                                                               aten.linear.default                                                      (layer_norm_34, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_158                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_159                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_115                                                                 aten.view.default                                                        (linear_157, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_92                                                             aten.transpose.int                                                       (view_115, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_116                                                                 aten.view.default                                                        (linear_158, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_93                                                             aten.transpose.int                                                       (view_116, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_117                                                                 aten.view.default                                                        (linear_159, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_94                                                             aten.transpose.int                                                       (view_117, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_23                                          aten.scaled_dot_product_attention.default                                (transpose_92, transpose_93, transpose_94)                                                                                                                     {}\n",
      "call_function  transpose_95                                                             aten.transpose.int                                                       (scaled_dot_product_attention_23, 1, 2)                                                                                                                        {}\n",
      "call_function  view_118                                                                 aten.view.default                                                        (transpose_95, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_25                                                              aten._to_copy.default                                                    (view_118,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_160                                                               aten.linear.default                                                      (_to_copy_25, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_52                                                               aten.dropout.default                                                     (linear_160, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_42                                                                   aten.div.Tensor                                                          (dropout_52, 1.0)                                                                                                                                              {}\n",
      "call_function  add_81                                                                   aten.add.Tensor                                                          (div_42, add_80)                                                                                                                                               {}\n",
      "call_function  layer_norm_35                                                            aten.layer_norm.default                                                  (add_81, [640], p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_161                                                               aten.linear.default                                                      (layer_norm_35, p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_11                                                                 aten.split.Tensor                                                        (linear_161, 2560, -1)                                                                                                                                         {}\n",
      "call_function  getitem_22                                                               <built-in function getitem>                                              (split_11, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_23                                                               <built-in function getitem>                                              (split_11, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_11                                                                  aten.gelu.default                                                        (getitem_23,)                                                                                                                                                  {}\n",
      "call_function  mul_14                                                                   aten.mul.Tensor                                                          (getitem_22, gelu_11)                                                                                                                                          {}\n",
      "call_function  dropout_53                                                               aten.dropout.default                                                     (mul_14, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_162                                                               aten.linear.default                                                      (dropout_53, p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_82                                                                   aten.add.Tensor                                                          (linear_162, add_81)                                                                                                                                           {}\n",
      "call_function  linear_163                                                               aten.linear.default                                                      (add_82, p_up_blocks_2_attentions_1_proj_out_weight, p_up_blocks_2_attentions_1_proj_out_bias)                                                                 {}\n",
      "call_function  view_119                                                                 aten.view.default                                                        (linear_163, [1, 32, 32, 640])                                                                                                                                 {}\n",
      "call_function  permute_23                                                               aten.permute.default                                                     (view_119, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_11                                                                 aten.clone.default                                                       (permute_23,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_83                                                                   aten.add.Tensor                                                          (clone_11, div_40)                                                                                                                                             {}\n",
      "call_function  cat_10                                                                   aten.cat.default                                                         ([add_83, conv2d_5], 1)                                                                                                                                        {}\n",
      "call_function  group_norm_48                                                            aten.group_norm.default                                                  (cat_10, 32, p_up_blocks_2_resnets_2_norm1_weight, p_up_blocks_2_resnets_2_norm1_bias)                                                                         {}\n",
      "call_function  silu_55                                                                  aten.silu.default                                                        (group_norm_48,)                                                                                                                                               {}\n",
      "call_function  conv2d_52                                                                aten.conv2d.default                                                      (silu_55, p_up_blocks_2_resnets_2_conv1_weight, p_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_56                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_164                                                               aten.linear.default                                                      (silu_56, p_up_blocks_2_resnets_2_time_emb_proj_weight, p_up_blocks_2_resnets_2_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_43                                                                 aten.slice.Tensor                                                        (linear_164, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_44                                                                 aten.slice.Tensor                                                        (slice_43, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_38                                                             aten.unsqueeze.default                                                   (slice_44, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_39                                                             aten.unsqueeze.default                                                   (unsqueeze_38, 3)                                                                                                                                              {}\n",
      "call_function  add_84                                                                   aten.add.Tensor                                                          (conv2d_52, unsqueeze_39)                                                                                                                                      {}\n",
      "call_function  group_norm_49                                                            aten.group_norm.default                                                  (add_84, 32, p_up_blocks_2_resnets_2_norm2_weight, p_up_blocks_2_resnets_2_norm2_bias)                                                                         {}\n",
      "call_function  silu_57                                                                  aten.silu.default                                                        (group_norm_49,)                                                                                                                                               {}\n",
      "call_function  dropout_54                                                               aten.dropout.default                                                     (silu_57, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_53                                                                aten.conv2d.default                                                      (dropout_54, p_up_blocks_2_resnets_2_conv2_weight, p_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_54                                                                aten.conv2d.default                                                      (cat_10, p_up_blocks_2_resnets_2_conv_shortcut_weight, p_up_blocks_2_resnets_2_conv_shortcut_bias)                                                             {}\n",
      "call_function  add_85                                                                   aten.add.Tensor                                                          (conv2d_54, conv2d_53)                                                                                                                                         {}\n",
      "call_function  div_43                                                                   aten.div.Tensor                                                          (add_85, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_50                                                            aten.group_norm.default                                                  (div_43, 32, p_up_blocks_2_attentions_2_norm_weight, p_up_blocks_2_attentions_2_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_24                                                               aten.permute.default                                                     (group_norm_50, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_120                                                                 aten.view.default                                                        (permute_24, [1, 1024, 640])                                                                                                                                   {}\n",
      "call_function  linear_165                                                               aten.linear.default                                                      (view_120, p_up_blocks_2_attentions_2_proj_in_weight, p_up_blocks_2_attentions_2_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_36                                                            aten.layer_norm.default                                                  (linear_165, [640], p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_166                                                               aten.linear.default                                                      (layer_norm_36, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_167                                                               aten.linear.default                                                      (layer_norm_36, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_168                                                               aten.linear.default                                                      (layer_norm_36, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_121                                                                 aten.view.default                                                        (linear_166, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_96                                                             aten.transpose.int                                                       (view_121, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_122                                                                 aten.view.default                                                        (linear_167, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_97                                                             aten.transpose.int                                                       (view_122, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_123                                                                 aten.view.default                                                        (linear_168, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_98                                                             aten.transpose.int                                                       (view_123, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_24                                          aten.scaled_dot_product_attention.default                                (transpose_96, transpose_97, transpose_98)                                                                                                                     {}\n",
      "call_function  transpose_99                                                             aten.transpose.int                                                       (scaled_dot_product_attention_24, 1, 2)                                                                                                                        {}\n",
      "call_function  view_124                                                                 aten.view.default                                                        (transpose_99, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_26                                                              aten._to_copy.default                                                    (view_124,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_169                                                               aten.linear.default                                                      (_to_copy_26, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_55                                                               aten.dropout.default                                                     (linear_169, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_44                                                                   aten.div.Tensor                                                          (dropout_55, 1.0)                                                                                                                                              {}\n",
      "call_function  add_86                                                                   aten.add.Tensor                                                          (div_44, linear_165)                                                                                                                                           {}\n",
      "call_function  layer_norm_37                                                            aten.layer_norm.default                                                  (add_86, [640], p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_170                                                               aten.linear.default                                                      (layer_norm_37, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_171                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_172                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_125                                                                 aten.view.default                                                        (linear_170, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_100                                                            aten.transpose.int                                                       (view_125, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_126                                                                 aten.view.default                                                        (linear_171, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_101                                                            aten.transpose.int                                                       (view_126, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_127                                                                 aten.view.default                                                        (linear_172, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_102                                                            aten.transpose.int                                                       (view_127, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_25                                          aten.scaled_dot_product_attention.default                                (transpose_100, transpose_101, transpose_102)                                                                                                                  {}\n",
      "call_function  transpose_103                                                            aten.transpose.int                                                       (scaled_dot_product_attention_25, 1, 2)                                                                                                                        {}\n",
      "call_function  view_128                                                                 aten.view.default                                                        (transpose_103, [1, -1, 640])                                                                                                                                  {}\n",
      "call_function  _to_copy_27                                                              aten._to_copy.default                                                    (view_128,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_173                                                               aten.linear.default                                                      (_to_copy_27, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_56                                                               aten.dropout.default                                                     (linear_173, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_45                                                                   aten.div.Tensor                                                          (dropout_56, 1.0)                                                                                                                                              {}\n",
      "call_function  add_87                                                                   aten.add.Tensor                                                          (div_45, add_86)                                                                                                                                               {}\n",
      "call_function  layer_norm_38                                                            aten.layer_norm.default                                                  (add_87, [640], p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_174                                                               aten.linear.default                                                      (layer_norm_38, p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_12                                                                 aten.split.Tensor                                                        (linear_174, 2560, -1)                                                                                                                                         {}\n",
      "call_function  getitem_24                                                               <built-in function getitem>                                              (split_12, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_25                                                               <built-in function getitem>                                              (split_12, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_12                                                                  aten.gelu.default                                                        (getitem_25,)                                                                                                                                                  {}\n",
      "call_function  mul_15                                                                   aten.mul.Tensor                                                          (getitem_24, gelu_12)                                                                                                                                          {}\n",
      "call_function  dropout_57                                                               aten.dropout.default                                                     (mul_15, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_175                                                               aten.linear.default                                                      (dropout_57, p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_88                                                                   aten.add.Tensor                                                          (linear_175, add_87)                                                                                                                                           {}\n",
      "call_function  linear_176                                                               aten.linear.default                                                      (add_88, p_up_blocks_2_attentions_2_proj_out_weight, p_up_blocks_2_attentions_2_proj_out_bias)                                                                 {}\n",
      "call_function  view_129                                                                 aten.view.default                                                        (linear_176, [1, 32, 32, 640])                                                                                                                                 {}\n",
      "call_function  permute_25                                                               aten.permute.default                                                     (view_129, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_12                                                                 aten.clone.default                                                       (permute_25,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_89                                                                   aten.add.Tensor                                                          (clone_12, div_43)                                                                                                                                             {}\n",
      "call_function  upsample_nearest2d_2                                                     aten.upsample_nearest2d.vec                                              (add_89, None, [2.0, 2.0])                                                                                                                                     {}\n",
      "call_function  conv2d_55                                                                aten.conv2d.default                                                      (upsample_nearest2d_2, p_up_blocks_2_upsamplers_0_conv_weight, p_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1])                                           {}\n",
      "call_function  cat_11                                                                   aten.cat.default                                                         ([conv2d_55, add_11], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_51                                                            aten.group_norm.default                                                  (cat_11, 32, p_up_blocks_3_resnets_0_norm1_weight, p_up_blocks_3_resnets_0_norm1_bias)                                                                         {}\n",
      "call_function  silu_58                                                                  aten.silu.default                                                        (group_norm_51,)                                                                                                                                               {}\n",
      "call_function  conv2d_56                                                                aten.conv2d.default                                                      (silu_58, p_up_blocks_3_resnets_0_conv1_weight, p_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_59                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_177                                                               aten.linear.default                                                      (silu_59, p_up_blocks_3_resnets_0_time_emb_proj_weight, p_up_blocks_3_resnets_0_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_45                                                                 aten.slice.Tensor                                                        (linear_177, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_46                                                                 aten.slice.Tensor                                                        (slice_45, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_40                                                             aten.unsqueeze.default                                                   (slice_46, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_41                                                             aten.unsqueeze.default                                                   (unsqueeze_40, 3)                                                                                                                                              {}\n",
      "call_function  add_90                                                                   aten.add.Tensor                                                          (conv2d_56, unsqueeze_41)                                                                                                                                      {}\n",
      "call_function  group_norm_52                                                            aten.group_norm.default                                                  (add_90, 32, p_up_blocks_3_resnets_0_norm2_weight, p_up_blocks_3_resnets_0_norm2_bias)                                                                         {}\n",
      "call_function  silu_60                                                                  aten.silu.default                                                        (group_norm_52,)                                                                                                                                               {}\n",
      "call_function  dropout_58                                                               aten.dropout.default                                                     (silu_60, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_57                                                                aten.conv2d.default                                                      (dropout_58, p_up_blocks_3_resnets_0_conv2_weight, p_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_58                                                                aten.conv2d.default                                                      (cat_11, p_up_blocks_3_resnets_0_conv_shortcut_weight, p_up_blocks_3_resnets_0_conv_shortcut_bias)                                                             {}\n",
      "call_function  add_91                                                                   aten.add.Tensor                                                          (conv2d_58, conv2d_57)                                                                                                                                         {}\n",
      "call_function  div_46                                                                   aten.div.Tensor                                                          (add_91, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_53                                                            aten.group_norm.default                                                  (div_46, 32, p_up_blocks_3_attentions_0_norm_weight, p_up_blocks_3_attentions_0_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_26                                                               aten.permute.default                                                     (group_norm_53, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_130                                                                 aten.view.default                                                        (permute_26, [1, 4096, 320])                                                                                                                                   {}\n",
      "call_function  linear_178                                                               aten.linear.default                                                      (view_130, p_up_blocks_3_attentions_0_proj_in_weight, p_up_blocks_3_attentions_0_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_39                                                            aten.layer_norm.default                                                  (linear_178, [320], p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_179                                                               aten.linear.default                                                      (layer_norm_39, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_180                                                               aten.linear.default                                                      (layer_norm_39, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_181                                                               aten.linear.default                                                      (layer_norm_39, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_131                                                                 aten.view.default                                                        (linear_179, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_104                                                            aten.transpose.int                                                       (view_131, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_132                                                                 aten.view.default                                                        (linear_180, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_105                                                            aten.transpose.int                                                       (view_132, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_133                                                                 aten.view.default                                                        (linear_181, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_106                                                            aten.transpose.int                                                       (view_133, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_26                                          aten.scaled_dot_product_attention.default                                (transpose_104, transpose_105, transpose_106)                                                                                                                  {}\n",
      "call_function  transpose_107                                                            aten.transpose.int                                                       (scaled_dot_product_attention_26, 1, 2)                                                                                                                        {}\n",
      "call_function  view_134                                                                 aten.view.default                                                        (transpose_107, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_28                                                              aten._to_copy.default                                                    (view_134,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_182                                                               aten.linear.default                                                      (_to_copy_28, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_59                                                               aten.dropout.default                                                     (linear_182, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_47                                                                   aten.div.Tensor                                                          (dropout_59, 1.0)                                                                                                                                              {}\n",
      "call_function  add_92                                                                   aten.add.Tensor                                                          (div_47, linear_178)                                                                                                                                           {}\n",
      "call_function  layer_norm_40                                                            aten.layer_norm.default                                                  (add_92, [320], p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_183                                                               aten.linear.default                                                      (layer_norm_40, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_184                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_185                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_135                                                                 aten.view.default                                                        (linear_183, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_108                                                            aten.transpose.int                                                       (view_135, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_136                                                                 aten.view.default                                                        (linear_184, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_109                                                            aten.transpose.int                                                       (view_136, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_137                                                                 aten.view.default                                                        (linear_185, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_110                                                            aten.transpose.int                                                       (view_137, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_27                                          aten.scaled_dot_product_attention.default                                (transpose_108, transpose_109, transpose_110)                                                                                                                  {}\n",
      "call_function  transpose_111                                                            aten.transpose.int                                                       (scaled_dot_product_attention_27, 1, 2)                                                                                                                        {}\n",
      "call_function  view_138                                                                 aten.view.default                                                        (transpose_111, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_29                                                              aten._to_copy.default                                                    (view_138,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_186                                                               aten.linear.default                                                      (_to_copy_29, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_60                                                               aten.dropout.default                                                     (linear_186, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_48                                                                   aten.div.Tensor                                                          (dropout_60, 1.0)                                                                                                                                              {}\n",
      "call_function  add_93                                                                   aten.add.Tensor                                                          (div_48, add_92)                                                                                                                                               {}\n",
      "call_function  layer_norm_41                                                            aten.layer_norm.default                                                  (add_93, [320], p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_187                                                               aten.linear.default                                                      (layer_norm_41, p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_13                                                                 aten.split.Tensor                                                        (linear_187, 1280, -1)                                                                                                                                         {}\n",
      "call_function  getitem_26                                                               <built-in function getitem>                                              (split_13, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_27                                                               <built-in function getitem>                                              (split_13, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_13                                                                  aten.gelu.default                                                        (getitem_27,)                                                                                                                                                  {}\n",
      "call_function  mul_16                                                                   aten.mul.Tensor                                                          (getitem_26, gelu_13)                                                                                                                                          {}\n",
      "call_function  dropout_61                                                               aten.dropout.default                                                     (mul_16, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_188                                                               aten.linear.default                                                      (dropout_61, p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_94                                                                   aten.add.Tensor                                                          (linear_188, add_93)                                                                                                                                           {}\n",
      "call_function  linear_189                                                               aten.linear.default                                                      (add_94, p_up_blocks_3_attentions_0_proj_out_weight, p_up_blocks_3_attentions_0_proj_out_bias)                                                                 {}\n",
      "call_function  view_139                                                                 aten.view.default                                                        (linear_189, [1, 64, 64, 320])                                                                                                                                 {}\n",
      "call_function  permute_27                                                               aten.permute.default                                                     (view_139, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_13                                                                 aten.clone.default                                                       (permute_27,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_95                                                                   aten.add.Tensor                                                          (clone_13, div_46)                                                                                                                                             {}\n",
      "call_function  cat_12                                                                   aten.cat.default                                                         ([add_95, add_5], 1)                                                                                                                                           {}\n",
      "call_function  group_norm_54                                                            aten.group_norm.default                                                  (cat_12, 32, p_up_blocks_3_resnets_1_norm1_weight, p_up_blocks_3_resnets_1_norm1_bias)                                                                         {}\n",
      "call_function  silu_61                                                                  aten.silu.default                                                        (group_norm_54,)                                                                                                                                               {}\n",
      "call_function  conv2d_59                                                                aten.conv2d.default                                                      (silu_61, p_up_blocks_3_resnets_1_conv1_weight, p_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_62                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_190                                                               aten.linear.default                                                      (silu_62, p_up_blocks_3_resnets_1_time_emb_proj_weight, p_up_blocks_3_resnets_1_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_47                                                                 aten.slice.Tensor                                                        (linear_190, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_48                                                                 aten.slice.Tensor                                                        (slice_47, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_42                                                             aten.unsqueeze.default                                                   (slice_48, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_43                                                             aten.unsqueeze.default                                                   (unsqueeze_42, 3)                                                                                                                                              {}\n",
      "call_function  add_96                                                                   aten.add.Tensor                                                          (conv2d_59, unsqueeze_43)                                                                                                                                      {}\n",
      "call_function  group_norm_55                                                            aten.group_norm.default                                                  (add_96, 32, p_up_blocks_3_resnets_1_norm2_weight, p_up_blocks_3_resnets_1_norm2_bias)                                                                         {}\n",
      "call_function  silu_63                                                                  aten.silu.default                                                        (group_norm_55,)                                                                                                                                               {}\n",
      "call_function  dropout_62                                                               aten.dropout.default                                                     (silu_63, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_60                                                                aten.conv2d.default                                                      (dropout_62, p_up_blocks_3_resnets_1_conv2_weight, p_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_61                                                                aten.conv2d.default                                                      (cat_12, p_up_blocks_3_resnets_1_conv_shortcut_weight, p_up_blocks_3_resnets_1_conv_shortcut_bias)                                                             {}\n",
      "call_function  add_97                                                                   aten.add.Tensor                                                          (conv2d_61, conv2d_60)                                                                                                                                         {}\n",
      "call_function  div_49                                                                   aten.div.Tensor                                                          (add_97, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_56                                                            aten.group_norm.default                                                  (div_49, 32, p_up_blocks_3_attentions_1_norm_weight, p_up_blocks_3_attentions_1_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_28                                                               aten.permute.default                                                     (group_norm_56, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_140                                                                 aten.view.default                                                        (permute_28, [1, 4096, 320])                                                                                                                                   {}\n",
      "call_function  linear_191                                                               aten.linear.default                                                      (view_140, p_up_blocks_3_attentions_1_proj_in_weight, p_up_blocks_3_attentions_1_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_42                                                            aten.layer_norm.default                                                  (linear_191, [320], p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_192                                                               aten.linear.default                                                      (layer_norm_42, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_193                                                               aten.linear.default                                                      (layer_norm_42, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_194                                                               aten.linear.default                                                      (layer_norm_42, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_141                                                                 aten.view.default                                                        (linear_192, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_112                                                            aten.transpose.int                                                       (view_141, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_142                                                                 aten.view.default                                                        (linear_193, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_113                                                            aten.transpose.int                                                       (view_142, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_143                                                                 aten.view.default                                                        (linear_194, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_114                                                            aten.transpose.int                                                       (view_143, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_28                                          aten.scaled_dot_product_attention.default                                (transpose_112, transpose_113, transpose_114)                                                                                                                  {}\n",
      "call_function  transpose_115                                                            aten.transpose.int                                                       (scaled_dot_product_attention_28, 1, 2)                                                                                                                        {}\n",
      "call_function  view_144                                                                 aten.view.default                                                        (transpose_115, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_30                                                              aten._to_copy.default                                                    (view_144,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_195                                                               aten.linear.default                                                      (_to_copy_30, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_63                                                               aten.dropout.default                                                     (linear_195, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_50                                                                   aten.div.Tensor                                                          (dropout_63, 1.0)                                                                                                                                              {}\n",
      "call_function  add_98                                                                   aten.add.Tensor                                                          (div_50, linear_191)                                                                                                                                           {}\n",
      "call_function  layer_norm_43                                                            aten.layer_norm.default                                                  (add_98, [320], p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_196                                                               aten.linear.default                                                      (layer_norm_43, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_197                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_198                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_145                                                                 aten.view.default                                                        (linear_196, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_116                                                            aten.transpose.int                                                       (view_145, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_146                                                                 aten.view.default                                                        (linear_197, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_117                                                            aten.transpose.int                                                       (view_146, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_147                                                                 aten.view.default                                                        (linear_198, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_118                                                            aten.transpose.int                                                       (view_147, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_29                                          aten.scaled_dot_product_attention.default                                (transpose_116, transpose_117, transpose_118)                                                                                                                  {}\n",
      "call_function  transpose_119                                                            aten.transpose.int                                                       (scaled_dot_product_attention_29, 1, 2)                                                                                                                        {}\n",
      "call_function  view_148                                                                 aten.view.default                                                        (transpose_119, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_31                                                              aten._to_copy.default                                                    (view_148,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_199                                                               aten.linear.default                                                      (_to_copy_31, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_64                                                               aten.dropout.default                                                     (linear_199, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_51                                                                   aten.div.Tensor                                                          (dropout_64, 1.0)                                                                                                                                              {}\n",
      "call_function  add_99                                                                   aten.add.Tensor                                                          (div_51, add_98)                                                                                                                                               {}\n",
      "call_function  layer_norm_44                                                            aten.layer_norm.default                                                  (add_99, [320], p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_200                                                               aten.linear.default                                                      (layer_norm_44, p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_14                                                                 aten.split.Tensor                                                        (linear_200, 1280, -1)                                                                                                                                         {}\n",
      "call_function  getitem_28                                                               <built-in function getitem>                                              (split_14, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_29                                                               <built-in function getitem>                                              (split_14, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_14                                                                  aten.gelu.default                                                        (getitem_29,)                                                                                                                                                  {}\n",
      "call_function  mul_17                                                                   aten.mul.Tensor                                                          (getitem_28, gelu_14)                                                                                                                                          {}\n",
      "call_function  dropout_65                                                               aten.dropout.default                                                     (mul_17, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_201                                                               aten.linear.default                                                      (dropout_65, p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_100                                                                  aten.add.Tensor                                                          (linear_201, add_99)                                                                                                                                           {}\n",
      "call_function  linear_202                                                               aten.linear.default                                                      (add_100, p_up_blocks_3_attentions_1_proj_out_weight, p_up_blocks_3_attentions_1_proj_out_bias)                                                                {}\n",
      "call_function  view_149                                                                 aten.view.default                                                        (linear_202, [1, 64, 64, 320])                                                                                                                                 {}\n",
      "call_function  permute_29                                                               aten.permute.default                                                     (view_149, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_14                                                                 aten.clone.default                                                       (permute_29,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_101                                                                  aten.add.Tensor                                                          (clone_14, div_49)                                                                                                                                             {}\n",
      "call_function  cat_13                                                                   aten.cat.default                                                         ([add_101, conv2d], 1)                                                                                                                                         {}\n",
      "call_function  group_norm_57                                                            aten.group_norm.default                                                  (cat_13, 32, p_up_blocks_3_resnets_2_norm1_weight, p_up_blocks_3_resnets_2_norm1_bias)                                                                         {}\n",
      "call_function  silu_64                                                                  aten.silu.default                                                        (group_norm_57,)                                                                                                                                               {}\n",
      "call_function  conv2d_62                                                                aten.conv2d.default                                                      (silu_64, p_up_blocks_3_resnets_2_conv1_weight, p_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_65                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_203                                                               aten.linear.default                                                      (silu_65, p_up_blocks_3_resnets_2_time_emb_proj_weight, p_up_blocks_3_resnets_2_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_49                                                                 aten.slice.Tensor                                                        (linear_203, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_50                                                                 aten.slice.Tensor                                                        (slice_49, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_44                                                             aten.unsqueeze.default                                                   (slice_50, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_45                                                             aten.unsqueeze.default                                                   (unsqueeze_44, 3)                                                                                                                                              {}\n",
      "call_function  add_102                                                                  aten.add.Tensor                                                          (conv2d_62, unsqueeze_45)                                                                                                                                      {}\n",
      "call_function  group_norm_58                                                            aten.group_norm.default                                                  (add_102, 32, p_up_blocks_3_resnets_2_norm2_weight, p_up_blocks_3_resnets_2_norm2_bias)                                                                        {}\n",
      "call_function  silu_66                                                                  aten.silu.default                                                        (group_norm_58,)                                                                                                                                               {}\n",
      "call_function  dropout_66                                                               aten.dropout.default                                                     (silu_66, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_63                                                                aten.conv2d.default                                                      (dropout_66, p_up_blocks_3_resnets_2_conv2_weight, p_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_64                                                                aten.conv2d.default                                                      (cat_13, p_up_blocks_3_resnets_2_conv_shortcut_weight, p_up_blocks_3_resnets_2_conv_shortcut_bias)                                                             {}\n",
      "call_function  add_103                                                                  aten.add.Tensor                                                          (conv2d_64, conv2d_63)                                                                                                                                         {}\n",
      "call_function  div_52                                                                   aten.div.Tensor                                                          (add_103, 1.0)                                                                                                                                                 {}\n",
      "call_function  group_norm_59                                                            aten.group_norm.default                                                  (div_52, 32, p_up_blocks_3_attentions_2_norm_weight, p_up_blocks_3_attentions_2_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_30                                                               aten.permute.default                                                     (group_norm_59, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_150                                                                 aten.view.default                                                        (permute_30, [1, 4096, 320])                                                                                                                                   {}\n",
      "call_function  linear_204                                                               aten.linear.default                                                      (view_150, p_up_blocks_3_attentions_2_proj_in_weight, p_up_blocks_3_attentions_2_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_45                                                            aten.layer_norm.default                                                  (linear_204, [320], p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_205                                                               aten.linear.default                                                      (layer_norm_45, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_206                                                               aten.linear.default                                                      (layer_norm_45, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_207                                                               aten.linear.default                                                      (layer_norm_45, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_151                                                                 aten.view.default                                                        (linear_205, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_120                                                            aten.transpose.int                                                       (view_151, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_152                                                                 aten.view.default                                                        (linear_206, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_121                                                            aten.transpose.int                                                       (view_152, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_153                                                                 aten.view.default                                                        (linear_207, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_122                                                            aten.transpose.int                                                       (view_153, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_30                                          aten.scaled_dot_product_attention.default                                (transpose_120, transpose_121, transpose_122)                                                                                                                  {}\n",
      "call_function  transpose_123                                                            aten.transpose.int                                                       (scaled_dot_product_attention_30, 1, 2)                                                                                                                        {}\n",
      "call_function  view_154                                                                 aten.view.default                                                        (transpose_123, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_32                                                              aten._to_copy.default                                                    (view_154,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_208                                                               aten.linear.default                                                      (_to_copy_32, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_67                                                               aten.dropout.default                                                     (linear_208, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_53                                                                   aten.div.Tensor                                                          (dropout_67, 1.0)                                                                                                                                              {}\n",
      "call_function  add_104                                                                  aten.add.Tensor                                                          (div_53, linear_204)                                                                                                                                           {}\n",
      "call_function  layer_norm_46                                                            aten.layer_norm.default                                                  (add_104, [320], p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias)                     {}\n",
      "call_function  linear_209                                                               aten.linear.default                                                      (layer_norm_46, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_210                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_211                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_155                                                                 aten.view.default                                                        (linear_209, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_124                                                            aten.transpose.int                                                       (view_155, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_156                                                                 aten.view.default                                                        (linear_210, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_125                                                            aten.transpose.int                                                       (view_156, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_157                                                                 aten.view.default                                                        (linear_211, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_126                                                            aten.transpose.int                                                       (view_157, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_31                                          aten.scaled_dot_product_attention.default                                (transpose_124, transpose_125, transpose_126)                                                                                                                  {}\n",
      "call_function  transpose_127                                                            aten.transpose.int                                                       (scaled_dot_product_attention_31, 1, 2)                                                                                                                        {}\n",
      "call_function  view_158                                                                 aten.view.default                                                        (transpose_127, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_33                                                              aten._to_copy.default                                                    (view_158,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_212                                                               aten.linear.default                                                      (_to_copy_33, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_68                                                               aten.dropout.default                                                     (linear_212, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_54                                                                   aten.div.Tensor                                                          (dropout_68, 1.0)                                                                                                                                              {}\n",
      "call_function  add_105                                                                  aten.add.Tensor                                                          (div_54, add_104)                                                                                                                                              {}\n",
      "call_function  layer_norm_47                                                            aten.layer_norm.default                                                  (add_105, [320], p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias)                     {}\n",
      "call_function  linear_213                                                               aten.linear.default                                                      (layer_norm_47, p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_15                                                                 aten.split.Tensor                                                        (linear_213, 1280, -1)                                                                                                                                         {}\n",
      "call_function  getitem_30                                                               <built-in function getitem>                                              (split_15, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_31                                                               <built-in function getitem>                                              (split_15, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_15                                                                  aten.gelu.default                                                        (getitem_31,)                                                                                                                                                  {}\n",
      "call_function  mul_18                                                                   aten.mul.Tensor                                                          (getitem_30, gelu_15)                                                                                                                                          {}\n",
      "call_function  dropout_69                                                               aten.dropout.default                                                     (mul_18, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_214                                                               aten.linear.default                                                      (dropout_69, p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_106                                                                  aten.add.Tensor                                                          (linear_214, add_105)                                                                                                                                          {}\n",
      "call_function  linear_215                                                               aten.linear.default                                                      (add_106, p_up_blocks_3_attentions_2_proj_out_weight, p_up_blocks_3_attentions_2_proj_out_bias)                                                                {}\n",
      "call_function  view_159                                                                 aten.view.default                                                        (linear_215, [1, 64, 64, 320])                                                                                                                                 {}\n",
      "call_function  permute_31                                                               aten.permute.default                                                     (view_159, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_15                                                                 aten.clone.default                                                       (permute_31,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_107                                                                  aten.add.Tensor                                                          (clone_15, div_52)                                                                                                                                             {}\n",
      "call_function  group_norm_60                                                            aten.group_norm.default                                                  (add_107, 32, p_conv_norm_out_weight, p_conv_norm_out_bias)                                                                                                    {}\n",
      "call_function  silu_67                                                                  aten.silu.default                                                        (group_norm_60,)                                                                                                                                               {}\n",
      "call_function  conv2d_65                                                                aten.conv2d.default                                                      (silu_67, p_conv_out_weight, p_conv_out_bias, [1, 1], [1, 1])                                                                                                  {}\n",
      "output         output                                                                   output                                                                   ((conv2d_65,),)                                                                                                                                                {}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import UNet2DConditionModel\n",
    "from torch.export import export, save\n",
    "from torch.export import ExportedProgram\n",
    "# working\n",
    "CKPT  = \"prs-eth/marigold-depth-v1-1\"\n",
    "unet  = UNet2DConditionModel.from_pretrained(CKPT, subfolder=\"unet\").cpu()\n",
    "unet.disable_xformers_memory_efficient_attention()\n",
    "\n",
    "example = (\n",
    "    torch.randn(1, 8, 64, 64),   # latent\n",
    "    torch.tensor([0]),           # timestep\n",
    "    torch.randn(1, 77, 1024)     # text enc\n",
    ")\n",
    "\n",
    "gm_unet: ExportedProgram = export(unet, example)\n",
    "save(gm_unet, \"unet_fp32.ep\")\n",
    "gm_unet.graph.print_tabular()\n",
    "\n",
    "\n",
    "# gm_unet = export(unet, example)       # ← this is already a GraphModule\n",
    "# gm_unet.graph.print_tabular()         # nicely formatted table\n",
    "# # or:\n",
    "# print(gm_unet.graph)                  # raw ATen graph\n",
    "\n",
    "# save(gm_unet, \"unet_fp32.ep\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8238dee7",
   "metadata": {},
   "source": [
    "## swap out the unsupported ops from the unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee1a1d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupported/Custom Ops to Handle:\n",
      "{'aten.conv2d.default', 'aten.gelu.default', 'aten.dropout.default', 'aten.unsqueeze.default', 'aten.cat.default', 'aten.layer_norm.default', 'aten.silu.default', 'aten.div.Tensor', 'aten.sin.default', 'aten.clone.default', 'aten.linear.default', 'aten.group_norm.default', 'aten.upsample_nearest2d.vec', 'aten.expand.default', 'aten.cos.default', 'aten._to_copy.default', 'aten.permute.default', 'aten.transpose.int', 'aten.mul.Tensor', 'aten.slice.Tensor', 'aten.add.Tensor', 'aten.exp.default', 'aten.view.default', 'aten.split.Tensor', 'aten.scaled_dot_product_attention.default', '<built-in function getitem>', 'aten.arange.start'}\n"
     ]
    }
   ],
   "source": [
    "unsupported_ops = set()\n",
    "for node in gm_unet.graph.nodes:\n",
    "    if node.op == \"call_function\" and \"quant\" not in str(node.target):\n",
    "        # Add your own conditions here if targeting quantization\n",
    "        unsupported_ops.add(str(node.target))\n",
    "\n",
    "print(\"Unsupported/Custom Ops to Handle:\")\n",
    "print(unsupported_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c84209",
   "metadata": {},
   "source": [
    "## This is for comaporing the graph module with the origional model (UNET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2193a0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_export/serde/serialize.py:317: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  artifact = torch.load(buffer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max |Δ| = tensor(0., grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from torch.export import load\n",
    "\n",
    "# 1. load the exported program\n",
    "ep = load(\"unet_fp32.ep\")\n",
    "\n",
    "# 2. get the GraphModule that does the work\n",
    "gm = ep.module()          # <-- now it's an ordinary nn.Module\n",
    "\n",
    "# 3. run the same inputs you used for export\n",
    "example = (\n",
    "    torch.randn(1, 8, 64, 64),   # latent\n",
    "    torch.tensor([0]),           # timestep\n",
    "    torch.randn(1, 77, 1024)     # text enc\n",
    ")\n",
    "\n",
    "out1 = gm(*example)       # exported graph\n",
    "out2 = unet(*example)     # original eager model\n",
    "\n",
    "print(\"max |Δ| =\", (out1.sample - out2.sample).abs().max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ca1c5",
   "metadata": {},
   "source": [
    "## another way that the implmnetnation works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae4fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Exported UNet saved to: unet_fp32.ep\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# import torch\n",
    "# from diffusers import UNet2DConditionModel\n",
    "# from torch.export import export, save\n",
    "# from torch.utils._pytree import (\n",
    "#     register_pytree_node, SUPPORTED_NODES   # SUPPORTED_NODES = current registry\n",
    "# )\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# # 1) Locate UNet2DConditionOutput regardless of diffusers version\n",
    "# # ------------------------------------------------------------------\n",
    "# def _find_unet_output_class():\n",
    "#     # new layout (>=0.24)\n",
    "#     try:\n",
    "#         return importlib.import_module(\n",
    "#             \"diffusers.models.unets.unet_2d_condition\"\n",
    "#         ).UNet2DConditionOutput\n",
    "#     except (ModuleNotFoundError, AttributeError):\n",
    "#         pass\n",
    "#     # old layout (<=0.23)\n",
    "#     try:\n",
    "#         return importlib.import_module(\n",
    "#             \"diffusers.models.unet_2d_condition\"\n",
    "#         ).UNet2DConditionOutput\n",
    "#     except (ModuleNotFoundError, AttributeError):\n",
    "#         pass\n",
    "#     raise RuntimeError(\"Could not locate UNet2DConditionOutput\")\n",
    "\n",
    "# UNet2DConditionOutput = _find_unet_output_class()\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# # 2) Register as a pytree **only if not registered yet**\n",
    "# # ------------------------------------------------------------------\n",
    "# if UNet2DConditionOutput not in SUPPORTED_NODES:\n",
    "#     def _flatten(o: UNet2DConditionOutput):\n",
    "#         return ((o.sample,), None)      # children, context\n",
    "\n",
    "#     def _unflatten(ctx, children):\n",
    "#         (sample,) = children\n",
    "#         return UNet2DConditionOutput(sample=sample)\n",
    "\n",
    "#     register_pytree_node(\n",
    "#         UNet2DConditionOutput, _flatten, _unflatten,\n",
    "#         serialized_type_name=\"UNet2DConditionOutput\"\n",
    "#     )\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# # 3) Load the fp32 UNet\n",
    "# # ------------------------------------------------------------------\n",
    "# CKPT = \"prs-eth/marigold-depth-v1-1\"\n",
    "# unet = UNet2DConditionModel.from_pretrained(CKPT, subfolder=\"unet\").cpu()\n",
    "# unet.disable_xformers_memory_efficient_attention()   # avoid un-exportable ops\n",
    "# unet.eval()\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# # 4) Example inputs (realistic shapes)\n",
    "# # ------------------------------------------------------------------\n",
    "# example_inputs = (\n",
    "#     torch.randn(1, 8, 64, 64),   # latent\n",
    "#     torch.tensor([0]),           # timestep\n",
    "#     torch.randn(1, 77, 1024)     # text embedding\n",
    "# )\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# # 5) Export  ➜  ExportedProgram  ➜  .ep file\n",
    "# # ------------------------------------------------------------------\n",
    "# ep = export(unet, example_inputs)\n",
    "# save(ep, \"unet_fp32.ep\")\n",
    "\n",
    "# print(\"✅  Exported UNet saved to: unet_fp32.ep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e1c0180",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.fx.passes.graph_draw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Find ops that lack a quantisation pattern\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_draw\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FxGraphDrawer\n\u001b[1;32m      4\u001b[0m unsupported \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m gm\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mnodes:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.fx.passes.graph_draw'"
     ]
    }
   ],
   "source": [
    "# Find ops that lack a quantisation pattern\n",
    "from torch.fx.passes.graph_draw import FxGraphDrawer\n",
    " \n",
    "unsupported = []\n",
    "for n in gm.graph.nodes:\n",
    "    if n.op == \"call_function\":\n",
    "        # Replace with your backend's supported op list\n",
    "        if n.target not in torch._inductor.lowering._registered_ops:\n",
    "            unsupported.append(n.target)\n",
    "\n",
    "print(\"Ops with no Inductor lowering:\", set(unsupported))\n",
    "# view the graph visually\n",
    "FxGraphDrawer(gm, \"unet\").run()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hwhe nweneed ot wuanitze\n",
    "from torch.ao.quantization.qconfig_mapping import get_default_qat_qconfig_mapping\n",
    "from torch.ao.quantization.quantizer import X86InductorQuantizer\n",
    "from torch.ao.quantization.quantize_pt2e import prepare_qat_pt2e, convert_pt2e\n",
    "\n",
    "qmap      = get_default_qat_qconfig_mapping(\"x86\")\n",
    "quantizer = X86InductorQuantizer().set_global(qmap)\n",
    "\n",
    "gm_qat = prepare_qat_pt2e(gm, quantizer)   # inserts fake-quant nodes\n",
    "# … fine-tune gm_qat for a few epochs …\n",
    "gm_int8 = convert_pt2e(gm_qat)\n",
    "gm_int8.save(\"unet_int8.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b9804",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012efa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                                                              target                                                            args                                                                                                                                                            kwargs\n",
      "-------------  ----------------------------------------------------------------  ----------------------------------------------------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------------------------------\n",
      "placeholder    p_encoder_conv_in_weight                                          p_encoder_conv_in_weight                                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_in_bias                                            p_encoder_conv_in_bias                                            ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm1_weight                    p_encoder_down_blocks_0_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm1_bias                      p_encoder_down_blocks_0_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv1_weight                    p_encoder_down_blocks_0_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv1_bias                      p_encoder_down_blocks_0_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm2_weight                    p_encoder_down_blocks_0_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm2_bias                      p_encoder_down_blocks_0_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv2_weight                    p_encoder_down_blocks_0_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv2_bias                      p_encoder_down_blocks_0_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm1_weight                    p_encoder_down_blocks_0_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm1_bias                      p_encoder_down_blocks_0_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv1_weight                    p_encoder_down_blocks_0_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv1_bias                      p_encoder_down_blocks_0_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm2_weight                    p_encoder_down_blocks_0_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm2_bias                      p_encoder_down_blocks_0_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv2_weight                    p_encoder_down_blocks_0_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv2_bias                      p_encoder_down_blocks_0_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_downsamplers_0_conv_weight                p_encoder_down_blocks_0_downsamplers_0_conv_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_downsamplers_0_conv_bias                  p_encoder_down_blocks_0_downsamplers_0_conv_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm1_weight                    p_encoder_down_blocks_1_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm1_bias                      p_encoder_down_blocks_1_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv1_weight                    p_encoder_down_blocks_1_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv1_bias                      p_encoder_down_blocks_1_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm2_weight                    p_encoder_down_blocks_1_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm2_bias                      p_encoder_down_blocks_1_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv2_weight                    p_encoder_down_blocks_1_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv2_bias                      p_encoder_down_blocks_1_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight            p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight            ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias              p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias              ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm1_weight                    p_encoder_down_blocks_1_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm1_bias                      p_encoder_down_blocks_1_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv1_weight                    p_encoder_down_blocks_1_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv1_bias                      p_encoder_down_blocks_1_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm2_weight                    p_encoder_down_blocks_1_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm2_bias                      p_encoder_down_blocks_1_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv2_weight                    p_encoder_down_blocks_1_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv2_bias                      p_encoder_down_blocks_1_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_downsamplers_0_conv_weight                p_encoder_down_blocks_1_downsamplers_0_conv_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_downsamplers_0_conv_bias                  p_encoder_down_blocks_1_downsamplers_0_conv_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm1_weight                    p_encoder_down_blocks_2_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm1_bias                      p_encoder_down_blocks_2_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv1_weight                    p_encoder_down_blocks_2_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv1_bias                      p_encoder_down_blocks_2_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm2_weight                    p_encoder_down_blocks_2_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm2_bias                      p_encoder_down_blocks_2_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv2_weight                    p_encoder_down_blocks_2_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv2_bias                      p_encoder_down_blocks_2_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight            p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight            ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias              p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias              ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm1_weight                    p_encoder_down_blocks_2_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm1_bias                      p_encoder_down_blocks_2_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv1_weight                    p_encoder_down_blocks_2_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv1_bias                      p_encoder_down_blocks_2_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm2_weight                    p_encoder_down_blocks_2_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm2_bias                      p_encoder_down_blocks_2_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv2_weight                    p_encoder_down_blocks_2_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv2_bias                      p_encoder_down_blocks_2_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_downsamplers_0_conv_weight                p_encoder_down_blocks_2_downsamplers_0_conv_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_downsamplers_0_conv_bias                  p_encoder_down_blocks_2_downsamplers_0_conv_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm1_weight                    p_encoder_down_blocks_3_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm1_bias                      p_encoder_down_blocks_3_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv1_weight                    p_encoder_down_blocks_3_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv1_bias                      p_encoder_down_blocks_3_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm2_weight                    p_encoder_down_blocks_3_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm2_bias                      p_encoder_down_blocks_3_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv2_weight                    p_encoder_down_blocks_3_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv2_bias                      p_encoder_down_blocks_3_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm1_weight                    p_encoder_down_blocks_3_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm1_bias                      p_encoder_down_blocks_3_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv1_weight                    p_encoder_down_blocks_3_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv1_bias                      p_encoder_down_blocks_3_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm2_weight                    p_encoder_down_blocks_3_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm2_bias                      p_encoder_down_blocks_3_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv2_weight                    p_encoder_down_blocks_3_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv2_bias                      p_encoder_down_blocks_3_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm1_weight                        p_encoder_mid_block_resnets_0_norm1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm1_bias                          p_encoder_mid_block_resnets_0_norm1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv1_weight                        p_encoder_mid_block_resnets_0_conv1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv1_bias                          p_encoder_mid_block_resnets_0_conv1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm2_weight                        p_encoder_mid_block_resnets_0_norm2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm2_bias                          p_encoder_mid_block_resnets_0_norm2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv2_weight                        p_encoder_mid_block_resnets_0_conv2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv2_bias                          p_encoder_mid_block_resnets_0_conv2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_group_norm_weight                p_encoder_mid_block_attentions_0_group_norm_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_group_norm_bias                  p_encoder_mid_block_attentions_0_group_norm_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_q_weight                      p_encoder_mid_block_attentions_0_to_q_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_q_bias                        p_encoder_mid_block_attentions_0_to_q_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_k_weight                      p_encoder_mid_block_attentions_0_to_k_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_k_bias                        p_encoder_mid_block_attentions_0_to_k_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_v_weight                      p_encoder_mid_block_attentions_0_to_v_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_v_bias                        p_encoder_mid_block_attentions_0_to_v_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_out_0_weight                  p_encoder_mid_block_attentions_0_to_out_0_weight                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_out_0_bias                    p_encoder_mid_block_attentions_0_to_out_0_bias                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_norm_out_weight                                    p_encoder_conv_norm_out_weight                                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_norm_out_bias                                      p_encoder_conv_norm_out_bias                                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_out_weight                                         p_encoder_conv_out_weight                                         ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_out_bias                                           p_encoder_conv_out_bias                                           ()                                                                                                                                                              {}\n",
      "placeholder    p_quant_conv_weight                                               p_quant_conv_weight                                               ()                                                                                                                                                              {}\n",
      "placeholder    p_quant_conv_bias                                                 p_quant_conv_bias                                                 ()                                                                                                                                                              {}\n",
      "placeholder    p_post_quant_conv_weight                                          p_post_quant_conv_weight                                          ()                                                                                                                                                              {}\n",
      "placeholder    p_post_quant_conv_bias                                            p_post_quant_conv_bias                                            ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_in_weight                                          p_decoder_conv_in_weight                                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_in_bias                                            p_decoder_conv_in_bias                                            ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm1_weight                        p_decoder_mid_block_resnets_0_norm1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm1_bias                          p_decoder_mid_block_resnets_0_norm1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv1_weight                        p_decoder_mid_block_resnets_0_conv1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv1_bias                          p_decoder_mid_block_resnets_0_conv1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm2_weight                        p_decoder_mid_block_resnets_0_norm2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm2_bias                          p_decoder_mid_block_resnets_0_norm2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv2_weight                        p_decoder_mid_block_resnets_0_conv2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv2_bias                          p_decoder_mid_block_resnets_0_conv2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_group_norm_weight                p_decoder_mid_block_attentions_0_group_norm_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_group_norm_bias                  p_decoder_mid_block_attentions_0_group_norm_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_q_weight                      p_decoder_mid_block_attentions_0_to_q_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_q_bias                        p_decoder_mid_block_attentions_0_to_q_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_k_weight                      p_decoder_mid_block_attentions_0_to_k_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_k_bias                        p_decoder_mid_block_attentions_0_to_k_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_v_weight                      p_decoder_mid_block_attentions_0_to_v_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_v_bias                        p_decoder_mid_block_attentions_0_to_v_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_out_0_weight                  p_decoder_mid_block_attentions_0_to_out_0_weight                  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_out_0_bias                    p_decoder_mid_block_attentions_0_to_out_0_bias                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm1_weight                      p_decoder_up_blocks_0_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm1_bias                        p_decoder_up_blocks_0_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv1_weight                      p_decoder_up_blocks_0_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv1_bias                        p_decoder_up_blocks_0_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm2_weight                      p_decoder_up_blocks_0_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm2_bias                        p_decoder_up_blocks_0_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv2_weight                      p_decoder_up_blocks_0_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv2_bias                        p_decoder_up_blocks_0_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm1_weight                      p_decoder_up_blocks_0_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm1_bias                        p_decoder_up_blocks_0_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv1_weight                      p_decoder_up_blocks_0_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv1_bias                        p_decoder_up_blocks_0_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm2_weight                      p_decoder_up_blocks_0_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm2_bias                        p_decoder_up_blocks_0_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv2_weight                      p_decoder_up_blocks_0_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv2_bias                        p_decoder_up_blocks_0_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm1_weight                      p_decoder_up_blocks_0_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm1_bias                        p_decoder_up_blocks_0_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv1_weight                      p_decoder_up_blocks_0_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv1_bias                        p_decoder_up_blocks_0_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm2_weight                      p_decoder_up_blocks_0_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm2_bias                        p_decoder_up_blocks_0_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv2_weight                      p_decoder_up_blocks_0_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv2_bias                        p_decoder_up_blocks_0_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_upsamplers_0_conv_weight                    p_decoder_up_blocks_0_upsamplers_0_conv_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_upsamplers_0_conv_bias                      p_decoder_up_blocks_0_upsamplers_0_conv_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm1_weight                      p_decoder_up_blocks_1_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm1_bias                        p_decoder_up_blocks_1_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv1_weight                      p_decoder_up_blocks_1_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv1_bias                        p_decoder_up_blocks_1_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm2_weight                      p_decoder_up_blocks_1_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm2_bias                        p_decoder_up_blocks_1_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv2_weight                      p_decoder_up_blocks_1_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv2_bias                        p_decoder_up_blocks_1_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm1_weight                      p_decoder_up_blocks_1_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm1_bias                        p_decoder_up_blocks_1_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv1_weight                      p_decoder_up_blocks_1_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv1_bias                        p_decoder_up_blocks_1_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm2_weight                      p_decoder_up_blocks_1_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm2_bias                        p_decoder_up_blocks_1_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv2_weight                      p_decoder_up_blocks_1_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv2_bias                        p_decoder_up_blocks_1_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm1_weight                      p_decoder_up_blocks_1_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm1_bias                        p_decoder_up_blocks_1_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv1_weight                      p_decoder_up_blocks_1_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv1_bias                        p_decoder_up_blocks_1_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm2_weight                      p_decoder_up_blocks_1_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm2_bias                        p_decoder_up_blocks_1_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv2_weight                      p_decoder_up_blocks_1_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv2_bias                        p_decoder_up_blocks_1_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_upsamplers_0_conv_weight                    p_decoder_up_blocks_1_upsamplers_0_conv_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_upsamplers_0_conv_bias                      p_decoder_up_blocks_1_upsamplers_0_conv_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm1_weight                      p_decoder_up_blocks_2_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm1_bias                        p_decoder_up_blocks_2_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv1_weight                      p_decoder_up_blocks_2_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv1_bias                        p_decoder_up_blocks_2_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm2_weight                      p_decoder_up_blocks_2_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm2_bias                        p_decoder_up_blocks_2_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv2_weight                      p_decoder_up_blocks_2_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv2_bias                        p_decoder_up_blocks_2_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight              p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight              ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias                p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias                ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm1_weight                      p_decoder_up_blocks_2_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm1_bias                        p_decoder_up_blocks_2_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv1_weight                      p_decoder_up_blocks_2_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv1_bias                        p_decoder_up_blocks_2_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm2_weight                      p_decoder_up_blocks_2_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm2_bias                        p_decoder_up_blocks_2_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv2_weight                      p_decoder_up_blocks_2_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv2_bias                        p_decoder_up_blocks_2_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm1_weight                      p_decoder_up_blocks_2_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm1_bias                        p_decoder_up_blocks_2_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv1_weight                      p_decoder_up_blocks_2_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv1_bias                        p_decoder_up_blocks_2_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm2_weight                      p_decoder_up_blocks_2_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm2_bias                        p_decoder_up_blocks_2_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv2_weight                      p_decoder_up_blocks_2_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv2_bias                        p_decoder_up_blocks_2_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_upsamplers_0_conv_weight                    p_decoder_up_blocks_2_upsamplers_0_conv_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_upsamplers_0_conv_bias                      p_decoder_up_blocks_2_upsamplers_0_conv_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm1_weight                      p_decoder_up_blocks_3_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm1_bias                        p_decoder_up_blocks_3_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv1_weight                      p_decoder_up_blocks_3_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv1_bias                        p_decoder_up_blocks_3_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm2_weight                      p_decoder_up_blocks_3_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm2_bias                        p_decoder_up_blocks_3_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv2_weight                      p_decoder_up_blocks_3_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv2_bias                        p_decoder_up_blocks_3_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight              p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight              ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias                p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias                ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm1_weight                      p_decoder_up_blocks_3_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm1_bias                        p_decoder_up_blocks_3_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv1_weight                      p_decoder_up_blocks_3_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv1_bias                        p_decoder_up_blocks_3_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm2_weight                      p_decoder_up_blocks_3_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm2_bias                        p_decoder_up_blocks_3_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv2_weight                      p_decoder_up_blocks_3_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv2_bias                        p_decoder_up_blocks_3_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm1_weight                      p_decoder_up_blocks_3_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm1_bias                        p_decoder_up_blocks_3_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv1_weight                      p_decoder_up_blocks_3_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv1_bias                        p_decoder_up_blocks_3_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm2_weight                      p_decoder_up_blocks_3_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm2_bias                        p_decoder_up_blocks_3_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv2_weight                      p_decoder_up_blocks_3_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv2_bias                        p_decoder_up_blocks_3_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_norm_out_weight                                    p_decoder_conv_norm_out_weight                                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_norm_out_bias                                      p_decoder_conv_norm_out_bias                                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_out_weight                                         p_decoder_conv_out_weight                                         ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_out_bias                                           p_decoder_conv_out_bias                                           ()                                                                                                                                                              {}\n",
      "placeholder    sample                                                            sample                                                            ()                                                                                                                                                              {}\n",
      "call_function  conv2d                                                            aten.conv2d.default                                               (sample, p_encoder_conv_in_weight, p_encoder_conv_in_bias, [1, 1], [1, 1])                                                                                      {}\n",
      "call_function  group_norm                                                        aten.group_norm.default                                           (conv2d, 32, p_encoder_down_blocks_0_resnets_0_norm1_weight, p_encoder_down_blocks_0_resnets_0_norm1_bias, 1e-06)                                               {}\n",
      "call_function  silu                                                              aten.silu.default                                                 (group_norm,)                                                                                                                                                   {}\n",
      "call_function  conv2d_1                                                          aten.conv2d.default                                               (silu, p_encoder_down_blocks_0_resnets_0_conv1_weight, p_encoder_down_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1])                                            {}\n",
      "call_function  group_norm_1                                                      aten.group_norm.default                                           (conv2d_1, 32, p_encoder_down_blocks_0_resnets_0_norm2_weight, p_encoder_down_blocks_0_resnets_0_norm2_bias, 1e-06)                                             {}\n",
      "call_function  silu_1                                                            aten.silu.default                                                 (group_norm_1,)                                                                                                                                                 {}\n",
      "call_function  dropout                                                           aten.dropout.default                                              (silu_1, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_2                                                          aten.conv2d.default                                               (dropout, p_encoder_down_blocks_0_resnets_0_conv2_weight, p_encoder_down_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  add                                                               aten.add.Tensor                                                   (conv2d, conv2d_2)                                                                                                                                              {}\n",
      "call_function  div                                                               aten.div.Tensor                                                   (add, 1.0)                                                                                                                                                      {}\n",
      "call_function  group_norm_2                                                      aten.group_norm.default                                           (div, 32, p_encoder_down_blocks_0_resnets_1_norm1_weight, p_encoder_down_blocks_0_resnets_1_norm1_bias, 1e-06)                                                  {}\n",
      "call_function  silu_2                                                            aten.silu.default                                                 (group_norm_2,)                                                                                                                                                 {}\n",
      "call_function  conv2d_3                                                          aten.conv2d.default                                               (silu_2, p_encoder_down_blocks_0_resnets_1_conv1_weight, p_encoder_down_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_3                                                      aten.group_norm.default                                           (conv2d_3, 32, p_encoder_down_blocks_0_resnets_1_norm2_weight, p_encoder_down_blocks_0_resnets_1_norm2_bias, 1e-06)                                             {}\n",
      "call_function  silu_3                                                            aten.silu.default                                                 (group_norm_3,)                                                                                                                                                 {}\n",
      "call_function  dropout_1                                                         aten.dropout.default                                              (silu_3, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_4                                                          aten.conv2d.default                                               (dropout_1, p_encoder_down_blocks_0_resnets_1_conv2_weight, p_encoder_down_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_1                                                             aten.add.Tensor                                                   (div, conv2d_4)                                                                                                                                                 {}\n",
      "call_function  div_1                                                             aten.div.Tensor                                                   (add_1, 1.0)                                                                                                                                                    {}\n",
      "call_function  pad                                                               aten.pad.default                                                  (div_1, [0, 1, 0, 1], 'constant', 0.0)                                                                                                                          {}\n",
      "call_function  conv2d_5                                                          aten.conv2d.default                                               (pad, p_encoder_down_blocks_0_downsamplers_0_conv_weight, p_encoder_down_blocks_0_downsamplers_0_conv_bias, [2, 2])                                             {}\n",
      "call_function  group_norm_4                                                      aten.group_norm.default                                           (conv2d_5, 32, p_encoder_down_blocks_1_resnets_0_norm1_weight, p_encoder_down_blocks_1_resnets_0_norm1_bias, 1e-06)                                             {}\n",
      "call_function  silu_4                                                            aten.silu.default                                                 (group_norm_4,)                                                                                                                                                 {}\n",
      "call_function  conv2d_6                                                          aten.conv2d.default                                               (silu_4, p_encoder_down_blocks_1_resnets_0_conv1_weight, p_encoder_down_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_5                                                      aten.group_norm.default                                           (conv2d_6, 32, p_encoder_down_blocks_1_resnets_0_norm2_weight, p_encoder_down_blocks_1_resnets_0_norm2_bias, 1e-06)                                             {}\n",
      "call_function  silu_5                                                            aten.silu.default                                                 (group_norm_5,)                                                                                                                                                 {}\n",
      "call_function  dropout_2                                                         aten.dropout.default                                              (silu_5, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_7                                                          aten.conv2d.default                                               (dropout_2, p_encoder_down_blocks_1_resnets_0_conv2_weight, p_encoder_down_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  conv2d_8                                                          aten.conv2d.default                                               (conv2d_5, p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight, p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias)                                        {}\n",
      "call_function  add_2                                                             aten.add.Tensor                                                   (conv2d_8, conv2d_7)                                                                                                                                            {}\n",
      "call_function  div_2                                                             aten.div.Tensor                                                   (add_2, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_6                                                      aten.group_norm.default                                           (div_2, 32, p_encoder_down_blocks_1_resnets_1_norm1_weight, p_encoder_down_blocks_1_resnets_1_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_6                                                            aten.silu.default                                                 (group_norm_6,)                                                                                                                                                 {}\n",
      "call_function  conv2d_9                                                          aten.conv2d.default                                               (silu_6, p_encoder_down_blocks_1_resnets_1_conv1_weight, p_encoder_down_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_7                                                      aten.group_norm.default                                           (conv2d_9, 32, p_encoder_down_blocks_1_resnets_1_norm2_weight, p_encoder_down_blocks_1_resnets_1_norm2_bias, 1e-06)                                             {}\n",
      "call_function  silu_7                                                            aten.silu.default                                                 (group_norm_7,)                                                                                                                                                 {}\n",
      "call_function  dropout_3                                                         aten.dropout.default                                              (silu_7, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_10                                                         aten.conv2d.default                                               (dropout_3, p_encoder_down_blocks_1_resnets_1_conv2_weight, p_encoder_down_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_3                                                             aten.add.Tensor                                                   (div_2, conv2d_10)                                                                                                                                              {}\n",
      "call_function  div_3                                                             aten.div.Tensor                                                   (add_3, 1.0)                                                                                                                                                    {}\n",
      "call_function  pad_1                                                             aten.pad.default                                                  (div_3, [0, 1, 0, 1], 'constant', 0.0)                                                                                                                          {}\n",
      "call_function  conv2d_11                                                         aten.conv2d.default                                               (pad_1, p_encoder_down_blocks_1_downsamplers_0_conv_weight, p_encoder_down_blocks_1_downsamplers_0_conv_bias, [2, 2])                                           {}\n",
      "call_function  group_norm_8                                                      aten.group_norm.default                                           (conv2d_11, 32, p_encoder_down_blocks_2_resnets_0_norm1_weight, p_encoder_down_blocks_2_resnets_0_norm1_bias, 1e-06)                                            {}\n",
      "call_function  silu_8                                                            aten.silu.default                                                 (group_norm_8,)                                                                                                                                                 {}\n",
      "call_function  conv2d_12                                                         aten.conv2d.default                                               (silu_8, p_encoder_down_blocks_2_resnets_0_conv1_weight, p_encoder_down_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_9                                                      aten.group_norm.default                                           (conv2d_12, 32, p_encoder_down_blocks_2_resnets_0_norm2_weight, p_encoder_down_blocks_2_resnets_0_norm2_bias, 1e-06)                                            {}\n",
      "call_function  silu_9                                                            aten.silu.default                                                 (group_norm_9,)                                                                                                                                                 {}\n",
      "call_function  dropout_4                                                         aten.dropout.default                                              (silu_9, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_13                                                         aten.conv2d.default                                               (dropout_4, p_encoder_down_blocks_2_resnets_0_conv2_weight, p_encoder_down_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  conv2d_14                                                         aten.conv2d.default                                               (conv2d_11, p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight, p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias)                                       {}\n",
      "call_function  add_4                                                             aten.add.Tensor                                                   (conv2d_14, conv2d_13)                                                                                                                                          {}\n",
      "call_function  div_4                                                             aten.div.Tensor                                                   (add_4, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_10                                                     aten.group_norm.default                                           (div_4, 32, p_encoder_down_blocks_2_resnets_1_norm1_weight, p_encoder_down_blocks_2_resnets_1_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_10                                                           aten.silu.default                                                 (group_norm_10,)                                                                                                                                                {}\n",
      "call_function  conv2d_15                                                         aten.conv2d.default                                               (silu_10, p_encoder_down_blocks_2_resnets_1_conv1_weight, p_encoder_down_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  group_norm_11                                                     aten.group_norm.default                                           (conv2d_15, 32, p_encoder_down_blocks_2_resnets_1_norm2_weight, p_encoder_down_blocks_2_resnets_1_norm2_bias, 1e-06)                                            {}\n",
      "call_function  silu_11                                                           aten.silu.default                                                 (group_norm_11,)                                                                                                                                                {}\n",
      "call_function  dropout_5                                                         aten.dropout.default                                              (silu_11, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_16                                                         aten.conv2d.default                                               (dropout_5, p_encoder_down_blocks_2_resnets_1_conv2_weight, p_encoder_down_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_5                                                             aten.add.Tensor                                                   (div_4, conv2d_16)                                                                                                                                              {}\n",
      "call_function  div_5                                                             aten.div.Tensor                                                   (add_5, 1.0)                                                                                                                                                    {}\n",
      "call_function  pad_2                                                             aten.pad.default                                                  (div_5, [0, 1, 0, 1], 'constant', 0.0)                                                                                                                          {}\n",
      "call_function  conv2d_17                                                         aten.conv2d.default                                               (pad_2, p_encoder_down_blocks_2_downsamplers_0_conv_weight, p_encoder_down_blocks_2_downsamplers_0_conv_bias, [2, 2])                                           {}\n",
      "call_function  group_norm_12                                                     aten.group_norm.default                                           (conv2d_17, 32, p_encoder_down_blocks_3_resnets_0_norm1_weight, p_encoder_down_blocks_3_resnets_0_norm1_bias, 1e-06)                                            {}\n",
      "call_function  silu_12                                                           aten.silu.default                                                 (group_norm_12,)                                                                                                                                                {}\n",
      "call_function  conv2d_18                                                         aten.conv2d.default                                               (silu_12, p_encoder_down_blocks_3_resnets_0_conv1_weight, p_encoder_down_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  group_norm_13                                                     aten.group_norm.default                                           (conv2d_18, 32, p_encoder_down_blocks_3_resnets_0_norm2_weight, p_encoder_down_blocks_3_resnets_0_norm2_bias, 1e-06)                                            {}\n",
      "call_function  silu_13                                                           aten.silu.default                                                 (group_norm_13,)                                                                                                                                                {}\n",
      "call_function  dropout_6                                                         aten.dropout.default                                              (silu_13, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_19                                                         aten.conv2d.default                                               (dropout_6, p_encoder_down_blocks_3_resnets_0_conv2_weight, p_encoder_down_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_6                                                             aten.add.Tensor                                                   (conv2d_17, conv2d_19)                                                                                                                                          {}\n",
      "call_function  div_6                                                             aten.div.Tensor                                                   (add_6, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_14                                                     aten.group_norm.default                                           (div_6, 32, p_encoder_down_blocks_3_resnets_1_norm1_weight, p_encoder_down_blocks_3_resnets_1_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_14                                                           aten.silu.default                                                 (group_norm_14,)                                                                                                                                                {}\n",
      "call_function  conv2d_20                                                         aten.conv2d.default                                               (silu_14, p_encoder_down_blocks_3_resnets_1_conv1_weight, p_encoder_down_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  group_norm_15                                                     aten.group_norm.default                                           (conv2d_20, 32, p_encoder_down_blocks_3_resnets_1_norm2_weight, p_encoder_down_blocks_3_resnets_1_norm2_bias, 1e-06)                                            {}\n",
      "call_function  silu_15                                                           aten.silu.default                                                 (group_norm_15,)                                                                                                                                                {}\n",
      "call_function  dropout_7                                                         aten.dropout.default                                              (silu_15, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_21                                                         aten.conv2d.default                                               (dropout_7, p_encoder_down_blocks_3_resnets_1_conv2_weight, p_encoder_down_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_7                                                             aten.add.Tensor                                                   (div_6, conv2d_21)                                                                                                                                              {}\n",
      "call_function  div_7                                                             aten.div.Tensor                                                   (add_7, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_16                                                     aten.group_norm.default                                           (div_7, 32, p_encoder_mid_block_resnets_0_norm1_weight, p_encoder_mid_block_resnets_0_norm1_bias, 1e-06)                                                        {}\n",
      "call_function  silu_16                                                           aten.silu.default                                                 (group_norm_16,)                                                                                                                                                {}\n",
      "call_function  conv2d_22                                                         aten.conv2d.default                                               (silu_16, p_encoder_mid_block_resnets_0_conv1_weight, p_encoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1])                                                 {}\n",
      "call_function  group_norm_17                                                     aten.group_norm.default                                           (conv2d_22, 32, p_encoder_mid_block_resnets_0_norm2_weight, p_encoder_mid_block_resnets_0_norm2_bias, 1e-06)                                                    {}\n",
      "call_function  silu_17                                                           aten.silu.default                                                 (group_norm_17,)                                                                                                                                                {}\n",
      "call_function  dropout_8                                                         aten.dropout.default                                              (silu_17, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_23                                                         aten.conv2d.default                                               (dropout_8, p_encoder_mid_block_resnets_0_conv2_weight, p_encoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1])                                               {}\n",
      "call_function  add_8                                                             aten.add.Tensor                                                   (div_7, conv2d_23)                                                                                                                                              {}\n",
      "call_function  div_8                                                             aten.div.Tensor                                                   (add_8, 1)                                                                                                                                                      {}\n",
      "call_function  view                                                              aten.view.default                                                 (div_8, [1, 512, 4096])                                                                                                                                         {}\n",
      "call_function  transpose                                                         aten.transpose.int                                                (view, 1, 2)                                                                                                                                                    {}\n",
      "call_function  transpose_1                                                       aten.transpose.int                                                (transpose, 1, 2)                                                                                                                                               {}\n",
      "call_function  group_norm_18                                                     aten.group_norm.default                                           (transpose_1, 32, p_encoder_mid_block_attentions_0_group_norm_weight, p_encoder_mid_block_attentions_0_group_norm_bias, 1e-06)                                  {}\n",
      "call_function  transpose_2                                                       aten.transpose.int                                                (group_norm_18, 1, 2)                                                                                                                                           {}\n",
      "call_function  linear                                                            aten.linear.default                                               (transpose_2, p_encoder_mid_block_attentions_0_to_q_weight, p_encoder_mid_block_attentions_0_to_q_bias)                                                         {}\n",
      "call_function  linear_1                                                          aten.linear.default                                               (transpose_2, p_encoder_mid_block_attentions_0_to_k_weight, p_encoder_mid_block_attentions_0_to_k_bias)                                                         {}\n",
      "call_function  linear_2                                                          aten.linear.default                                               (transpose_2, p_encoder_mid_block_attentions_0_to_v_weight, p_encoder_mid_block_attentions_0_to_v_bias)                                                         {}\n",
      "call_function  view_1                                                            aten.view.default                                                 (linear, [1, -1, 1, 512])                                                                                                                                       {}\n",
      "call_function  transpose_3                                                       aten.transpose.int                                                (view_1, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_2                                                            aten.view.default                                                 (linear_1, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_4                                                       aten.transpose.int                                                (view_2, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_3                                                            aten.view.default                                                 (linear_2, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_5                                                       aten.transpose.int                                                (view_3, 1, 2)                                                                                                                                                  {}\n",
      "call_function  scaled_dot_product_attention                                      aten.scaled_dot_product_attention.default                         (transpose_3, transpose_4, transpose_5)                                                                                                                         {}\n",
      "call_function  transpose_6                                                       aten.transpose.int                                                (scaled_dot_product_attention, 1, 2)                                                                                                                            {}\n",
      "call_function  view_4                                                            aten.view.default                                                 (transpose_6, [1, -1, 512])                                                                                                                                     {}\n",
      "call_function  _to_copy                                                          aten._to_copy.default                                             (view_4,)                                                                                                                                                       {'dtype': torch.float32}\n",
      "call_function  linear_3                                                          aten.linear.default                                               (_to_copy, p_encoder_mid_block_attentions_0_to_out_0_weight, p_encoder_mid_block_attentions_0_to_out_0_bias)                                                    {}\n",
      "call_function  dropout_9                                                         aten.dropout.default                                              (linear_3, 0.0, False)                                                                                                                                          {}\n",
      "call_function  transpose_7                                                       aten.transpose.int                                                (dropout_9, -1, -2)                                                                                                                                             {}\n",
      "call_function  view_5                                                            aten.view.default                                                 (transpose_7, [1, 512, 64, 64])                                                                                                                                 {}\n",
      "call_function  add_9                                                             aten.add.Tensor                                                   (view_5, div_8)                                                                                                                                                 {}\n",
      "call_function  div_9                                                             aten.div.Tensor                                                   (add_9, 1)                                                                                                                                                      {}\n",
      "call_function  group_norm_19                                                     aten.group_norm.default                                           (div_9, 32, p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias, 1e-06)            {}\n",
      "call_function  silu_18                                                           aten.silu.default                                                 (group_norm_19,)                                                                                                                                                {}\n",
      "call_function  conv2d_24                                                         aten.conv2d.default                                               (silu_18, p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1])     {}\n",
      "call_function  group_norm_20                                                     aten.group_norm.default                                           (conv2d_24, 32, p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias, 1e-06)        {}\n",
      "call_function  silu_19                                                           aten.silu.default                                                 (group_norm_20,)                                                                                                                                                {}\n",
      "call_function  dropout_10                                                        aten.dropout.default                                              (silu_19, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_25                                                         aten.conv2d.default                                               (dropout_10, p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1])  {}\n",
      "call_function  add_10                                                            aten.add.Tensor                                                   (div_9, conv2d_25)                                                                                                                                              {}\n",
      "call_function  div_10                                                            aten.div.Tensor                                                   (add_10, 1)                                                                                                                                                     {}\n",
      "call_function  group_norm_21                                                     aten.group_norm.default                                           (div_10, 32, p_encoder_conv_norm_out_weight, p_encoder_conv_norm_out_bias, 1e-06)                                                                               {}\n",
      "call_function  silu_20                                                           aten.silu.default                                                 (group_norm_21,)                                                                                                                                                {}\n",
      "call_function  conv2d_26                                                         aten.conv2d.default                                               (silu_20, p_encoder_conv_out_weight, p_encoder_conv_out_bias, [1, 1], [1, 1])                                                                                   {}\n",
      "call_function  conv2d_27                                                         aten.conv2d.default                                               (conv2d_26, p_quant_conv_weight, p_quant_conv_bias)                                                                                                             {}\n",
      "call_function  split                                                             aten.split.Tensor                                                 (conv2d_27, 4, 1)                                                                                                                                               {}\n",
      "call_function  getitem                                                           <built-in function getitem>                                       (split, 0)                                                                                                                                                      {}\n",
      "call_function  conv2d_28                                                         aten.conv2d.default                                               (getitem, p_post_quant_conv_weight, p_post_quant_conv_bias)                                                                                                     {}\n",
      "call_function  conv2d_29                                                         aten.conv2d.default                                               (conv2d_28, p_decoder_conv_in_weight, p_decoder_conv_in_bias, [1, 1], [1, 1])                                                                                   {}\n",
      "call_function  group_norm_22                                                     aten.group_norm.default                                           (conv2d_29, 32, p_decoder_mid_block_resnets_0_norm1_weight, p_decoder_mid_block_resnets_0_norm1_bias, 1e-06)                                                    {}\n",
      "call_function  silu_21                                                           aten.silu.default                                                 (group_norm_22,)                                                                                                                                                {}\n",
      "call_function  conv2d_30                                                         aten.conv2d.default                                               (silu_21, p_decoder_mid_block_resnets_0_conv1_weight, p_decoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1])                                                 {}\n",
      "call_function  group_norm_23                                                     aten.group_norm.default                                           (conv2d_30, 32, p_decoder_mid_block_resnets_0_norm2_weight, p_decoder_mid_block_resnets_0_norm2_bias, 1e-06)                                                    {}\n",
      "call_function  silu_22                                                           aten.silu.default                                                 (group_norm_23,)                                                                                                                                                {}\n",
      "call_function  dropout_11                                                        aten.dropout.default                                              (silu_22, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_31                                                         aten.conv2d.default                                               (dropout_11, p_decoder_mid_block_resnets_0_conv2_weight, p_decoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1])                                              {}\n",
      "call_function  add_11                                                            aten.add.Tensor                                                   (conv2d_29, conv2d_31)                                                                                                                                          {}\n",
      "call_function  div_11                                                            aten.div.Tensor                                                   (add_11, 1)                                                                                                                                                     {}\n",
      "call_function  view_6                                                            aten.view.default                                                 (div_11, [1, 512, 4096])                                                                                                                                        {}\n",
      "call_function  transpose_8                                                       aten.transpose.int                                                (view_6, 1, 2)                                                                                                                                                  {}\n",
      "call_function  transpose_9                                                       aten.transpose.int                                                (transpose_8, 1, 2)                                                                                                                                             {}\n",
      "call_function  group_norm_24                                                     aten.group_norm.default                                           (transpose_9, 32, p_decoder_mid_block_attentions_0_group_norm_weight, p_decoder_mid_block_attentions_0_group_norm_bias, 1e-06)                                  {}\n",
      "call_function  transpose_10                                                      aten.transpose.int                                                (group_norm_24, 1, 2)                                                                                                                                           {}\n",
      "call_function  linear_4                                                          aten.linear.default                                               (transpose_10, p_decoder_mid_block_attentions_0_to_q_weight, p_decoder_mid_block_attentions_0_to_q_bias)                                                        {}\n",
      "call_function  linear_5                                                          aten.linear.default                                               (transpose_10, p_decoder_mid_block_attentions_0_to_k_weight, p_decoder_mid_block_attentions_0_to_k_bias)                                                        {}\n",
      "call_function  linear_6                                                          aten.linear.default                                               (transpose_10, p_decoder_mid_block_attentions_0_to_v_weight, p_decoder_mid_block_attentions_0_to_v_bias)                                                        {}\n",
      "call_function  view_7                                                            aten.view.default                                                 (linear_4, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_11                                                      aten.transpose.int                                                (view_7, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_8                                                            aten.view.default                                                 (linear_5, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_12                                                      aten.transpose.int                                                (view_8, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_9                                                            aten.view.default                                                 (linear_6, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_13                                                      aten.transpose.int                                                (view_9, 1, 2)                                                                                                                                                  {}\n",
      "call_function  scaled_dot_product_attention_1                                    aten.scaled_dot_product_attention.default                         (transpose_11, transpose_12, transpose_13)                                                                                                                      {}\n",
      "call_function  transpose_14                                                      aten.transpose.int                                                (scaled_dot_product_attention_1, 1, 2)                                                                                                                          {}\n",
      "call_function  view_10                                                           aten.view.default                                                 (transpose_14, [1, -1, 512])                                                                                                                                    {}\n",
      "call_function  _to_copy_1                                                        aten._to_copy.default                                             (view_10,)                                                                                                                                                      {'dtype': torch.float32}\n",
      "call_function  linear_7                                                          aten.linear.default                                               (_to_copy_1, p_decoder_mid_block_attentions_0_to_out_0_weight, p_decoder_mid_block_attentions_0_to_out_0_bias)                                                  {}\n",
      "call_function  dropout_12                                                        aten.dropout.default                                              (linear_7, 0.0, False)                                                                                                                                          {}\n",
      "call_function  transpose_15                                                      aten.transpose.int                                                (dropout_12, -1, -2)                                                                                                                                            {}\n",
      "call_function  view_11                                                           aten.view.default                                                 (transpose_15, [1, 512, 64, 64])                                                                                                                                {}\n",
      "call_function  add_12                                                            aten.add.Tensor                                                   (view_11, div_11)                                                                                                                                               {}\n",
      "call_function  div_12                                                            aten.div.Tensor                                                   (add_12, 1)                                                                                                                                                     {}\n",
      "call_function  group_norm_25                                                     aten.group_norm.default                                           (div_12, 32, p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias, 1e-06)           {}\n",
      "call_function  silu_23                                                           aten.silu.default                                                 (group_norm_25,)                                                                                                                                                {}\n",
      "call_function  conv2d_32                                                         aten.conv2d.default                                               (silu_23, p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1])     {}\n",
      "call_function  group_norm_26                                                     aten.group_norm.default                                           (conv2d_32, 32, p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias, 1e-06)        {}\n",
      "call_function  silu_24                                                           aten.silu.default                                                 (group_norm_26,)                                                                                                                                                {}\n",
      "call_function  dropout_13                                                        aten.dropout.default                                              (silu_24, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_33                                                         aten.conv2d.default                                               (dropout_13, p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1])  {}\n",
      "call_function  add_13                                                            aten.add.Tensor                                                   (div_12, conv2d_33)                                                                                                                                             {}\n",
      "call_function  div_13                                                            aten.div.Tensor                                                   (add_13, 1)                                                                                                                                                     {}\n",
      "call_function  _to_copy_2                                                        aten._to_copy.default                                             (div_13,)                                                                                                                                                       {'dtype': torch.float32}\n",
      "call_function  group_norm_27                                                     aten.group_norm.default                                           (_to_copy_2, 32, p_decoder_up_blocks_0_resnets_0_norm1_weight, p_decoder_up_blocks_0_resnets_0_norm1_bias, 1e-06)                                               {}\n",
      "call_function  silu_25                                                           aten.silu.default                                                 (group_norm_27,)                                                                                                                                                {}\n",
      "call_function  conv2d_34                                                         aten.conv2d.default                                               (silu_25, p_decoder_up_blocks_0_resnets_0_conv1_weight, p_decoder_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_28                                                     aten.group_norm.default                                           (conv2d_34, 32, p_decoder_up_blocks_0_resnets_0_norm2_weight, p_decoder_up_blocks_0_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_26                                                           aten.silu.default                                                 (group_norm_28,)                                                                                                                                                {}\n",
      "call_function  dropout_14                                                        aten.dropout.default                                              (silu_26, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_35                                                         aten.conv2d.default                                               (dropout_14, p_decoder_up_blocks_0_resnets_0_conv2_weight, p_decoder_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_14                                                            aten.add.Tensor                                                   (_to_copy_2, conv2d_35)                                                                                                                                         {}\n",
      "call_function  div_14                                                            aten.div.Tensor                                                   (add_14, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_29                                                     aten.group_norm.default                                           (div_14, 32, p_decoder_up_blocks_0_resnets_1_norm1_weight, p_decoder_up_blocks_0_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_27                                                           aten.silu.default                                                 (group_norm_29,)                                                                                                                                                {}\n",
      "call_function  conv2d_36                                                         aten.conv2d.default                                               (silu_27, p_decoder_up_blocks_0_resnets_1_conv1_weight, p_decoder_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_30                                                     aten.group_norm.default                                           (conv2d_36, 32, p_decoder_up_blocks_0_resnets_1_norm2_weight, p_decoder_up_blocks_0_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_28                                                           aten.silu.default                                                 (group_norm_30,)                                                                                                                                                {}\n",
      "call_function  dropout_15                                                        aten.dropout.default                                              (silu_28, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_37                                                         aten.conv2d.default                                               (dropout_15, p_decoder_up_blocks_0_resnets_1_conv2_weight, p_decoder_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_15                                                            aten.add.Tensor                                                   (div_14, conv2d_37)                                                                                                                                             {}\n",
      "call_function  div_15                                                            aten.div.Tensor                                                   (add_15, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_31                                                     aten.group_norm.default                                           (div_15, 32, p_decoder_up_blocks_0_resnets_2_norm1_weight, p_decoder_up_blocks_0_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_29                                                           aten.silu.default                                                 (group_norm_31,)                                                                                                                                                {}\n",
      "call_function  conv2d_38                                                         aten.conv2d.default                                               (silu_29, p_decoder_up_blocks_0_resnets_2_conv1_weight, p_decoder_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_32                                                     aten.group_norm.default                                           (conv2d_38, 32, p_decoder_up_blocks_0_resnets_2_norm2_weight, p_decoder_up_blocks_0_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_30                                                           aten.silu.default                                                 (group_norm_32,)                                                                                                                                                {}\n",
      "call_function  dropout_16                                                        aten.dropout.default                                              (silu_30, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_39                                                         aten.conv2d.default                                               (dropout_16, p_decoder_up_blocks_0_resnets_2_conv2_weight, p_decoder_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_16                                                            aten.add.Tensor                                                   (div_15, conv2d_39)                                                                                                                                             {}\n",
      "call_function  div_16                                                            aten.div.Tensor                                                   (add_16, 1.0)                                                                                                                                                   {}\n",
      "call_function  upsample_nearest2d                                                aten.upsample_nearest2d.vec                                       (div_16, None, [2.0, 2.0])                                                                                                                                      {}\n",
      "call_function  conv2d_40                                                         aten.conv2d.default                                               (upsample_nearest2d, p_decoder_up_blocks_0_upsamplers_0_conv_weight, p_decoder_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1])                              {}\n",
      "call_function  group_norm_33                                                     aten.group_norm.default                                           (conv2d_40, 32, p_decoder_up_blocks_1_resnets_0_norm1_weight, p_decoder_up_blocks_1_resnets_0_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_31                                                           aten.silu.default                                                 (group_norm_33,)                                                                                                                                                {}\n",
      "call_function  conv2d_41                                                         aten.conv2d.default                                               (silu_31, p_decoder_up_blocks_1_resnets_0_conv1_weight, p_decoder_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_34                                                     aten.group_norm.default                                           (conv2d_41, 32, p_decoder_up_blocks_1_resnets_0_norm2_weight, p_decoder_up_blocks_1_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_32                                                           aten.silu.default                                                 (group_norm_34,)                                                                                                                                                {}\n",
      "call_function  dropout_17                                                        aten.dropout.default                                              (silu_32, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_42                                                         aten.conv2d.default                                               (dropout_17, p_decoder_up_blocks_1_resnets_0_conv2_weight, p_decoder_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_17                                                            aten.add.Tensor                                                   (conv2d_40, conv2d_42)                                                                                                                                          {}\n",
      "call_function  div_17                                                            aten.div.Tensor                                                   (add_17, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_35                                                     aten.group_norm.default                                           (div_17, 32, p_decoder_up_blocks_1_resnets_1_norm1_weight, p_decoder_up_blocks_1_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_33                                                           aten.silu.default                                                 (group_norm_35,)                                                                                                                                                {}\n",
      "call_function  conv2d_43                                                         aten.conv2d.default                                               (silu_33, p_decoder_up_blocks_1_resnets_1_conv1_weight, p_decoder_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_36                                                     aten.group_norm.default                                           (conv2d_43, 32, p_decoder_up_blocks_1_resnets_1_norm2_weight, p_decoder_up_blocks_1_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_34                                                           aten.silu.default                                                 (group_norm_36,)                                                                                                                                                {}\n",
      "call_function  dropout_18                                                        aten.dropout.default                                              (silu_34, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_44                                                         aten.conv2d.default                                               (dropout_18, p_decoder_up_blocks_1_resnets_1_conv2_weight, p_decoder_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_18                                                            aten.add.Tensor                                                   (div_17, conv2d_44)                                                                                                                                             {}\n",
      "call_function  div_18                                                            aten.div.Tensor                                                   (add_18, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_37                                                     aten.group_norm.default                                           (div_18, 32, p_decoder_up_blocks_1_resnets_2_norm1_weight, p_decoder_up_blocks_1_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_35                                                           aten.silu.default                                                 (group_norm_37,)                                                                                                                                                {}\n",
      "call_function  conv2d_45                                                         aten.conv2d.default                                               (silu_35, p_decoder_up_blocks_1_resnets_2_conv1_weight, p_decoder_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_38                                                     aten.group_norm.default                                           (conv2d_45, 32, p_decoder_up_blocks_1_resnets_2_norm2_weight, p_decoder_up_blocks_1_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_36                                                           aten.silu.default                                                 (group_norm_38,)                                                                                                                                                {}\n",
      "call_function  dropout_19                                                        aten.dropout.default                                              (silu_36, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_46                                                         aten.conv2d.default                                               (dropout_19, p_decoder_up_blocks_1_resnets_2_conv2_weight, p_decoder_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_19                                                            aten.add.Tensor                                                   (div_18, conv2d_46)                                                                                                                                             {}\n",
      "call_function  div_19                                                            aten.div.Tensor                                                   (add_19, 1.0)                                                                                                                                                   {}\n",
      "call_function  upsample_nearest2d_1                                              aten.upsample_nearest2d.vec                                       (div_19, None, [2.0, 2.0])                                                                                                                                      {}\n",
      "call_function  conv2d_47                                                         aten.conv2d.default                                               (upsample_nearest2d_1, p_decoder_up_blocks_1_upsamplers_0_conv_weight, p_decoder_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1])                            {}\n",
      "call_function  group_norm_39                                                     aten.group_norm.default                                           (conv2d_47, 32, p_decoder_up_blocks_2_resnets_0_norm1_weight, p_decoder_up_blocks_2_resnets_0_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_37                                                           aten.silu.default                                                 (group_norm_39,)                                                                                                                                                {}\n",
      "call_function  conv2d_48                                                         aten.conv2d.default                                               (silu_37, p_decoder_up_blocks_2_resnets_0_conv1_weight, p_decoder_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_40                                                     aten.group_norm.default                                           (conv2d_48, 32, p_decoder_up_blocks_2_resnets_0_norm2_weight, p_decoder_up_blocks_2_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_38                                                           aten.silu.default                                                 (group_norm_40,)                                                                                                                                                {}\n",
      "call_function  dropout_20                                                        aten.dropout.default                                              (silu_38, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_49                                                         aten.conv2d.default                                               (dropout_20, p_decoder_up_blocks_2_resnets_0_conv2_weight, p_decoder_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  clone                                                             aten.clone.default                                                (conv2d_47,)                                                                                                                                                    {'memory_format': torch.contiguous_format}\n",
      "call_function  conv2d_50                                                         aten.conv2d.default                                               (clone, p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight, p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias)                                               {}\n",
      "call_function  add_20                                                            aten.add.Tensor                                                   (conv2d_50, conv2d_49)                                                                                                                                          {}\n",
      "call_function  div_20                                                            aten.div.Tensor                                                   (add_20, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_41                                                     aten.group_norm.default                                           (div_20, 32, p_decoder_up_blocks_2_resnets_1_norm1_weight, p_decoder_up_blocks_2_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_39                                                           aten.silu.default                                                 (group_norm_41,)                                                                                                                                                {}\n",
      "call_function  conv2d_51                                                         aten.conv2d.default                                               (silu_39, p_decoder_up_blocks_2_resnets_1_conv1_weight, p_decoder_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_42                                                     aten.group_norm.default                                           (conv2d_51, 32, p_decoder_up_blocks_2_resnets_1_norm2_weight, p_decoder_up_blocks_2_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_40                                                           aten.silu.default                                                 (group_norm_42,)                                                                                                                                                {}\n",
      "call_function  dropout_21                                                        aten.dropout.default                                              (silu_40, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_52                                                         aten.conv2d.default                                               (dropout_21, p_decoder_up_blocks_2_resnets_1_conv2_weight, p_decoder_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_21                                                            aten.add.Tensor                                                   (div_20, conv2d_52)                                                                                                                                             {}\n",
      "call_function  div_21                                                            aten.div.Tensor                                                   (add_21, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_43                                                     aten.group_norm.default                                           (div_21, 32, p_decoder_up_blocks_2_resnets_2_norm1_weight, p_decoder_up_blocks_2_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_41                                                           aten.silu.default                                                 (group_norm_43,)                                                                                                                                                {}\n",
      "call_function  conv2d_53                                                         aten.conv2d.default                                               (silu_41, p_decoder_up_blocks_2_resnets_2_conv1_weight, p_decoder_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_44                                                     aten.group_norm.default                                           (conv2d_53, 32, p_decoder_up_blocks_2_resnets_2_norm2_weight, p_decoder_up_blocks_2_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_42                                                           aten.silu.default                                                 (group_norm_44,)                                                                                                                                                {}\n",
      "call_function  dropout_22                                                        aten.dropout.default                                              (silu_42, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_54                                                         aten.conv2d.default                                               (dropout_22, p_decoder_up_blocks_2_resnets_2_conv2_weight, p_decoder_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_22                                                            aten.add.Tensor                                                   (div_21, conv2d_54)                                                                                                                                             {}\n",
      "call_function  div_22                                                            aten.div.Tensor                                                   (add_22, 1.0)                                                                                                                                                   {}\n",
      "call_function  upsample_nearest2d_2                                              aten.upsample_nearest2d.vec                                       (div_22, None, [2.0, 2.0])                                                                                                                                      {}\n",
      "call_function  conv2d_55                                                         aten.conv2d.default                                               (upsample_nearest2d_2, p_decoder_up_blocks_2_upsamplers_0_conv_weight, p_decoder_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1])                            {}\n",
      "call_function  group_norm_45                                                     aten.group_norm.default                                           (conv2d_55, 32, p_decoder_up_blocks_3_resnets_0_norm1_weight, p_decoder_up_blocks_3_resnets_0_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_43                                                           aten.silu.default                                                 (group_norm_45,)                                                                                                                                                {}\n",
      "call_function  conv2d_56                                                         aten.conv2d.default                                               (silu_43, p_decoder_up_blocks_3_resnets_0_conv1_weight, p_decoder_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_46                                                     aten.group_norm.default                                           (conv2d_56, 32, p_decoder_up_blocks_3_resnets_0_norm2_weight, p_decoder_up_blocks_3_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_44                                                           aten.silu.default                                                 (group_norm_46,)                                                                                                                                                {}\n",
      "call_function  dropout_23                                                        aten.dropout.default                                              (silu_44, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_57                                                         aten.conv2d.default                                               (dropout_23, p_decoder_up_blocks_3_resnets_0_conv2_weight, p_decoder_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  conv2d_58                                                         aten.conv2d.default                                               (conv2d_55, p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight, p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias)                                           {}\n",
      "call_function  add_23                                                            aten.add.Tensor                                                   (conv2d_58, conv2d_57)                                                                                                                                          {}\n",
      "call_function  div_23                                                            aten.div.Tensor                                                   (add_23, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_47                                                     aten.group_norm.default                                           (div_23, 32, p_decoder_up_blocks_3_resnets_1_norm1_weight, p_decoder_up_blocks_3_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_45                                                           aten.silu.default                                                 (group_norm_47,)                                                                                                                                                {}\n",
      "call_function  conv2d_59                                                         aten.conv2d.default                                               (silu_45, p_decoder_up_blocks_3_resnets_1_conv1_weight, p_decoder_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_48                                                     aten.group_norm.default                                           (conv2d_59, 32, p_decoder_up_blocks_3_resnets_1_norm2_weight, p_decoder_up_blocks_3_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_46                                                           aten.silu.default                                                 (group_norm_48,)                                                                                                                                                {}\n",
      "call_function  dropout_24                                                        aten.dropout.default                                              (silu_46, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_60                                                         aten.conv2d.default                                               (dropout_24, p_decoder_up_blocks_3_resnets_1_conv2_weight, p_decoder_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_24                                                            aten.add.Tensor                                                   (div_23, conv2d_60)                                                                                                                                             {}\n",
      "call_function  div_24                                                            aten.div.Tensor                                                   (add_24, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_49                                                     aten.group_norm.default                                           (div_24, 32, p_decoder_up_blocks_3_resnets_2_norm1_weight, p_decoder_up_blocks_3_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_47                                                           aten.silu.default                                                 (group_norm_49,)                                                                                                                                                {}\n",
      "call_function  conv2d_61                                                         aten.conv2d.default                                               (silu_47, p_decoder_up_blocks_3_resnets_2_conv1_weight, p_decoder_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_50                                                     aten.group_norm.default                                           (conv2d_61, 32, p_decoder_up_blocks_3_resnets_2_norm2_weight, p_decoder_up_blocks_3_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_48                                                           aten.silu.default                                                 (group_norm_50,)                                                                                                                                                {}\n",
      "call_function  dropout_25                                                        aten.dropout.default                                              (silu_48, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_62                                                         aten.conv2d.default                                               (dropout_25, p_decoder_up_blocks_3_resnets_2_conv2_weight, p_decoder_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_25                                                            aten.add.Tensor                                                   (div_24, conv2d_62)                                                                                                                                             {}\n",
      "call_function  div_25                                                            aten.div.Tensor                                                   (add_25, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_51                                                     aten.group_norm.default                                           (div_25, 32, p_decoder_conv_norm_out_weight, p_decoder_conv_norm_out_bias, 1e-06)                                                                               {}\n",
      "call_function  silu_49                                                           aten.silu.default                                                 (group_norm_51,)                                                                                                                                                {}\n",
      "call_function  conv2d_63                                                         aten.conv2d.default                                               (silu_49, p_decoder_conv_out_weight, p_decoder_conv_out_bias, [1, 1], [1, 1])                                                                                   {}\n",
      "output         output                                                            output                                                            ((conv2d_63,),)                                                                                                                                                 {}\n",
      "graph():\n",
      "    %p_encoder_conv_in_weight : [num_users=1] = placeholder[target=p_encoder_conv_in_weight]\n",
      "    %p_encoder_conv_in_bias : [num_users=1] = placeholder[target=p_encoder_conv_in_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_norm1_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_norm1_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_conv1_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_conv1_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_norm2_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_norm2_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_conv2_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_conv2_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_norm1_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_norm1_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_conv1_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_conv1_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_norm2_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_norm2_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_conv2_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_conv2_bias]\n",
      "    %p_encoder_down_blocks_0_downsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_downsamplers_0_conv_weight]\n",
      "    %p_encoder_down_blocks_0_downsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_downsamplers_0_conv_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_norm1_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_norm1_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv1_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv1_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_norm2_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_norm2_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv2_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv2_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_norm1_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_norm1_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_conv1_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_conv1_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_norm2_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_norm2_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_conv2_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_conv2_bias]\n",
      "    %p_encoder_down_blocks_1_downsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_downsamplers_0_conv_weight]\n",
      "    %p_encoder_down_blocks_1_downsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_downsamplers_0_conv_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_norm1_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_norm1_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv1_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv1_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_norm2_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_norm2_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv2_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv2_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_norm1_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_norm1_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_conv1_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_conv1_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_norm2_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_norm2_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_conv2_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_conv2_bias]\n",
      "    %p_encoder_down_blocks_2_downsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_downsamplers_0_conv_weight]\n",
      "    %p_encoder_down_blocks_2_downsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_downsamplers_0_conv_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_norm1_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_norm1_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_conv1_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_conv1_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_norm2_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_norm2_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_conv2_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_conv2_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_norm1_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_norm1_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_conv1_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_conv1_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_norm2_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_norm2_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_conv2_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_conv2_bias]\n",
      "    %p_encoder_mid_block_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_norm1_weight]\n",
      "    %p_encoder_mid_block_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_norm1_bias]\n",
      "    %p_encoder_mid_block_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_conv1_weight]\n",
      "    %p_encoder_mid_block_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_conv1_bias]\n",
      "    %p_encoder_mid_block_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_norm2_weight]\n",
      "    %p_encoder_mid_block_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_norm2_bias]\n",
      "    %p_encoder_mid_block_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_conv2_weight]\n",
      "    %p_encoder_mid_block_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_conv2_bias]\n",
      "    %p_encoder_mid_block_attentions_0_group_norm_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_group_norm_weight]\n",
      "    %p_encoder_mid_block_attentions_0_group_norm_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_group_norm_bias]\n",
      "    %p_encoder_mid_block_attentions_0_to_q_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_q_weight]\n",
      "    %p_encoder_mid_block_attentions_0_to_q_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_q_bias]\n",
      "    %p_encoder_mid_block_attentions_0_to_k_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_k_weight]\n",
      "    %p_encoder_mid_block_attentions_0_to_k_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_k_bias]\n",
      "    %p_encoder_mid_block_attentions_0_to_v_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_v_weight]\n",
      "    %p_encoder_mid_block_attentions_0_to_v_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_v_bias]\n",
      "    %p_encoder_mid_block_attentions_0_to_out_0_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_out_0_weight]\n",
      "    %p_encoder_mid_block_attentions_0_to_out_0_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_out_0_bias]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias]\n",
      "    %p_encoder_conv_norm_out_weight : [num_users=1] = placeholder[target=p_encoder_conv_norm_out_weight]\n",
      "    %p_encoder_conv_norm_out_bias : [num_users=1] = placeholder[target=p_encoder_conv_norm_out_bias]\n",
      "    %p_encoder_conv_out_weight : [num_users=1] = placeholder[target=p_encoder_conv_out_weight]\n",
      "    %p_encoder_conv_out_bias : [num_users=1] = placeholder[target=p_encoder_conv_out_bias]\n",
      "    %p_quant_conv_weight : [num_users=1] = placeholder[target=p_quant_conv_weight]\n",
      "    %p_quant_conv_bias : [num_users=1] = placeholder[target=p_quant_conv_bias]\n",
      "    %p_post_quant_conv_weight : [num_users=1] = placeholder[target=p_post_quant_conv_weight]\n",
      "    %p_post_quant_conv_bias : [num_users=1] = placeholder[target=p_post_quant_conv_bias]\n",
      "    %p_decoder_conv_in_weight : [num_users=1] = placeholder[target=p_decoder_conv_in_weight]\n",
      "    %p_decoder_conv_in_bias : [num_users=1] = placeholder[target=p_decoder_conv_in_bias]\n",
      "    %p_decoder_mid_block_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_norm1_weight]\n",
      "    %p_decoder_mid_block_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_norm1_bias]\n",
      "    %p_decoder_mid_block_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_conv1_weight]\n",
      "    %p_decoder_mid_block_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_conv1_bias]\n",
      "    %p_decoder_mid_block_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_norm2_weight]\n",
      "    %p_decoder_mid_block_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_norm2_bias]\n",
      "    %p_decoder_mid_block_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_conv2_weight]\n",
      "    %p_decoder_mid_block_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_conv2_bias]\n",
      "    %p_decoder_mid_block_attentions_0_group_norm_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_group_norm_weight]\n",
      "    %p_decoder_mid_block_attentions_0_group_norm_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_group_norm_bias]\n",
      "    %p_decoder_mid_block_attentions_0_to_q_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_q_weight]\n",
      "    %p_decoder_mid_block_attentions_0_to_q_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_q_bias]\n",
      "    %p_decoder_mid_block_attentions_0_to_k_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_k_weight]\n",
      "    %p_decoder_mid_block_attentions_0_to_k_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_k_bias]\n",
      "    %p_decoder_mid_block_attentions_0_to_v_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_v_weight]\n",
      "    %p_decoder_mid_block_attentions_0_to_v_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_v_bias]\n",
      "    %p_decoder_mid_block_attentions_0_to_out_0_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_out_0_weight]\n",
      "    %p_decoder_mid_block_attentions_0_to_out_0_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_out_0_bias]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_norm1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_norm1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_conv1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_conv1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_norm2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_norm2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_conv2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_conv2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_norm1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_norm1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_conv1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_conv1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_norm2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_norm2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_conv2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_conv2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_norm1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_norm1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_conv1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_conv1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_norm2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_norm2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_conv2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_conv2_bias]\n",
      "    %p_decoder_up_blocks_0_upsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_upsamplers_0_conv_weight]\n",
      "    %p_decoder_up_blocks_0_upsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_upsamplers_0_conv_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_norm1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_norm1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_conv1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_conv1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_norm2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_norm2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_conv2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_conv2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_norm1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_norm1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_conv1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_conv1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_norm2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_norm2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_conv2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_conv2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_norm1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_norm1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_conv1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_conv1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_norm2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_norm2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_conv2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_conv2_bias]\n",
      "    %p_decoder_up_blocks_1_upsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_upsamplers_0_conv_weight]\n",
      "    %p_decoder_up_blocks_1_upsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_upsamplers_0_conv_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_norm1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_norm1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_norm2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_norm2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_norm1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_norm1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_conv1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_conv1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_norm2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_norm2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_conv2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_conv2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_norm1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_norm1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_conv1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_conv1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_norm2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_norm2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_conv2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_conv2_bias]\n",
      "    %p_decoder_up_blocks_2_upsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_upsamplers_0_conv_weight]\n",
      "    %p_decoder_up_blocks_2_upsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_upsamplers_0_conv_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_norm1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_norm1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_norm2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_norm2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_norm1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_norm1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_conv1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_conv1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_norm2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_norm2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_conv2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_conv2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_norm1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_norm1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_conv1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_conv1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_norm2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_norm2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_conv2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_conv2_bias]\n",
      "    %p_decoder_conv_norm_out_weight : [num_users=1] = placeholder[target=p_decoder_conv_norm_out_weight]\n",
      "    %p_decoder_conv_norm_out_bias : [num_users=1] = placeholder[target=p_decoder_conv_norm_out_bias]\n",
      "    %p_decoder_conv_out_weight : [num_users=1] = placeholder[target=p_decoder_conv_out_weight]\n",
      "    %p_decoder_conv_out_bias : [num_users=1] = placeholder[target=p_decoder_conv_out_bias]\n",
      "    %sample : [num_users=1] = placeholder[target=sample]\n",
      "    %conv2d : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%sample, %p_encoder_conv_in_weight, %p_encoder_conv_in_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d, 32, %p_encoder_down_blocks_0_resnets_0_norm1_weight, %p_encoder_down_blocks_0_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm,), kwargs = {})\n",
      "    %conv2d_1 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu, %p_encoder_down_blocks_0_resnets_0_conv1_weight, %p_encoder_down_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_1 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_1, 32, %p_encoder_down_blocks_0_resnets_0_norm2_weight, %p_encoder_down_blocks_0_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_1 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_1,), kwargs = {})\n",
      "    %dropout : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_1, 0.0, False), kwargs = {})\n",
      "    %conv2d_2 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout, %p_encoder_down_blocks_0_resnets_0_conv2_weight, %p_encoder_down_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d, %conv2d_2), kwargs = {})\n",
      "    %div : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add, 1.0), kwargs = {})\n",
      "    %group_norm_2 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div, 32, %p_encoder_down_blocks_0_resnets_1_norm1_weight, %p_encoder_down_blocks_0_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_2 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_2,), kwargs = {})\n",
      "    %conv2d_3 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_2, %p_encoder_down_blocks_0_resnets_1_conv1_weight, %p_encoder_down_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_3 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_3, 32, %p_encoder_down_blocks_0_resnets_1_norm2_weight, %p_encoder_down_blocks_0_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_3 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_3,), kwargs = {})\n",
      "    %dropout_1 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_3, 0.0, False), kwargs = {})\n",
      "    %conv2d_4 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_1, %p_encoder_down_blocks_0_resnets_1_conv2_weight, %p_encoder_down_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div, %conv2d_4), kwargs = {})\n",
      "    %div_1 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_1, 1.0), kwargs = {})\n",
      "    %pad : [num_users=1] = call_function[target=torch.ops.aten.pad.default](args = (%div_1, [0, 1, 0, 1], constant, 0.0), kwargs = {})\n",
      "    %conv2d_5 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%pad, %p_encoder_down_blocks_0_downsamplers_0_conv_weight, %p_encoder_down_blocks_0_downsamplers_0_conv_bias, [2, 2]), kwargs = {})\n",
      "    %group_norm_4 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_5, 32, %p_encoder_down_blocks_1_resnets_0_norm1_weight, %p_encoder_down_blocks_1_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_4 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_4,), kwargs = {})\n",
      "    %conv2d_6 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_4, %p_encoder_down_blocks_1_resnets_0_conv1_weight, %p_encoder_down_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_5 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_6, 32, %p_encoder_down_blocks_1_resnets_0_norm2_weight, %p_encoder_down_blocks_1_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_5 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_5,), kwargs = {})\n",
      "    %dropout_2 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_5, 0.0, False), kwargs = {})\n",
      "    %conv2d_7 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_2, %p_encoder_down_blocks_1_resnets_0_conv2_weight, %p_encoder_down_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_8 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_5, %p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight, %p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_8, %conv2d_7), kwargs = {})\n",
      "    %div_2 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_2, 1.0), kwargs = {})\n",
      "    %group_norm_6 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_2, 32, %p_encoder_down_blocks_1_resnets_1_norm1_weight, %p_encoder_down_blocks_1_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_6 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_6,), kwargs = {})\n",
      "    %conv2d_9 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_6, %p_encoder_down_blocks_1_resnets_1_conv1_weight, %p_encoder_down_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_7 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_9, 32, %p_encoder_down_blocks_1_resnets_1_norm2_weight, %p_encoder_down_blocks_1_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_7 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_7,), kwargs = {})\n",
      "    %dropout_3 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_7, 0.0, False), kwargs = {})\n",
      "    %conv2d_10 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_3, %p_encoder_down_blocks_1_resnets_1_conv2_weight, %p_encoder_down_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_3 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_2, %conv2d_10), kwargs = {})\n",
      "    %div_3 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_3, 1.0), kwargs = {})\n",
      "    %pad_1 : [num_users=1] = call_function[target=torch.ops.aten.pad.default](args = (%div_3, [0, 1, 0, 1], constant, 0.0), kwargs = {})\n",
      "    %conv2d_11 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%pad_1, %p_encoder_down_blocks_1_downsamplers_0_conv_weight, %p_encoder_down_blocks_1_downsamplers_0_conv_bias, [2, 2]), kwargs = {})\n",
      "    %group_norm_8 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_11, 32, %p_encoder_down_blocks_2_resnets_0_norm1_weight, %p_encoder_down_blocks_2_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_8 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_8,), kwargs = {})\n",
      "    %conv2d_12 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_8, %p_encoder_down_blocks_2_resnets_0_conv1_weight, %p_encoder_down_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_9 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_12, 32, %p_encoder_down_blocks_2_resnets_0_norm2_weight, %p_encoder_down_blocks_2_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_9 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_9,), kwargs = {})\n",
      "    %dropout_4 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_9, 0.0, False), kwargs = {})\n",
      "    %conv2d_13 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_4, %p_encoder_down_blocks_2_resnets_0_conv2_weight, %p_encoder_down_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_14 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_11, %p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight, %p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_4 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_14, %conv2d_13), kwargs = {})\n",
      "    %div_4 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_4, 1.0), kwargs = {})\n",
      "    %group_norm_10 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_4, 32, %p_encoder_down_blocks_2_resnets_1_norm1_weight, %p_encoder_down_blocks_2_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_10 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_10,), kwargs = {})\n",
      "    %conv2d_15 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_10, %p_encoder_down_blocks_2_resnets_1_conv1_weight, %p_encoder_down_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_11 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_15, 32, %p_encoder_down_blocks_2_resnets_1_norm2_weight, %p_encoder_down_blocks_2_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_11 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_11,), kwargs = {})\n",
      "    %dropout_5 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_11, 0.0, False), kwargs = {})\n",
      "    %conv2d_16 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_5, %p_encoder_down_blocks_2_resnets_1_conv2_weight, %p_encoder_down_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_5 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_4, %conv2d_16), kwargs = {})\n",
      "    %div_5 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_5, 1.0), kwargs = {})\n",
      "    %pad_2 : [num_users=1] = call_function[target=torch.ops.aten.pad.default](args = (%div_5, [0, 1, 0, 1], constant, 0.0), kwargs = {})\n",
      "    %conv2d_17 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%pad_2, %p_encoder_down_blocks_2_downsamplers_0_conv_weight, %p_encoder_down_blocks_2_downsamplers_0_conv_bias, [2, 2]), kwargs = {})\n",
      "    %group_norm_12 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_17, 32, %p_encoder_down_blocks_3_resnets_0_norm1_weight, %p_encoder_down_blocks_3_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_12 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_12,), kwargs = {})\n",
      "    %conv2d_18 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_12, %p_encoder_down_blocks_3_resnets_0_conv1_weight, %p_encoder_down_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_13 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_18, 32, %p_encoder_down_blocks_3_resnets_0_norm2_weight, %p_encoder_down_blocks_3_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_13 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_13,), kwargs = {})\n",
      "    %dropout_6 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_13, 0.0, False), kwargs = {})\n",
      "    %conv2d_19 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_6, %p_encoder_down_blocks_3_resnets_0_conv2_weight, %p_encoder_down_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_6 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_17, %conv2d_19), kwargs = {})\n",
      "    %div_6 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_6, 1.0), kwargs = {})\n",
      "    %group_norm_14 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_6, 32, %p_encoder_down_blocks_3_resnets_1_norm1_weight, %p_encoder_down_blocks_3_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_14 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_14,), kwargs = {})\n",
      "    %conv2d_20 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_14, %p_encoder_down_blocks_3_resnets_1_conv1_weight, %p_encoder_down_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_15 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_20, 32, %p_encoder_down_blocks_3_resnets_1_norm2_weight, %p_encoder_down_blocks_3_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_15 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_15,), kwargs = {})\n",
      "    %dropout_7 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_15, 0.0, False), kwargs = {})\n",
      "    %conv2d_21 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_7, %p_encoder_down_blocks_3_resnets_1_conv2_weight, %p_encoder_down_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_7 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_6, %conv2d_21), kwargs = {})\n",
      "    %div_7 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_7, 1.0), kwargs = {})\n",
      "    %group_norm_16 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_7, 32, %p_encoder_mid_block_resnets_0_norm1_weight, %p_encoder_mid_block_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_16 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_16,), kwargs = {})\n",
      "    %conv2d_22 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_16, %p_encoder_mid_block_resnets_0_conv1_weight, %p_encoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_17 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_22, 32, %p_encoder_mid_block_resnets_0_norm2_weight, %p_encoder_mid_block_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_17 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_17,), kwargs = {})\n",
      "    %dropout_8 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_17, 0.0, False), kwargs = {})\n",
      "    %conv2d_23 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_8, %p_encoder_mid_block_resnets_0_conv2_weight, %p_encoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_8 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_7, %conv2d_23), kwargs = {})\n",
      "    %div_8 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_8, 1), kwargs = {})\n",
      "    %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%div_8, [1, 512, 4096]), kwargs = {})\n",
      "    %transpose : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view, 1, 2), kwargs = {})\n",
      "    %transpose_1 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%transpose, 1, 2), kwargs = {})\n",
      "    %group_norm_18 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%transpose_1, 32, %p_encoder_mid_block_attentions_0_group_norm_weight, %p_encoder_mid_block_attentions_0_group_norm_bias, 1e-06), kwargs = {})\n",
      "    %transpose_2 : [num_users=3] = call_function[target=torch.ops.aten.transpose.int](args = (%group_norm_18, 1, 2), kwargs = {})\n",
      "    %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_2, %p_encoder_mid_block_attentions_0_to_q_weight, %p_encoder_mid_block_attentions_0_to_q_bias), kwargs = {})\n",
      "    %linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_2, %p_encoder_mid_block_attentions_0_to_k_weight, %p_encoder_mid_block_attentions_0_to_k_bias), kwargs = {})\n",
      "    %linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_2, %p_encoder_mid_block_attentions_0_to_v_weight, %p_encoder_mid_block_attentions_0_to_v_bias), kwargs = {})\n",
      "    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_3 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_1, 1, 2), kwargs = {})\n",
      "    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_1, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_4 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_2, 1, 2), kwargs = {})\n",
      "    %view_3 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_2, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_5 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_3, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_3, %transpose_4, %transpose_5), kwargs = {})\n",
      "    %transpose_6 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention, 1, 2), kwargs = {})\n",
      "    %view_4 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_6, [1, -1, 512]), kwargs = {})\n",
      "    %_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_4,), kwargs = {dtype: torch.float32})\n",
      "    %linear_3 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy, %p_encoder_mid_block_attentions_0_to_out_0_weight, %p_encoder_mid_block_attentions_0_to_out_0_bias), kwargs = {})\n",
      "    %dropout_9 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_3, 0.0, False), kwargs = {})\n",
      "    %transpose_7 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%dropout_9, -1, -2), kwargs = {})\n",
      "    %view_5 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_7, [1, 512, 64, 64]), kwargs = {})\n",
      "    %add_9 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_5, %div_8), kwargs = {})\n",
      "    %div_9 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_9, 1), kwargs = {})\n",
      "    %group_norm_19 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_9, 32, %p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight, %p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_18 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_19,), kwargs = {})\n",
      "    %conv2d_24 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_18, %p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight, %p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_20 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_24, 32, %p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight, %p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_19 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_20,), kwargs = {})\n",
      "    %dropout_10 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_19, 0.0, False), kwargs = {})\n",
      "    %conv2d_25 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_10, %p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight, %p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_10 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_9, %conv2d_25), kwargs = {})\n",
      "    %div_10 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_10, 1), kwargs = {})\n",
      "    %group_norm_21 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_10, 32, %p_encoder_conv_norm_out_weight, %p_encoder_conv_norm_out_bias, 1e-06), kwargs = {})\n",
      "    %silu_20 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_21,), kwargs = {})\n",
      "    %conv2d_26 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_20, %p_encoder_conv_out_weight, %p_encoder_conv_out_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_27 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_26, %p_quant_conv_weight, %p_quant_conv_bias), kwargs = {})\n",
      "    %split : [num_users=1] = call_function[target=torch.ops.aten.split.Tensor](args = (%conv2d_27, 4, 1), kwargs = {})\n",
      "    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n",
      "    %conv2d_28 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem, %p_post_quant_conv_weight, %p_post_quant_conv_bias), kwargs = {})\n",
      "    %conv2d_29 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_28, %p_decoder_conv_in_weight, %p_decoder_conv_in_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_22 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_29, 32, %p_decoder_mid_block_resnets_0_norm1_weight, %p_decoder_mid_block_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_21 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_22,), kwargs = {})\n",
      "    %conv2d_30 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_21, %p_decoder_mid_block_resnets_0_conv1_weight, %p_decoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_23 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_30, 32, %p_decoder_mid_block_resnets_0_norm2_weight, %p_decoder_mid_block_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_22 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_23,), kwargs = {})\n",
      "    %dropout_11 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_22, 0.0, False), kwargs = {})\n",
      "    %conv2d_31 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_11, %p_decoder_mid_block_resnets_0_conv2_weight, %p_decoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_11 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_29, %conv2d_31), kwargs = {})\n",
      "    %div_11 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_11, 1), kwargs = {})\n",
      "    %view_6 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%div_11, [1, 512, 4096]), kwargs = {})\n",
      "    %transpose_8 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_6, 1, 2), kwargs = {})\n",
      "    %transpose_9 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%transpose_8, 1, 2), kwargs = {})\n",
      "    %group_norm_24 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%transpose_9, 32, %p_decoder_mid_block_attentions_0_group_norm_weight, %p_decoder_mid_block_attentions_0_group_norm_bias, 1e-06), kwargs = {})\n",
      "    %transpose_10 : [num_users=3] = call_function[target=torch.ops.aten.transpose.int](args = (%group_norm_24, 1, 2), kwargs = {})\n",
      "    %linear_4 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_10, %p_decoder_mid_block_attentions_0_to_q_weight, %p_decoder_mid_block_attentions_0_to_q_bias), kwargs = {})\n",
      "    %linear_5 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_10, %p_decoder_mid_block_attentions_0_to_k_weight, %p_decoder_mid_block_attentions_0_to_k_bias), kwargs = {})\n",
      "    %linear_6 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_10, %p_decoder_mid_block_attentions_0_to_v_weight, %p_decoder_mid_block_attentions_0_to_v_bias), kwargs = {})\n",
      "    %view_7 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_4, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_11 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_7, 1, 2), kwargs = {})\n",
      "    %view_8 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_5, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_12 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_8, 1, 2), kwargs = {})\n",
      "    %view_9 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_6, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_13 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_9, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_11, %transpose_12, %transpose_13), kwargs = {})\n",
      "    %transpose_14 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_1, 1, 2), kwargs = {})\n",
      "    %view_10 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_14, [1, -1, 512]), kwargs = {})\n",
      "    %_to_copy_1 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_10,), kwargs = {dtype: torch.float32})\n",
      "    %linear_7 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_1, %p_decoder_mid_block_attentions_0_to_out_0_weight, %p_decoder_mid_block_attentions_0_to_out_0_bias), kwargs = {})\n",
      "    %dropout_12 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_7, 0.0, False), kwargs = {})\n",
      "    %transpose_15 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%dropout_12, -1, -2), kwargs = {})\n",
      "    %view_11 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_15, [1, 512, 64, 64]), kwargs = {})\n",
      "    %add_12 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_11, %div_11), kwargs = {})\n",
      "    %div_12 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_12, 1), kwargs = {})\n",
      "    %group_norm_25 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_12, 32, %p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight, %p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_23 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_25,), kwargs = {})\n",
      "    %conv2d_32 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_23, %p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight, %p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_26 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_32, 32, %p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight, %p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_24 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_26,), kwargs = {})\n",
      "    %dropout_13 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_24, 0.0, False), kwargs = {})\n",
      "    %conv2d_33 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_13, %p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight, %p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_13 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_12, %conv2d_33), kwargs = {})\n",
      "    %div_13 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_13, 1), kwargs = {})\n",
      "    %_to_copy_2 : [num_users=2] = call_function[target=torch.ops.aten._to_copy.default](args = (%div_13,), kwargs = {dtype: torch.float32})\n",
      "    %group_norm_27 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%_to_copy_2, 32, %p_decoder_up_blocks_0_resnets_0_norm1_weight, %p_decoder_up_blocks_0_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_25 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_27,), kwargs = {})\n",
      "    %conv2d_34 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_25, %p_decoder_up_blocks_0_resnets_0_conv1_weight, %p_decoder_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_28 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_34, 32, %p_decoder_up_blocks_0_resnets_0_norm2_weight, %p_decoder_up_blocks_0_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_26 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_28,), kwargs = {})\n",
      "    %dropout_14 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_26, 0.0, False), kwargs = {})\n",
      "    %conv2d_35 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_14, %p_decoder_up_blocks_0_resnets_0_conv2_weight, %p_decoder_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_14 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%_to_copy_2, %conv2d_35), kwargs = {})\n",
      "    %div_14 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_14, 1.0), kwargs = {})\n",
      "    %group_norm_29 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_14, 32, %p_decoder_up_blocks_0_resnets_1_norm1_weight, %p_decoder_up_blocks_0_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_27 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_29,), kwargs = {})\n",
      "    %conv2d_36 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_27, %p_decoder_up_blocks_0_resnets_1_conv1_weight, %p_decoder_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_30 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_36, 32, %p_decoder_up_blocks_0_resnets_1_norm2_weight, %p_decoder_up_blocks_0_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_28 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_30,), kwargs = {})\n",
      "    %dropout_15 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_28, 0.0, False), kwargs = {})\n",
      "    %conv2d_37 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_15, %p_decoder_up_blocks_0_resnets_1_conv2_weight, %p_decoder_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_15 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_14, %conv2d_37), kwargs = {})\n",
      "    %div_15 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_15, 1.0), kwargs = {})\n",
      "    %group_norm_31 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_15, 32, %p_decoder_up_blocks_0_resnets_2_norm1_weight, %p_decoder_up_blocks_0_resnets_2_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_29 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_31,), kwargs = {})\n",
      "    %conv2d_38 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_29, %p_decoder_up_blocks_0_resnets_2_conv1_weight, %p_decoder_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_32 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_38, 32, %p_decoder_up_blocks_0_resnets_2_norm2_weight, %p_decoder_up_blocks_0_resnets_2_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_30 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_32,), kwargs = {})\n",
      "    %dropout_16 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_30, 0.0, False), kwargs = {})\n",
      "    %conv2d_39 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_16, %p_decoder_up_blocks_0_resnets_2_conv2_weight, %p_decoder_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_16 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_15, %conv2d_39), kwargs = {})\n",
      "    %div_16 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_16, 1.0), kwargs = {})\n",
      "    %upsample_nearest2d : [num_users=1] = call_function[target=torch.ops.aten.upsample_nearest2d.vec](args = (%div_16, None, [2.0, 2.0]), kwargs = {})\n",
      "    %conv2d_40 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%upsample_nearest2d, %p_decoder_up_blocks_0_upsamplers_0_conv_weight, %p_decoder_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_33 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_40, 32, %p_decoder_up_blocks_1_resnets_0_norm1_weight, %p_decoder_up_blocks_1_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_31 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_33,), kwargs = {})\n",
      "    %conv2d_41 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_31, %p_decoder_up_blocks_1_resnets_0_conv1_weight, %p_decoder_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_34 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_41, 32, %p_decoder_up_blocks_1_resnets_0_norm2_weight, %p_decoder_up_blocks_1_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_32 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_34,), kwargs = {})\n",
      "    %dropout_17 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_32, 0.0, False), kwargs = {})\n",
      "    %conv2d_42 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_17, %p_decoder_up_blocks_1_resnets_0_conv2_weight, %p_decoder_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_17 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_40, %conv2d_42), kwargs = {})\n",
      "    %div_17 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_17, 1.0), kwargs = {})\n",
      "    %group_norm_35 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_17, 32, %p_decoder_up_blocks_1_resnets_1_norm1_weight, %p_decoder_up_blocks_1_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_33 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_35,), kwargs = {})\n",
      "    %conv2d_43 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_33, %p_decoder_up_blocks_1_resnets_1_conv1_weight, %p_decoder_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_36 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_43, 32, %p_decoder_up_blocks_1_resnets_1_norm2_weight, %p_decoder_up_blocks_1_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_34 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_36,), kwargs = {})\n",
      "    %dropout_18 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_34, 0.0, False), kwargs = {})\n",
      "    %conv2d_44 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_18, %p_decoder_up_blocks_1_resnets_1_conv2_weight, %p_decoder_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_18 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_17, %conv2d_44), kwargs = {})\n",
      "    %div_18 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_18, 1.0), kwargs = {})\n",
      "    %group_norm_37 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_18, 32, %p_decoder_up_blocks_1_resnets_2_norm1_weight, %p_decoder_up_blocks_1_resnets_2_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_35 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_37,), kwargs = {})\n",
      "    %conv2d_45 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_35, %p_decoder_up_blocks_1_resnets_2_conv1_weight, %p_decoder_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_38 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_45, 32, %p_decoder_up_blocks_1_resnets_2_norm2_weight, %p_decoder_up_blocks_1_resnets_2_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_36 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_38,), kwargs = {})\n",
      "    %dropout_19 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_36, 0.0, False), kwargs = {})\n",
      "    %conv2d_46 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_19, %p_decoder_up_blocks_1_resnets_2_conv2_weight, %p_decoder_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_19 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_18, %conv2d_46), kwargs = {})\n",
      "    %div_19 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_19, 1.0), kwargs = {})\n",
      "    %upsample_nearest2d_1 : [num_users=1] = call_function[target=torch.ops.aten.upsample_nearest2d.vec](args = (%div_19, None, [2.0, 2.0]), kwargs = {})\n",
      "    %conv2d_47 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%upsample_nearest2d_1, %p_decoder_up_blocks_1_upsamplers_0_conv_weight, %p_decoder_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_39 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_47, 32, %p_decoder_up_blocks_2_resnets_0_norm1_weight, %p_decoder_up_blocks_2_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_37 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_39,), kwargs = {})\n",
      "    %conv2d_48 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_37, %p_decoder_up_blocks_2_resnets_0_conv1_weight, %p_decoder_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_40 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_48, 32, %p_decoder_up_blocks_2_resnets_0_norm2_weight, %p_decoder_up_blocks_2_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_38 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_40,), kwargs = {})\n",
      "    %dropout_20 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_38, 0.0, False), kwargs = {})\n",
      "    %conv2d_49 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_20, %p_decoder_up_blocks_2_resnets_0_conv2_weight, %p_decoder_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%conv2d_47,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %conv2d_50 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone, %p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight, %p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_20 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_50, %conv2d_49), kwargs = {})\n",
      "    %div_20 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_20, 1.0), kwargs = {})\n",
      "    %group_norm_41 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_20, 32, %p_decoder_up_blocks_2_resnets_1_norm1_weight, %p_decoder_up_blocks_2_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_39 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_41,), kwargs = {})\n",
      "    %conv2d_51 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_39, %p_decoder_up_blocks_2_resnets_1_conv1_weight, %p_decoder_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_42 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_51, 32, %p_decoder_up_blocks_2_resnets_1_norm2_weight, %p_decoder_up_blocks_2_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_40 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_42,), kwargs = {})\n",
      "    %dropout_21 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_40, 0.0, False), kwargs = {})\n",
      "    %conv2d_52 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_21, %p_decoder_up_blocks_2_resnets_1_conv2_weight, %p_decoder_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_21 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_20, %conv2d_52), kwargs = {})\n",
      "    %div_21 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_21, 1.0), kwargs = {})\n",
      "    %group_norm_43 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_21, 32, %p_decoder_up_blocks_2_resnets_2_norm1_weight, %p_decoder_up_blocks_2_resnets_2_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_41 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_43,), kwargs = {})\n",
      "    %conv2d_53 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_41, %p_decoder_up_blocks_2_resnets_2_conv1_weight, %p_decoder_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_44 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_53, 32, %p_decoder_up_blocks_2_resnets_2_norm2_weight, %p_decoder_up_blocks_2_resnets_2_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_42 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_44,), kwargs = {})\n",
      "    %dropout_22 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_42, 0.0, False), kwargs = {})\n",
      "    %conv2d_54 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_22, %p_decoder_up_blocks_2_resnets_2_conv2_weight, %p_decoder_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_22 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_21, %conv2d_54), kwargs = {})\n",
      "    %div_22 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_22, 1.0), kwargs = {})\n",
      "    %upsample_nearest2d_2 : [num_users=1] = call_function[target=torch.ops.aten.upsample_nearest2d.vec](args = (%div_22, None, [2.0, 2.0]), kwargs = {})\n",
      "    %conv2d_55 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%upsample_nearest2d_2, %p_decoder_up_blocks_2_upsamplers_0_conv_weight, %p_decoder_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_45 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_55, 32, %p_decoder_up_blocks_3_resnets_0_norm1_weight, %p_decoder_up_blocks_3_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_43 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_45,), kwargs = {})\n",
      "    %conv2d_56 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_43, %p_decoder_up_blocks_3_resnets_0_conv1_weight, %p_decoder_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_46 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_56, 32, %p_decoder_up_blocks_3_resnets_0_norm2_weight, %p_decoder_up_blocks_3_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_44 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_46,), kwargs = {})\n",
      "    %dropout_23 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_44, 0.0, False), kwargs = {})\n",
      "    %conv2d_57 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_23, %p_decoder_up_blocks_3_resnets_0_conv2_weight, %p_decoder_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_58 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_55, %p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight, %p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_23 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_58, %conv2d_57), kwargs = {})\n",
      "    %div_23 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_23, 1.0), kwargs = {})\n",
      "    %group_norm_47 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_23, 32, %p_decoder_up_blocks_3_resnets_1_norm1_weight, %p_decoder_up_blocks_3_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_45 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_47,), kwargs = {})\n",
      "    %conv2d_59 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_45, %p_decoder_up_blocks_3_resnets_1_conv1_weight, %p_decoder_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_48 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_59, 32, %p_decoder_up_blocks_3_resnets_1_norm2_weight, %p_decoder_up_blocks_3_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_46 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_48,), kwargs = {})\n",
      "    %dropout_24 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_46, 0.0, False), kwargs = {})\n",
      "    %conv2d_60 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_24, %p_decoder_up_blocks_3_resnets_1_conv2_weight, %p_decoder_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_24 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_23, %conv2d_60), kwargs = {})\n",
      "    %div_24 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_24, 1.0), kwargs = {})\n",
      "    %group_norm_49 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_24, 32, %p_decoder_up_blocks_3_resnets_2_norm1_weight, %p_decoder_up_blocks_3_resnets_2_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_47 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_49,), kwargs = {})\n",
      "    %conv2d_61 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_47, %p_decoder_up_blocks_3_resnets_2_conv1_weight, %p_decoder_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_50 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_61, 32, %p_decoder_up_blocks_3_resnets_2_norm2_weight, %p_decoder_up_blocks_3_resnets_2_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_48 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_50,), kwargs = {})\n",
      "    %dropout_25 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_48, 0.0, False), kwargs = {})\n",
      "    %conv2d_62 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_25, %p_decoder_up_blocks_3_resnets_2_conv2_weight, %p_decoder_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_25 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_24, %conv2d_62), kwargs = {})\n",
      "    %div_25 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_25, 1.0), kwargs = {})\n",
      "    %group_norm_51 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_25, 32, %p_decoder_conv_norm_out_weight, %p_decoder_conv_norm_out_bias, 1e-06), kwargs = {})\n",
      "    %silu_49 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_51,), kwargs = {})\n",
      "    %conv2d_63 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_49, %p_decoder_conv_out_weight, %p_decoder_conv_out_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    return (conv2d_63,)\n",
      "VAE exported ➜  vae_fp32.ep\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from diffusers import AutoencoderKL\n",
    "# from torch.export import export\n",
    "# import inspect\n",
    "\n",
    "# CKPT = \"prs-eth/marigold-depth-v1-1\"\n",
    "\n",
    "# vae = AutoencoderKL.from_pretrained(CKPT, subfolder=\"vae\").cpu()\n",
    "# vae.eval()                                   # switch off dropout\n",
    "# class VAEEncodeDecode(torch.nn.Module):\n",
    "#     def __init__(self, core):\n",
    "#         super().__init__()\n",
    "#         self.core = core\n",
    "#     def forward(self, x):\n",
    "#         # Encode RGB → latent\n",
    "#         lat = self.core.encode(x).latent_dist.sample()\n",
    "#         # Decode latent → reconstruction\n",
    "#         img = self.core.decode(lat).sample\n",
    "#         return lat, img            # returns a tuple\n",
    "\n",
    "# wrapper = VAEEncodeDecode(vae)\n",
    "# rgb = torch.randn(1, 3, 512, 512)\n",
    "# gm = export(wrapper, (rgb,))        # gm is already a GraphModule\n",
    "# gm.graph.print_tabular()            # pretty table\n",
    "# # or just:\n",
    "# print(gm.graph)                   # raw text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f14f8a",
   "metadata": {},
   "source": [
    "## VAE implemenation fo F32graph saving before ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae287a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                                                              target                                                            args                                                                                                                                                            kwargs\n",
      "-------------  ----------------------------------------------------------------  ----------------------------------------------------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------------------------------\n",
      "placeholder    p_encoder_conv_in_weight                                          p_encoder_conv_in_weight                                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_in_bias                                            p_encoder_conv_in_bias                                            ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm1_weight                    p_encoder_down_blocks_0_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm1_bias                      p_encoder_down_blocks_0_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv1_weight                    p_encoder_down_blocks_0_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv1_bias                      p_encoder_down_blocks_0_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm2_weight                    p_encoder_down_blocks_0_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm2_bias                      p_encoder_down_blocks_0_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv2_weight                    p_encoder_down_blocks_0_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv2_bias                      p_encoder_down_blocks_0_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm1_weight                    p_encoder_down_blocks_0_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm1_bias                      p_encoder_down_blocks_0_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv1_weight                    p_encoder_down_blocks_0_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv1_bias                      p_encoder_down_blocks_0_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm2_weight                    p_encoder_down_blocks_0_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm2_bias                      p_encoder_down_blocks_0_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv2_weight                    p_encoder_down_blocks_0_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv2_bias                      p_encoder_down_blocks_0_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_downsamplers_0_conv_weight                p_encoder_down_blocks_0_downsamplers_0_conv_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_downsamplers_0_conv_bias                  p_encoder_down_blocks_0_downsamplers_0_conv_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm1_weight                    p_encoder_down_blocks_1_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm1_bias                      p_encoder_down_blocks_1_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv1_weight                    p_encoder_down_blocks_1_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv1_bias                      p_encoder_down_blocks_1_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm2_weight                    p_encoder_down_blocks_1_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm2_bias                      p_encoder_down_blocks_1_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv2_weight                    p_encoder_down_blocks_1_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv2_bias                      p_encoder_down_blocks_1_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight            p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight            ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias              p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias              ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm1_weight                    p_encoder_down_blocks_1_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm1_bias                      p_encoder_down_blocks_1_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv1_weight                    p_encoder_down_blocks_1_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv1_bias                      p_encoder_down_blocks_1_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm2_weight                    p_encoder_down_blocks_1_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm2_bias                      p_encoder_down_blocks_1_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv2_weight                    p_encoder_down_blocks_1_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv2_bias                      p_encoder_down_blocks_1_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_downsamplers_0_conv_weight                p_encoder_down_blocks_1_downsamplers_0_conv_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_downsamplers_0_conv_bias                  p_encoder_down_blocks_1_downsamplers_0_conv_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm1_weight                    p_encoder_down_blocks_2_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm1_bias                      p_encoder_down_blocks_2_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv1_weight                    p_encoder_down_blocks_2_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv1_bias                      p_encoder_down_blocks_2_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm2_weight                    p_encoder_down_blocks_2_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm2_bias                      p_encoder_down_blocks_2_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv2_weight                    p_encoder_down_blocks_2_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv2_bias                      p_encoder_down_blocks_2_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight            p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight            ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias              p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias              ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm1_weight                    p_encoder_down_blocks_2_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm1_bias                      p_encoder_down_blocks_2_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv1_weight                    p_encoder_down_blocks_2_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv1_bias                      p_encoder_down_blocks_2_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm2_weight                    p_encoder_down_blocks_2_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm2_bias                      p_encoder_down_blocks_2_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv2_weight                    p_encoder_down_blocks_2_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv2_bias                      p_encoder_down_blocks_2_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_downsamplers_0_conv_weight                p_encoder_down_blocks_2_downsamplers_0_conv_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_downsamplers_0_conv_bias                  p_encoder_down_blocks_2_downsamplers_0_conv_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm1_weight                    p_encoder_down_blocks_3_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm1_bias                      p_encoder_down_blocks_3_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv1_weight                    p_encoder_down_blocks_3_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv1_bias                      p_encoder_down_blocks_3_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm2_weight                    p_encoder_down_blocks_3_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm2_bias                      p_encoder_down_blocks_3_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv2_weight                    p_encoder_down_blocks_3_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv2_bias                      p_encoder_down_blocks_3_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm1_weight                    p_encoder_down_blocks_3_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm1_bias                      p_encoder_down_blocks_3_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv1_weight                    p_encoder_down_blocks_3_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv1_bias                      p_encoder_down_blocks_3_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm2_weight                    p_encoder_down_blocks_3_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm2_bias                      p_encoder_down_blocks_3_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv2_weight                    p_encoder_down_blocks_3_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv2_bias                      p_encoder_down_blocks_3_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm1_weight                        p_encoder_mid_block_resnets_0_norm1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm1_bias                          p_encoder_mid_block_resnets_0_norm1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv1_weight                        p_encoder_mid_block_resnets_0_conv1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv1_bias                          p_encoder_mid_block_resnets_0_conv1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm2_weight                        p_encoder_mid_block_resnets_0_norm2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm2_bias                          p_encoder_mid_block_resnets_0_norm2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv2_weight                        p_encoder_mid_block_resnets_0_conv2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv2_bias                          p_encoder_mid_block_resnets_0_conv2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_group_norm_weight                p_encoder_mid_block_attentions_0_group_norm_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_group_norm_bias                  p_encoder_mid_block_attentions_0_group_norm_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_q_weight                      p_encoder_mid_block_attentions_0_to_q_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_q_bias                        p_encoder_mid_block_attentions_0_to_q_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_k_weight                      p_encoder_mid_block_attentions_0_to_k_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_k_bias                        p_encoder_mid_block_attentions_0_to_k_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_v_weight                      p_encoder_mid_block_attentions_0_to_v_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_v_bias                        p_encoder_mid_block_attentions_0_to_v_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_out_0_weight                  p_encoder_mid_block_attentions_0_to_out_0_weight                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_out_0_bias                    p_encoder_mid_block_attentions_0_to_out_0_bias                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_norm_out_weight                                    p_encoder_conv_norm_out_weight                                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_norm_out_bias                                      p_encoder_conv_norm_out_bias                                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_out_weight                                         p_encoder_conv_out_weight                                         ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_out_bias                                           p_encoder_conv_out_bias                                           ()                                                                                                                                                              {}\n",
      "placeholder    p_quant_conv_weight                                               p_quant_conv_weight                                               ()                                                                                                                                                              {}\n",
      "placeholder    p_quant_conv_bias                                                 p_quant_conv_bias                                                 ()                                                                                                                                                              {}\n",
      "placeholder    p_post_quant_conv_weight                                          p_post_quant_conv_weight                                          ()                                                                                                                                                              {}\n",
      "placeholder    p_post_quant_conv_bias                                            p_post_quant_conv_bias                                            ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_in_weight                                          p_decoder_conv_in_weight                                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_in_bias                                            p_decoder_conv_in_bias                                            ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm1_weight                        p_decoder_mid_block_resnets_0_norm1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm1_bias                          p_decoder_mid_block_resnets_0_norm1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv1_weight                        p_decoder_mid_block_resnets_0_conv1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv1_bias                          p_decoder_mid_block_resnets_0_conv1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm2_weight                        p_decoder_mid_block_resnets_0_norm2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm2_bias                          p_decoder_mid_block_resnets_0_norm2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv2_weight                        p_decoder_mid_block_resnets_0_conv2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv2_bias                          p_decoder_mid_block_resnets_0_conv2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_group_norm_weight                p_decoder_mid_block_attentions_0_group_norm_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_group_norm_bias                  p_decoder_mid_block_attentions_0_group_norm_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_q_weight                      p_decoder_mid_block_attentions_0_to_q_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_q_bias                        p_decoder_mid_block_attentions_0_to_q_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_k_weight                      p_decoder_mid_block_attentions_0_to_k_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_k_bias                        p_decoder_mid_block_attentions_0_to_k_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_v_weight                      p_decoder_mid_block_attentions_0_to_v_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_v_bias                        p_decoder_mid_block_attentions_0_to_v_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_out_0_weight                  p_decoder_mid_block_attentions_0_to_out_0_weight                  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_out_0_bias                    p_decoder_mid_block_attentions_0_to_out_0_bias                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm1_weight                      p_decoder_up_blocks_0_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm1_bias                        p_decoder_up_blocks_0_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv1_weight                      p_decoder_up_blocks_0_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv1_bias                        p_decoder_up_blocks_0_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm2_weight                      p_decoder_up_blocks_0_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm2_bias                        p_decoder_up_blocks_0_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv2_weight                      p_decoder_up_blocks_0_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv2_bias                        p_decoder_up_blocks_0_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm1_weight                      p_decoder_up_blocks_0_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm1_bias                        p_decoder_up_blocks_0_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv1_weight                      p_decoder_up_blocks_0_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv1_bias                        p_decoder_up_blocks_0_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm2_weight                      p_decoder_up_blocks_0_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm2_bias                        p_decoder_up_blocks_0_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv2_weight                      p_decoder_up_blocks_0_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv2_bias                        p_decoder_up_blocks_0_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm1_weight                      p_decoder_up_blocks_0_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm1_bias                        p_decoder_up_blocks_0_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv1_weight                      p_decoder_up_blocks_0_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv1_bias                        p_decoder_up_blocks_0_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm2_weight                      p_decoder_up_blocks_0_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm2_bias                        p_decoder_up_blocks_0_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv2_weight                      p_decoder_up_blocks_0_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv2_bias                        p_decoder_up_blocks_0_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_upsamplers_0_conv_weight                    p_decoder_up_blocks_0_upsamplers_0_conv_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_upsamplers_0_conv_bias                      p_decoder_up_blocks_0_upsamplers_0_conv_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm1_weight                      p_decoder_up_blocks_1_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm1_bias                        p_decoder_up_blocks_1_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv1_weight                      p_decoder_up_blocks_1_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv1_bias                        p_decoder_up_blocks_1_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm2_weight                      p_decoder_up_blocks_1_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm2_bias                        p_decoder_up_blocks_1_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv2_weight                      p_decoder_up_blocks_1_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv2_bias                        p_decoder_up_blocks_1_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm1_weight                      p_decoder_up_blocks_1_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm1_bias                        p_decoder_up_blocks_1_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv1_weight                      p_decoder_up_blocks_1_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv1_bias                        p_decoder_up_blocks_1_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm2_weight                      p_decoder_up_blocks_1_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm2_bias                        p_decoder_up_blocks_1_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv2_weight                      p_decoder_up_blocks_1_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv2_bias                        p_decoder_up_blocks_1_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm1_weight                      p_decoder_up_blocks_1_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm1_bias                        p_decoder_up_blocks_1_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv1_weight                      p_decoder_up_blocks_1_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv1_bias                        p_decoder_up_blocks_1_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm2_weight                      p_decoder_up_blocks_1_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm2_bias                        p_decoder_up_blocks_1_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv2_weight                      p_decoder_up_blocks_1_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv2_bias                        p_decoder_up_blocks_1_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_upsamplers_0_conv_weight                    p_decoder_up_blocks_1_upsamplers_0_conv_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_upsamplers_0_conv_bias                      p_decoder_up_blocks_1_upsamplers_0_conv_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm1_weight                      p_decoder_up_blocks_2_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm1_bias                        p_decoder_up_blocks_2_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv1_weight                      p_decoder_up_blocks_2_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv1_bias                        p_decoder_up_blocks_2_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm2_weight                      p_decoder_up_blocks_2_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm2_bias                        p_decoder_up_blocks_2_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv2_weight                      p_decoder_up_blocks_2_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv2_bias                        p_decoder_up_blocks_2_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight              p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight              ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias                p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias                ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm1_weight                      p_decoder_up_blocks_2_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm1_bias                        p_decoder_up_blocks_2_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv1_weight                      p_decoder_up_blocks_2_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv1_bias                        p_decoder_up_blocks_2_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm2_weight                      p_decoder_up_blocks_2_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm2_bias                        p_decoder_up_blocks_2_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv2_weight                      p_decoder_up_blocks_2_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv2_bias                        p_decoder_up_blocks_2_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm1_weight                      p_decoder_up_blocks_2_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm1_bias                        p_decoder_up_blocks_2_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv1_weight                      p_decoder_up_blocks_2_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv1_bias                        p_decoder_up_blocks_2_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm2_weight                      p_decoder_up_blocks_2_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm2_bias                        p_decoder_up_blocks_2_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv2_weight                      p_decoder_up_blocks_2_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv2_bias                        p_decoder_up_blocks_2_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_upsamplers_0_conv_weight                    p_decoder_up_blocks_2_upsamplers_0_conv_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_upsamplers_0_conv_bias                      p_decoder_up_blocks_2_upsamplers_0_conv_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm1_weight                      p_decoder_up_blocks_3_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm1_bias                        p_decoder_up_blocks_3_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv1_weight                      p_decoder_up_blocks_3_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv1_bias                        p_decoder_up_blocks_3_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm2_weight                      p_decoder_up_blocks_3_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm2_bias                        p_decoder_up_blocks_3_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv2_weight                      p_decoder_up_blocks_3_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv2_bias                        p_decoder_up_blocks_3_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight              p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight              ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias                p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias                ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm1_weight                      p_decoder_up_blocks_3_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm1_bias                        p_decoder_up_blocks_3_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv1_weight                      p_decoder_up_blocks_3_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv1_bias                        p_decoder_up_blocks_3_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm2_weight                      p_decoder_up_blocks_3_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm2_bias                        p_decoder_up_blocks_3_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv2_weight                      p_decoder_up_blocks_3_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv2_bias                        p_decoder_up_blocks_3_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm1_weight                      p_decoder_up_blocks_3_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm1_bias                        p_decoder_up_blocks_3_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv1_weight                      p_decoder_up_blocks_3_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv1_bias                        p_decoder_up_blocks_3_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm2_weight                      p_decoder_up_blocks_3_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm2_bias                        p_decoder_up_blocks_3_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv2_weight                      p_decoder_up_blocks_3_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv2_bias                        p_decoder_up_blocks_3_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_norm_out_weight                                    p_decoder_conv_norm_out_weight                                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_norm_out_bias                                      p_decoder_conv_norm_out_bias                                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_out_weight                                         p_decoder_conv_out_weight                                         ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_out_bias                                           p_decoder_conv_out_bias                                           ()                                                                                                                                                              {}\n",
      "placeholder    sample                                                            sample                                                            ()                                                                                                                                                              {}\n",
      "call_function  conv2d                                                            aten.conv2d.default                                               (sample, p_encoder_conv_in_weight, p_encoder_conv_in_bias, [1, 1], [1, 1])                                                                                      {}\n",
      "call_function  group_norm                                                        aten.group_norm.default                                           (conv2d, 32, p_encoder_down_blocks_0_resnets_0_norm1_weight, p_encoder_down_blocks_0_resnets_0_norm1_bias, 1e-06)                                               {}\n",
      "call_function  silu                                                              aten.silu.default                                                 (group_norm,)                                                                                                                                                   {}\n",
      "call_function  conv2d_1                                                          aten.conv2d.default                                               (silu, p_encoder_down_blocks_0_resnets_0_conv1_weight, p_encoder_down_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1])                                            {}\n",
      "call_function  group_norm_1                                                      aten.group_norm.default                                           (conv2d_1, 32, p_encoder_down_blocks_0_resnets_0_norm2_weight, p_encoder_down_blocks_0_resnets_0_norm2_bias, 1e-06)                                             {}\n",
      "call_function  silu_1                                                            aten.silu.default                                                 (group_norm_1,)                                                                                                                                                 {}\n",
      "call_function  dropout                                                           aten.dropout.default                                              (silu_1, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_2                                                          aten.conv2d.default                                               (dropout, p_encoder_down_blocks_0_resnets_0_conv2_weight, p_encoder_down_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  add                                                               aten.add.Tensor                                                   (conv2d, conv2d_2)                                                                                                                                              {}\n",
      "call_function  div                                                               aten.div.Tensor                                                   (add, 1.0)                                                                                                                                                      {}\n",
      "call_function  group_norm_2                                                      aten.group_norm.default                                           (div, 32, p_encoder_down_blocks_0_resnets_1_norm1_weight, p_encoder_down_blocks_0_resnets_1_norm1_bias, 1e-06)                                                  {}\n",
      "call_function  silu_2                                                            aten.silu.default                                                 (group_norm_2,)                                                                                                                                                 {}\n",
      "call_function  conv2d_3                                                          aten.conv2d.default                                               (silu_2, p_encoder_down_blocks_0_resnets_1_conv1_weight, p_encoder_down_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_3                                                      aten.group_norm.default                                           (conv2d_3, 32, p_encoder_down_blocks_0_resnets_1_norm2_weight, p_encoder_down_blocks_0_resnets_1_norm2_bias, 1e-06)                                             {}\n",
      "call_function  silu_3                                                            aten.silu.default                                                 (group_norm_3,)                                                                                                                                                 {}\n",
      "call_function  dropout_1                                                         aten.dropout.default                                              (silu_3, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_4                                                          aten.conv2d.default                                               (dropout_1, p_encoder_down_blocks_0_resnets_1_conv2_weight, p_encoder_down_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_1                                                             aten.add.Tensor                                                   (div, conv2d_4)                                                                                                                                                 {}\n",
      "call_function  div_1                                                             aten.div.Tensor                                                   (add_1, 1.0)                                                                                                                                                    {}\n",
      "call_function  pad                                                               aten.pad.default                                                  (div_1, [0, 1, 0, 1], 'constant', 0.0)                                                                                                                          {}\n",
      "call_function  conv2d_5                                                          aten.conv2d.default                                               (pad, p_encoder_down_blocks_0_downsamplers_0_conv_weight, p_encoder_down_blocks_0_downsamplers_0_conv_bias, [2, 2])                                             {}\n",
      "call_function  group_norm_4                                                      aten.group_norm.default                                           (conv2d_5, 32, p_encoder_down_blocks_1_resnets_0_norm1_weight, p_encoder_down_blocks_1_resnets_0_norm1_bias, 1e-06)                                             {}\n",
      "call_function  silu_4                                                            aten.silu.default                                                 (group_norm_4,)                                                                                                                                                 {}\n",
      "call_function  conv2d_6                                                          aten.conv2d.default                                               (silu_4, p_encoder_down_blocks_1_resnets_0_conv1_weight, p_encoder_down_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_5                                                      aten.group_norm.default                                           (conv2d_6, 32, p_encoder_down_blocks_1_resnets_0_norm2_weight, p_encoder_down_blocks_1_resnets_0_norm2_bias, 1e-06)                                             {}\n",
      "call_function  silu_5                                                            aten.silu.default                                                 (group_norm_5,)                                                                                                                                                 {}\n",
      "call_function  dropout_2                                                         aten.dropout.default                                              (silu_5, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_7                                                          aten.conv2d.default                                               (dropout_2, p_encoder_down_blocks_1_resnets_0_conv2_weight, p_encoder_down_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  conv2d_8                                                          aten.conv2d.default                                               (conv2d_5, p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight, p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias)                                        {}\n",
      "call_function  add_2                                                             aten.add.Tensor                                                   (conv2d_8, conv2d_7)                                                                                                                                            {}\n",
      "call_function  div_2                                                             aten.div.Tensor                                                   (add_2, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_6                                                      aten.group_norm.default                                           (div_2, 32, p_encoder_down_blocks_1_resnets_1_norm1_weight, p_encoder_down_blocks_1_resnets_1_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_6                                                            aten.silu.default                                                 (group_norm_6,)                                                                                                                                                 {}\n",
      "call_function  conv2d_9                                                          aten.conv2d.default                                               (silu_6, p_encoder_down_blocks_1_resnets_1_conv1_weight, p_encoder_down_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_7                                                      aten.group_norm.default                                           (conv2d_9, 32, p_encoder_down_blocks_1_resnets_1_norm2_weight, p_encoder_down_blocks_1_resnets_1_norm2_bias, 1e-06)                                             {}\n",
      "call_function  silu_7                                                            aten.silu.default                                                 (group_norm_7,)                                                                                                                                                 {}\n",
      "call_function  dropout_3                                                         aten.dropout.default                                              (silu_7, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_10                                                         aten.conv2d.default                                               (dropout_3, p_encoder_down_blocks_1_resnets_1_conv2_weight, p_encoder_down_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_3                                                             aten.add.Tensor                                                   (div_2, conv2d_10)                                                                                                                                              {}\n",
      "call_function  div_3                                                             aten.div.Tensor                                                   (add_3, 1.0)                                                                                                                                                    {}\n",
      "call_function  pad_1                                                             aten.pad.default                                                  (div_3, [0, 1, 0, 1], 'constant', 0.0)                                                                                                                          {}\n",
      "call_function  conv2d_11                                                         aten.conv2d.default                                               (pad_1, p_encoder_down_blocks_1_downsamplers_0_conv_weight, p_encoder_down_blocks_1_downsamplers_0_conv_bias, [2, 2])                                           {}\n",
      "call_function  group_norm_8                                                      aten.group_norm.default                                           (conv2d_11, 32, p_encoder_down_blocks_2_resnets_0_norm1_weight, p_encoder_down_blocks_2_resnets_0_norm1_bias, 1e-06)                                            {}\n",
      "call_function  silu_8                                                            aten.silu.default                                                 (group_norm_8,)                                                                                                                                                 {}\n",
      "call_function  conv2d_12                                                         aten.conv2d.default                                               (silu_8, p_encoder_down_blocks_2_resnets_0_conv1_weight, p_encoder_down_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_9                                                      aten.group_norm.default                                           (conv2d_12, 32, p_encoder_down_blocks_2_resnets_0_norm2_weight, p_encoder_down_blocks_2_resnets_0_norm2_bias, 1e-06)                                            {}\n",
      "call_function  silu_9                                                            aten.silu.default                                                 (group_norm_9,)                                                                                                                                                 {}\n",
      "call_function  dropout_4                                                         aten.dropout.default                                              (silu_9, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_13                                                         aten.conv2d.default                                               (dropout_4, p_encoder_down_blocks_2_resnets_0_conv2_weight, p_encoder_down_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  conv2d_14                                                         aten.conv2d.default                                               (conv2d_11, p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight, p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias)                                       {}\n",
      "call_function  add_4                                                             aten.add.Tensor                                                   (conv2d_14, conv2d_13)                                                                                                                                          {}\n",
      "call_function  div_4                                                             aten.div.Tensor                                                   (add_4, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_10                                                     aten.group_norm.default                                           (div_4, 32, p_encoder_down_blocks_2_resnets_1_norm1_weight, p_encoder_down_blocks_2_resnets_1_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_10                                                           aten.silu.default                                                 (group_norm_10,)                                                                                                                                                {}\n",
      "call_function  conv2d_15                                                         aten.conv2d.default                                               (silu_10, p_encoder_down_blocks_2_resnets_1_conv1_weight, p_encoder_down_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  group_norm_11                                                     aten.group_norm.default                                           (conv2d_15, 32, p_encoder_down_blocks_2_resnets_1_norm2_weight, p_encoder_down_blocks_2_resnets_1_norm2_bias, 1e-06)                                            {}\n",
      "call_function  silu_11                                                           aten.silu.default                                                 (group_norm_11,)                                                                                                                                                {}\n",
      "call_function  dropout_5                                                         aten.dropout.default                                              (silu_11, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_16                                                         aten.conv2d.default                                               (dropout_5, p_encoder_down_blocks_2_resnets_1_conv2_weight, p_encoder_down_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_5                                                             aten.add.Tensor                                                   (div_4, conv2d_16)                                                                                                                                              {}\n",
      "call_function  div_5                                                             aten.div.Tensor                                                   (add_5, 1.0)                                                                                                                                                    {}\n",
      "call_function  pad_2                                                             aten.pad.default                                                  (div_5, [0, 1, 0, 1], 'constant', 0.0)                                                                                                                          {}\n",
      "call_function  conv2d_17                                                         aten.conv2d.default                                               (pad_2, p_encoder_down_blocks_2_downsamplers_0_conv_weight, p_encoder_down_blocks_2_downsamplers_0_conv_bias, [2, 2])                                           {}\n",
      "call_function  group_norm_12                                                     aten.group_norm.default                                           (conv2d_17, 32, p_encoder_down_blocks_3_resnets_0_norm1_weight, p_encoder_down_blocks_3_resnets_0_norm1_bias, 1e-06)                                            {}\n",
      "call_function  silu_12                                                           aten.silu.default                                                 (group_norm_12,)                                                                                                                                                {}\n",
      "call_function  conv2d_18                                                         aten.conv2d.default                                               (silu_12, p_encoder_down_blocks_3_resnets_0_conv1_weight, p_encoder_down_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  group_norm_13                                                     aten.group_norm.default                                           (conv2d_18, 32, p_encoder_down_blocks_3_resnets_0_norm2_weight, p_encoder_down_blocks_3_resnets_0_norm2_bias, 1e-06)                                            {}\n",
      "call_function  silu_13                                                           aten.silu.default                                                 (group_norm_13,)                                                                                                                                                {}\n",
      "call_function  dropout_6                                                         aten.dropout.default                                              (silu_13, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_19                                                         aten.conv2d.default                                               (dropout_6, p_encoder_down_blocks_3_resnets_0_conv2_weight, p_encoder_down_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_6                                                             aten.add.Tensor                                                   (conv2d_17, conv2d_19)                                                                                                                                          {}\n",
      "call_function  div_6                                                             aten.div.Tensor                                                   (add_6, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_14                                                     aten.group_norm.default                                           (div_6, 32, p_encoder_down_blocks_3_resnets_1_norm1_weight, p_encoder_down_blocks_3_resnets_1_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_14                                                           aten.silu.default                                                 (group_norm_14,)                                                                                                                                                {}\n",
      "call_function  conv2d_20                                                         aten.conv2d.default                                               (silu_14, p_encoder_down_blocks_3_resnets_1_conv1_weight, p_encoder_down_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  group_norm_15                                                     aten.group_norm.default                                           (conv2d_20, 32, p_encoder_down_blocks_3_resnets_1_norm2_weight, p_encoder_down_blocks_3_resnets_1_norm2_bias, 1e-06)                                            {}\n",
      "call_function  silu_15                                                           aten.silu.default                                                 (group_norm_15,)                                                                                                                                                {}\n",
      "call_function  dropout_7                                                         aten.dropout.default                                              (silu_15, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_21                                                         aten.conv2d.default                                               (dropout_7, p_encoder_down_blocks_3_resnets_1_conv2_weight, p_encoder_down_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_7                                                             aten.add.Tensor                                                   (div_6, conv2d_21)                                                                                                                                              {}\n",
      "call_function  div_7                                                             aten.div.Tensor                                                   (add_7, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_16                                                     aten.group_norm.default                                           (div_7, 32, p_encoder_mid_block_resnets_0_norm1_weight, p_encoder_mid_block_resnets_0_norm1_bias, 1e-06)                                                        {}\n",
      "call_function  silu_16                                                           aten.silu.default                                                 (group_norm_16,)                                                                                                                                                {}\n",
      "call_function  conv2d_22                                                         aten.conv2d.default                                               (silu_16, p_encoder_mid_block_resnets_0_conv1_weight, p_encoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1])                                                 {}\n",
      "call_function  group_norm_17                                                     aten.group_norm.default                                           (conv2d_22, 32, p_encoder_mid_block_resnets_0_norm2_weight, p_encoder_mid_block_resnets_0_norm2_bias, 1e-06)                                                    {}\n",
      "call_function  silu_17                                                           aten.silu.default                                                 (group_norm_17,)                                                                                                                                                {}\n",
      "call_function  dropout_8                                                         aten.dropout.default                                              (silu_17, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_23                                                         aten.conv2d.default                                               (dropout_8, p_encoder_mid_block_resnets_0_conv2_weight, p_encoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1])                                               {}\n",
      "call_function  add_8                                                             aten.add.Tensor                                                   (div_7, conv2d_23)                                                                                                                                              {}\n",
      "call_function  div_8                                                             aten.div.Tensor                                                   (add_8, 1)                                                                                                                                                      {}\n",
      "call_function  view                                                              aten.view.default                                                 (div_8, [1, 512, 4096])                                                                                                                                         {}\n",
      "call_function  transpose                                                         aten.transpose.int                                                (view, 1, 2)                                                                                                                                                    {}\n",
      "call_function  transpose_1                                                       aten.transpose.int                                                (transpose, 1, 2)                                                                                                                                               {}\n",
      "call_function  group_norm_18                                                     aten.group_norm.default                                           (transpose_1, 32, p_encoder_mid_block_attentions_0_group_norm_weight, p_encoder_mid_block_attentions_0_group_norm_bias, 1e-06)                                  {}\n",
      "call_function  transpose_2                                                       aten.transpose.int                                                (group_norm_18, 1, 2)                                                                                                                                           {}\n",
      "call_function  linear                                                            aten.linear.default                                               (transpose_2, p_encoder_mid_block_attentions_0_to_q_weight, p_encoder_mid_block_attentions_0_to_q_bias)                                                         {}\n",
      "call_function  linear_1                                                          aten.linear.default                                               (transpose_2, p_encoder_mid_block_attentions_0_to_k_weight, p_encoder_mid_block_attentions_0_to_k_bias)                                                         {}\n",
      "call_function  linear_2                                                          aten.linear.default                                               (transpose_2, p_encoder_mid_block_attentions_0_to_v_weight, p_encoder_mid_block_attentions_0_to_v_bias)                                                         {}\n",
      "call_function  view_1                                                            aten.view.default                                                 (linear, [1, -1, 1, 512])                                                                                                                                       {}\n",
      "call_function  transpose_3                                                       aten.transpose.int                                                (view_1, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_2                                                            aten.view.default                                                 (linear_1, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_4                                                       aten.transpose.int                                                (view_2, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_3                                                            aten.view.default                                                 (linear_2, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_5                                                       aten.transpose.int                                                (view_3, 1, 2)                                                                                                                                                  {}\n",
      "call_function  scaled_dot_product_attention                                      aten.scaled_dot_product_attention.default                         (transpose_3, transpose_4, transpose_5)                                                                                                                         {}\n",
      "call_function  transpose_6                                                       aten.transpose.int                                                (scaled_dot_product_attention, 1, 2)                                                                                                                            {}\n",
      "call_function  view_4                                                            aten.view.default                                                 (transpose_6, [1, -1, 512])                                                                                                                                     {}\n",
      "call_function  _to_copy                                                          aten._to_copy.default                                             (view_4,)                                                                                                                                                       {'dtype': torch.float32}\n",
      "call_function  linear_3                                                          aten.linear.default                                               (_to_copy, p_encoder_mid_block_attentions_0_to_out_0_weight, p_encoder_mid_block_attentions_0_to_out_0_bias)                                                    {}\n",
      "call_function  dropout_9                                                         aten.dropout.default                                              (linear_3, 0.0, False)                                                                                                                                          {}\n",
      "call_function  transpose_7                                                       aten.transpose.int                                                (dropout_9, -1, -2)                                                                                                                                             {}\n",
      "call_function  view_5                                                            aten.view.default                                                 (transpose_7, [1, 512, 64, 64])                                                                                                                                 {}\n",
      "call_function  add_9                                                             aten.add.Tensor                                                   (view_5, div_8)                                                                                                                                                 {}\n",
      "call_function  div_9                                                             aten.div.Tensor                                                   (add_9, 1)                                                                                                                                                      {}\n",
      "call_function  group_norm_19                                                     aten.group_norm.default                                           (div_9, 32, p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias, 1e-06)            {}\n",
      "call_function  silu_18                                                           aten.silu.default                                                 (group_norm_19,)                                                                                                                                                {}\n",
      "call_function  conv2d_24                                                         aten.conv2d.default                                               (silu_18, p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1])     {}\n",
      "call_function  group_norm_20                                                     aten.group_norm.default                                           (conv2d_24, 32, p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias, 1e-06)        {}\n",
      "call_function  silu_19                                                           aten.silu.default                                                 (group_norm_20,)                                                                                                                                                {}\n",
      "call_function  dropout_10                                                        aten.dropout.default                                              (silu_19, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_25                                                         aten.conv2d.default                                               (dropout_10, p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1])  {}\n",
      "call_function  add_10                                                            aten.add.Tensor                                                   (div_9, conv2d_25)                                                                                                                                              {}\n",
      "call_function  div_10                                                            aten.div.Tensor                                                   (add_10, 1)                                                                                                                                                     {}\n",
      "call_function  group_norm_21                                                     aten.group_norm.default                                           (div_10, 32, p_encoder_conv_norm_out_weight, p_encoder_conv_norm_out_bias, 1e-06)                                                                               {}\n",
      "call_function  silu_20                                                           aten.silu.default                                                 (group_norm_21,)                                                                                                                                                {}\n",
      "call_function  conv2d_26                                                         aten.conv2d.default                                               (silu_20, p_encoder_conv_out_weight, p_encoder_conv_out_bias, [1, 1], [1, 1])                                                                                   {}\n",
      "call_function  conv2d_27                                                         aten.conv2d.default                                               (conv2d_26, p_quant_conv_weight, p_quant_conv_bias)                                                                                                             {}\n",
      "call_function  split                                                             aten.split.Tensor                                                 (conv2d_27, 4, 1)                                                                                                                                               {}\n",
      "call_function  getitem                                                           <built-in function getitem>                                       (split, 0)                                                                                                                                                      {}\n",
      "call_function  conv2d_28                                                         aten.conv2d.default                                               (getitem, p_post_quant_conv_weight, p_post_quant_conv_bias)                                                                                                     {}\n",
      "call_function  conv2d_29                                                         aten.conv2d.default                                               (conv2d_28, p_decoder_conv_in_weight, p_decoder_conv_in_bias, [1, 1], [1, 1])                                                                                   {}\n",
      "call_function  group_norm_22                                                     aten.group_norm.default                                           (conv2d_29, 32, p_decoder_mid_block_resnets_0_norm1_weight, p_decoder_mid_block_resnets_0_norm1_bias, 1e-06)                                                    {}\n",
      "call_function  silu_21                                                           aten.silu.default                                                 (group_norm_22,)                                                                                                                                                {}\n",
      "call_function  conv2d_30                                                         aten.conv2d.default                                               (silu_21, p_decoder_mid_block_resnets_0_conv1_weight, p_decoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1])                                                 {}\n",
      "call_function  group_norm_23                                                     aten.group_norm.default                                           (conv2d_30, 32, p_decoder_mid_block_resnets_0_norm2_weight, p_decoder_mid_block_resnets_0_norm2_bias, 1e-06)                                                    {}\n",
      "call_function  silu_22                                                           aten.silu.default                                                 (group_norm_23,)                                                                                                                                                {}\n",
      "call_function  dropout_11                                                        aten.dropout.default                                              (silu_22, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_31                                                         aten.conv2d.default                                               (dropout_11, p_decoder_mid_block_resnets_0_conv2_weight, p_decoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1])                                              {}\n",
      "call_function  add_11                                                            aten.add.Tensor                                                   (conv2d_29, conv2d_31)                                                                                                                                          {}\n",
      "call_function  div_11                                                            aten.div.Tensor                                                   (add_11, 1)                                                                                                                                                     {}\n",
      "call_function  view_6                                                            aten.view.default                                                 (div_11, [1, 512, 4096])                                                                                                                                        {}\n",
      "call_function  transpose_8                                                       aten.transpose.int                                                (view_6, 1, 2)                                                                                                                                                  {}\n",
      "call_function  transpose_9                                                       aten.transpose.int                                                (transpose_8, 1, 2)                                                                                                                                             {}\n",
      "call_function  group_norm_24                                                     aten.group_norm.default                                           (transpose_9, 32, p_decoder_mid_block_attentions_0_group_norm_weight, p_decoder_mid_block_attentions_0_group_norm_bias, 1e-06)                                  {}\n",
      "call_function  transpose_10                                                      aten.transpose.int                                                (group_norm_24, 1, 2)                                                                                                                                           {}\n",
      "call_function  linear_4                                                          aten.linear.default                                               (transpose_10, p_decoder_mid_block_attentions_0_to_q_weight, p_decoder_mid_block_attentions_0_to_q_bias)                                                        {}\n",
      "call_function  linear_5                                                          aten.linear.default                                               (transpose_10, p_decoder_mid_block_attentions_0_to_k_weight, p_decoder_mid_block_attentions_0_to_k_bias)                                                        {}\n",
      "call_function  linear_6                                                          aten.linear.default                                               (transpose_10, p_decoder_mid_block_attentions_0_to_v_weight, p_decoder_mid_block_attentions_0_to_v_bias)                                                        {}\n",
      "call_function  view_7                                                            aten.view.default                                                 (linear_4, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_11                                                      aten.transpose.int                                                (view_7, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_8                                                            aten.view.default                                                 (linear_5, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_12                                                      aten.transpose.int                                                (view_8, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_9                                                            aten.view.default                                                 (linear_6, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_13                                                      aten.transpose.int                                                (view_9, 1, 2)                                                                                                                                                  {}\n",
      "call_function  scaled_dot_product_attention_1                                    aten.scaled_dot_product_attention.default                         (transpose_11, transpose_12, transpose_13)                                                                                                                      {}\n",
      "call_function  transpose_14                                                      aten.transpose.int                                                (scaled_dot_product_attention_1, 1, 2)                                                                                                                          {}\n",
      "call_function  view_10                                                           aten.view.default                                                 (transpose_14, [1, -1, 512])                                                                                                                                    {}\n",
      "call_function  _to_copy_1                                                        aten._to_copy.default                                             (view_10,)                                                                                                                                                      {'dtype': torch.float32}\n",
      "call_function  linear_7                                                          aten.linear.default                                               (_to_copy_1, p_decoder_mid_block_attentions_0_to_out_0_weight, p_decoder_mid_block_attentions_0_to_out_0_bias)                                                  {}\n",
      "call_function  dropout_12                                                        aten.dropout.default                                              (linear_7, 0.0, False)                                                                                                                                          {}\n",
      "call_function  transpose_15                                                      aten.transpose.int                                                (dropout_12, -1, -2)                                                                                                                                            {}\n",
      "call_function  view_11                                                           aten.view.default                                                 (transpose_15, [1, 512, 64, 64])                                                                                                                                {}\n",
      "call_function  add_12                                                            aten.add.Tensor                                                   (view_11, div_11)                                                                                                                                               {}\n",
      "call_function  div_12                                                            aten.div.Tensor                                                   (add_12, 1)                                                                                                                                                     {}\n",
      "call_function  group_norm_25                                                     aten.group_norm.default                                           (div_12, 32, p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias, 1e-06)           {}\n",
      "call_function  silu_23                                                           aten.silu.default                                                 (group_norm_25,)                                                                                                                                                {}\n",
      "call_function  conv2d_32                                                         aten.conv2d.default                                               (silu_23, p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1])     {}\n",
      "call_function  group_norm_26                                                     aten.group_norm.default                                           (conv2d_32, 32, p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias, 1e-06)        {}\n",
      "call_function  silu_24                                                           aten.silu.default                                                 (group_norm_26,)                                                                                                                                                {}\n",
      "call_function  dropout_13                                                        aten.dropout.default                                              (silu_24, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_33                                                         aten.conv2d.default                                               (dropout_13, p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1])  {}\n",
      "call_function  add_13                                                            aten.add.Tensor                                                   (div_12, conv2d_33)                                                                                                                                             {}\n",
      "call_function  div_13                                                            aten.div.Tensor                                                   (add_13, 1)                                                                                                                                                     {}\n",
      "call_function  _to_copy_2                                                        aten._to_copy.default                                             (div_13,)                                                                                                                                                       {'dtype': torch.float32}\n",
      "call_function  group_norm_27                                                     aten.group_norm.default                                           (_to_copy_2, 32, p_decoder_up_blocks_0_resnets_0_norm1_weight, p_decoder_up_blocks_0_resnets_0_norm1_bias, 1e-06)                                               {}\n",
      "call_function  silu_25                                                           aten.silu.default                                                 (group_norm_27,)                                                                                                                                                {}\n",
      "call_function  conv2d_34                                                         aten.conv2d.default                                               (silu_25, p_decoder_up_blocks_0_resnets_0_conv1_weight, p_decoder_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_28                                                     aten.group_norm.default                                           (conv2d_34, 32, p_decoder_up_blocks_0_resnets_0_norm2_weight, p_decoder_up_blocks_0_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_26                                                           aten.silu.default                                                 (group_norm_28,)                                                                                                                                                {}\n",
      "call_function  dropout_14                                                        aten.dropout.default                                              (silu_26, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_35                                                         aten.conv2d.default                                               (dropout_14, p_decoder_up_blocks_0_resnets_0_conv2_weight, p_decoder_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_14                                                            aten.add.Tensor                                                   (_to_copy_2, conv2d_35)                                                                                                                                         {}\n",
      "call_function  div_14                                                            aten.div.Tensor                                                   (add_14, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_29                                                     aten.group_norm.default                                           (div_14, 32, p_decoder_up_blocks_0_resnets_1_norm1_weight, p_decoder_up_blocks_0_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_27                                                           aten.silu.default                                                 (group_norm_29,)                                                                                                                                                {}\n",
      "call_function  conv2d_36                                                         aten.conv2d.default                                               (silu_27, p_decoder_up_blocks_0_resnets_1_conv1_weight, p_decoder_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_30                                                     aten.group_norm.default                                           (conv2d_36, 32, p_decoder_up_blocks_0_resnets_1_norm2_weight, p_decoder_up_blocks_0_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_28                                                           aten.silu.default                                                 (group_norm_30,)                                                                                                                                                {}\n",
      "call_function  dropout_15                                                        aten.dropout.default                                              (silu_28, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_37                                                         aten.conv2d.default                                               (dropout_15, p_decoder_up_blocks_0_resnets_1_conv2_weight, p_decoder_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_15                                                            aten.add.Tensor                                                   (div_14, conv2d_37)                                                                                                                                             {}\n",
      "call_function  div_15                                                            aten.div.Tensor                                                   (add_15, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_31                                                     aten.group_norm.default                                           (div_15, 32, p_decoder_up_blocks_0_resnets_2_norm1_weight, p_decoder_up_blocks_0_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_29                                                           aten.silu.default                                                 (group_norm_31,)                                                                                                                                                {}\n",
      "call_function  conv2d_38                                                         aten.conv2d.default                                               (silu_29, p_decoder_up_blocks_0_resnets_2_conv1_weight, p_decoder_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_32                                                     aten.group_norm.default                                           (conv2d_38, 32, p_decoder_up_blocks_0_resnets_2_norm2_weight, p_decoder_up_blocks_0_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_30                                                           aten.silu.default                                                 (group_norm_32,)                                                                                                                                                {}\n",
      "call_function  dropout_16                                                        aten.dropout.default                                              (silu_30, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_39                                                         aten.conv2d.default                                               (dropout_16, p_decoder_up_blocks_0_resnets_2_conv2_weight, p_decoder_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_16                                                            aten.add.Tensor                                                   (div_15, conv2d_39)                                                                                                                                             {}\n",
      "call_function  div_16                                                            aten.div.Tensor                                                   (add_16, 1.0)                                                                                                                                                   {}\n",
      "call_function  upsample_nearest2d                                                aten.upsample_nearest2d.vec                                       (div_16, None, [2.0, 2.0])                                                                                                                                      {}\n",
      "call_function  conv2d_40                                                         aten.conv2d.default                                               (upsample_nearest2d, p_decoder_up_blocks_0_upsamplers_0_conv_weight, p_decoder_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1])                              {}\n",
      "call_function  group_norm_33                                                     aten.group_norm.default                                           (conv2d_40, 32, p_decoder_up_blocks_1_resnets_0_norm1_weight, p_decoder_up_blocks_1_resnets_0_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_31                                                           aten.silu.default                                                 (group_norm_33,)                                                                                                                                                {}\n",
      "call_function  conv2d_41                                                         aten.conv2d.default                                               (silu_31, p_decoder_up_blocks_1_resnets_0_conv1_weight, p_decoder_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_34                                                     aten.group_norm.default                                           (conv2d_41, 32, p_decoder_up_blocks_1_resnets_0_norm2_weight, p_decoder_up_blocks_1_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_32                                                           aten.silu.default                                                 (group_norm_34,)                                                                                                                                                {}\n",
      "call_function  dropout_17                                                        aten.dropout.default                                              (silu_32, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_42                                                         aten.conv2d.default                                               (dropout_17, p_decoder_up_blocks_1_resnets_0_conv2_weight, p_decoder_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_17                                                            aten.add.Tensor                                                   (conv2d_40, conv2d_42)                                                                                                                                          {}\n",
      "call_function  div_17                                                            aten.div.Tensor                                                   (add_17, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_35                                                     aten.group_norm.default                                           (div_17, 32, p_decoder_up_blocks_1_resnets_1_norm1_weight, p_decoder_up_blocks_1_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_33                                                           aten.silu.default                                                 (group_norm_35,)                                                                                                                                                {}\n",
      "call_function  conv2d_43                                                         aten.conv2d.default                                               (silu_33, p_decoder_up_blocks_1_resnets_1_conv1_weight, p_decoder_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_36                                                     aten.group_norm.default                                           (conv2d_43, 32, p_decoder_up_blocks_1_resnets_1_norm2_weight, p_decoder_up_blocks_1_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_34                                                           aten.silu.default                                                 (group_norm_36,)                                                                                                                                                {}\n",
      "call_function  dropout_18                                                        aten.dropout.default                                              (silu_34, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_44                                                         aten.conv2d.default                                               (dropout_18, p_decoder_up_blocks_1_resnets_1_conv2_weight, p_decoder_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_18                                                            aten.add.Tensor                                                   (div_17, conv2d_44)                                                                                                                                             {}\n",
      "call_function  div_18                                                            aten.div.Tensor                                                   (add_18, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_37                                                     aten.group_norm.default                                           (div_18, 32, p_decoder_up_blocks_1_resnets_2_norm1_weight, p_decoder_up_blocks_1_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_35                                                           aten.silu.default                                                 (group_norm_37,)                                                                                                                                                {}\n",
      "call_function  conv2d_45                                                         aten.conv2d.default                                               (silu_35, p_decoder_up_blocks_1_resnets_2_conv1_weight, p_decoder_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_38                                                     aten.group_norm.default                                           (conv2d_45, 32, p_decoder_up_blocks_1_resnets_2_norm2_weight, p_decoder_up_blocks_1_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_36                                                           aten.silu.default                                                 (group_norm_38,)                                                                                                                                                {}\n",
      "call_function  dropout_19                                                        aten.dropout.default                                              (silu_36, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_46                                                         aten.conv2d.default                                               (dropout_19, p_decoder_up_blocks_1_resnets_2_conv2_weight, p_decoder_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_19                                                            aten.add.Tensor                                                   (div_18, conv2d_46)                                                                                                                                             {}\n",
      "call_function  div_19                                                            aten.div.Tensor                                                   (add_19, 1.0)                                                                                                                                                   {}\n",
      "call_function  upsample_nearest2d_1                                              aten.upsample_nearest2d.vec                                       (div_19, None, [2.0, 2.0])                                                                                                                                      {}\n",
      "call_function  conv2d_47                                                         aten.conv2d.default                                               (upsample_nearest2d_1, p_decoder_up_blocks_1_upsamplers_0_conv_weight, p_decoder_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1])                            {}\n",
      "call_function  group_norm_39                                                     aten.group_norm.default                                           (conv2d_47, 32, p_decoder_up_blocks_2_resnets_0_norm1_weight, p_decoder_up_blocks_2_resnets_0_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_37                                                           aten.silu.default                                                 (group_norm_39,)                                                                                                                                                {}\n",
      "call_function  conv2d_48                                                         aten.conv2d.default                                               (silu_37, p_decoder_up_blocks_2_resnets_0_conv1_weight, p_decoder_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_40                                                     aten.group_norm.default                                           (conv2d_48, 32, p_decoder_up_blocks_2_resnets_0_norm2_weight, p_decoder_up_blocks_2_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_38                                                           aten.silu.default                                                 (group_norm_40,)                                                                                                                                                {}\n",
      "call_function  dropout_20                                                        aten.dropout.default                                              (silu_38, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_49                                                         aten.conv2d.default                                               (dropout_20, p_decoder_up_blocks_2_resnets_0_conv2_weight, p_decoder_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  clone                                                             aten.clone.default                                                (conv2d_47,)                                                                                                                                                    {'memory_format': torch.contiguous_format}\n",
      "call_function  conv2d_50                                                         aten.conv2d.default                                               (clone, p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight, p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias)                                               {}\n",
      "call_function  add_20                                                            aten.add.Tensor                                                   (conv2d_50, conv2d_49)                                                                                                                                          {}\n",
      "call_function  div_20                                                            aten.div.Tensor                                                   (add_20, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_41                                                     aten.group_norm.default                                           (div_20, 32, p_decoder_up_blocks_2_resnets_1_norm1_weight, p_decoder_up_blocks_2_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_39                                                           aten.silu.default                                                 (group_norm_41,)                                                                                                                                                {}\n",
      "call_function  conv2d_51                                                         aten.conv2d.default                                               (silu_39, p_decoder_up_blocks_2_resnets_1_conv1_weight, p_decoder_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_42                                                     aten.group_norm.default                                           (conv2d_51, 32, p_decoder_up_blocks_2_resnets_1_norm2_weight, p_decoder_up_blocks_2_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_40                                                           aten.silu.default                                                 (group_norm_42,)                                                                                                                                                {}\n",
      "call_function  dropout_21                                                        aten.dropout.default                                              (silu_40, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_52                                                         aten.conv2d.default                                               (dropout_21, p_decoder_up_blocks_2_resnets_1_conv2_weight, p_decoder_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_21                                                            aten.add.Tensor                                                   (div_20, conv2d_52)                                                                                                                                             {}\n",
      "call_function  div_21                                                            aten.div.Tensor                                                   (add_21, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_43                                                     aten.group_norm.default                                           (div_21, 32, p_decoder_up_blocks_2_resnets_2_norm1_weight, p_decoder_up_blocks_2_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_41                                                           aten.silu.default                                                 (group_norm_43,)                                                                                                                                                {}\n",
      "call_function  conv2d_53                                                         aten.conv2d.default                                               (silu_41, p_decoder_up_blocks_2_resnets_2_conv1_weight, p_decoder_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_44                                                     aten.group_norm.default                                           (conv2d_53, 32, p_decoder_up_blocks_2_resnets_2_norm2_weight, p_decoder_up_blocks_2_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_42                                                           aten.silu.default                                                 (group_norm_44,)                                                                                                                                                {}\n",
      "call_function  dropout_22                                                        aten.dropout.default                                              (silu_42, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_54                                                         aten.conv2d.default                                               (dropout_22, p_decoder_up_blocks_2_resnets_2_conv2_weight, p_decoder_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_22                                                            aten.add.Tensor                                                   (div_21, conv2d_54)                                                                                                                                             {}\n",
      "call_function  div_22                                                            aten.div.Tensor                                                   (add_22, 1.0)                                                                                                                                                   {}\n",
      "call_function  upsample_nearest2d_2                                              aten.upsample_nearest2d.vec                                       (div_22, None, [2.0, 2.0])                                                                                                                                      {}\n",
      "call_function  conv2d_55                                                         aten.conv2d.default                                               (upsample_nearest2d_2, p_decoder_up_blocks_2_upsamplers_0_conv_weight, p_decoder_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1])                            {}\n",
      "call_function  group_norm_45                                                     aten.group_norm.default                                           (conv2d_55, 32, p_decoder_up_blocks_3_resnets_0_norm1_weight, p_decoder_up_blocks_3_resnets_0_norm1_bias, 1e-06)                                                {}\n",
      "call_function  silu_43                                                           aten.silu.default                                                 (group_norm_45,)                                                                                                                                                {}\n",
      "call_function  conv2d_56                                                         aten.conv2d.default                                               (silu_43, p_decoder_up_blocks_3_resnets_0_conv1_weight, p_decoder_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_46                                                     aten.group_norm.default                                           (conv2d_56, 32, p_decoder_up_blocks_3_resnets_0_norm2_weight, p_decoder_up_blocks_3_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_44                                                           aten.silu.default                                                 (group_norm_46,)                                                                                                                                                {}\n",
      "call_function  dropout_23                                                        aten.dropout.default                                              (silu_44, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_57                                                         aten.conv2d.default                                               (dropout_23, p_decoder_up_blocks_3_resnets_0_conv2_weight, p_decoder_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  conv2d_58                                                         aten.conv2d.default                                               (conv2d_55, p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight, p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias)                                           {}\n",
      "call_function  add_23                                                            aten.add.Tensor                                                   (conv2d_58, conv2d_57)                                                                                                                                          {}\n",
      "call_function  div_23                                                            aten.div.Tensor                                                   (add_23, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_47                                                     aten.group_norm.default                                           (div_23, 32, p_decoder_up_blocks_3_resnets_1_norm1_weight, p_decoder_up_blocks_3_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_45                                                           aten.silu.default                                                 (group_norm_47,)                                                                                                                                                {}\n",
      "call_function  conv2d_59                                                         aten.conv2d.default                                               (silu_45, p_decoder_up_blocks_3_resnets_1_conv1_weight, p_decoder_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_48                                                     aten.group_norm.default                                           (conv2d_59, 32, p_decoder_up_blocks_3_resnets_1_norm2_weight, p_decoder_up_blocks_3_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_46                                                           aten.silu.default                                                 (group_norm_48,)                                                                                                                                                {}\n",
      "call_function  dropout_24                                                        aten.dropout.default                                              (silu_46, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_60                                                         aten.conv2d.default                                               (dropout_24, p_decoder_up_blocks_3_resnets_1_conv2_weight, p_decoder_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_24                                                            aten.add.Tensor                                                   (div_23, conv2d_60)                                                                                                                                             {}\n",
      "call_function  div_24                                                            aten.div.Tensor                                                   (add_24, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_49                                                     aten.group_norm.default                                           (div_24, 32, p_decoder_up_blocks_3_resnets_2_norm1_weight, p_decoder_up_blocks_3_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  silu_47                                                           aten.silu.default                                                 (group_norm_49,)                                                                                                                                                {}\n",
      "call_function  conv2d_61                                                         aten.conv2d.default                                               (silu_47, p_decoder_up_blocks_3_resnets_2_conv1_weight, p_decoder_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_50                                                     aten.group_norm.default                                           (conv2d_61, 32, p_decoder_up_blocks_3_resnets_2_norm2_weight, p_decoder_up_blocks_3_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  silu_48                                                           aten.silu.default                                                 (group_norm_50,)                                                                                                                                                {}\n",
      "call_function  dropout_25                                                        aten.dropout.default                                              (silu_48, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_62                                                         aten.conv2d.default                                               (dropout_25, p_decoder_up_blocks_3_resnets_2_conv2_weight, p_decoder_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_25                                                            aten.add.Tensor                                                   (div_24, conv2d_62)                                                                                                                                             {}\n",
      "call_function  div_25                                                            aten.div.Tensor                                                   (add_25, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_51                                                     aten.group_norm.default                                           (div_25, 32, p_decoder_conv_norm_out_weight, p_decoder_conv_norm_out_bias, 1e-06)                                                                               {}\n",
      "call_function  silu_49                                                           aten.silu.default                                                 (group_norm_51,)                                                                                                                                                {}\n",
      "call_function  conv2d_63                                                         aten.conv2d.default                                               (silu_49, p_decoder_conv_out_weight, p_decoder_conv_out_bias, [1, 1], [1, 1])                                                                                   {}\n",
      "output         output                                                            output                                                            ((conv2d_63,),)                                                                                                                                                 {}\n",
      "graph():\n",
      "    %p_encoder_conv_in_weight : [num_users=1] = placeholder[target=p_encoder_conv_in_weight]\n",
      "    %p_encoder_conv_in_bias : [num_users=1] = placeholder[target=p_encoder_conv_in_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_norm1_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_norm1_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_conv1_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_conv1_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_norm2_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_norm2_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_conv2_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_0_conv2_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_norm1_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_norm1_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_conv1_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_conv1_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_norm2_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_norm2_bias]\n",
      "    %p_encoder_down_blocks_0_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_conv2_weight]\n",
      "    %p_encoder_down_blocks_0_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_resnets_1_conv2_bias]\n",
      "    %p_encoder_down_blocks_0_downsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_downsamplers_0_conv_weight]\n",
      "    %p_encoder_down_blocks_0_downsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_0_downsamplers_0_conv_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_norm1_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_norm1_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv1_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv1_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_norm2_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_norm2_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv2_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv2_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_norm1_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_norm1_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_conv1_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_conv1_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_norm2_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_norm2_bias]\n",
      "    %p_encoder_down_blocks_1_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_conv2_weight]\n",
      "    %p_encoder_down_blocks_1_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_resnets_1_conv2_bias]\n",
      "    %p_encoder_down_blocks_1_downsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_downsamplers_0_conv_weight]\n",
      "    %p_encoder_down_blocks_1_downsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_1_downsamplers_0_conv_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_norm1_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_norm1_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv1_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv1_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_norm2_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_norm2_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv2_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv2_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_norm1_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_norm1_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_conv1_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_conv1_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_norm2_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_norm2_bias]\n",
      "    %p_encoder_down_blocks_2_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_conv2_weight]\n",
      "    %p_encoder_down_blocks_2_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_resnets_1_conv2_bias]\n",
      "    %p_encoder_down_blocks_2_downsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_downsamplers_0_conv_weight]\n",
      "    %p_encoder_down_blocks_2_downsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_2_downsamplers_0_conv_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_norm1_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_norm1_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_conv1_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_conv1_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_norm2_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_norm2_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_conv2_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_0_conv2_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_norm1_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_norm1_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_conv1_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_conv1_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_norm2_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_norm2_bias]\n",
      "    %p_encoder_down_blocks_3_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_conv2_weight]\n",
      "    %p_encoder_down_blocks_3_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_encoder_down_blocks_3_resnets_1_conv2_bias]\n",
      "    %p_encoder_mid_block_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_norm1_weight]\n",
      "    %p_encoder_mid_block_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_norm1_bias]\n",
      "    %p_encoder_mid_block_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_conv1_weight]\n",
      "    %p_encoder_mid_block_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_conv1_bias]\n",
      "    %p_encoder_mid_block_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_norm2_weight]\n",
      "    %p_encoder_mid_block_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_norm2_bias]\n",
      "    %p_encoder_mid_block_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_conv2_weight]\n",
      "    %p_encoder_mid_block_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_0_conv2_bias]\n",
      "    %p_encoder_mid_block_attentions_0_group_norm_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_group_norm_weight]\n",
      "    %p_encoder_mid_block_attentions_0_group_norm_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_group_norm_bias]\n",
      "    %p_encoder_mid_block_attentions_0_to_q_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_q_weight]\n",
      "    %p_encoder_mid_block_attentions_0_to_q_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_q_bias]\n",
      "    %p_encoder_mid_block_attentions_0_to_k_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_k_weight]\n",
      "    %p_encoder_mid_block_attentions_0_to_k_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_k_bias]\n",
      "    %p_encoder_mid_block_attentions_0_to_v_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_v_weight]\n",
      "    %p_encoder_mid_block_attentions_0_to_v_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_v_bias]\n",
      "    %p_encoder_mid_block_attentions_0_to_out_0_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_out_0_weight]\n",
      "    %p_encoder_mid_block_attentions_0_to_out_0_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_attentions_0_to_out_0_bias]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight]\n",
      "    %p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias : [num_users=1] = placeholder[target=p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias]\n",
      "    %p_encoder_conv_norm_out_weight : [num_users=1] = placeholder[target=p_encoder_conv_norm_out_weight]\n",
      "    %p_encoder_conv_norm_out_bias : [num_users=1] = placeholder[target=p_encoder_conv_norm_out_bias]\n",
      "    %p_encoder_conv_out_weight : [num_users=1] = placeholder[target=p_encoder_conv_out_weight]\n",
      "    %p_encoder_conv_out_bias : [num_users=1] = placeholder[target=p_encoder_conv_out_bias]\n",
      "    %p_quant_conv_weight : [num_users=1] = placeholder[target=p_quant_conv_weight]\n",
      "    %p_quant_conv_bias : [num_users=1] = placeholder[target=p_quant_conv_bias]\n",
      "    %p_post_quant_conv_weight : [num_users=1] = placeholder[target=p_post_quant_conv_weight]\n",
      "    %p_post_quant_conv_bias : [num_users=1] = placeholder[target=p_post_quant_conv_bias]\n",
      "    %p_decoder_conv_in_weight : [num_users=1] = placeholder[target=p_decoder_conv_in_weight]\n",
      "    %p_decoder_conv_in_bias : [num_users=1] = placeholder[target=p_decoder_conv_in_bias]\n",
      "    %p_decoder_mid_block_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_norm1_weight]\n",
      "    %p_decoder_mid_block_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_norm1_bias]\n",
      "    %p_decoder_mid_block_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_conv1_weight]\n",
      "    %p_decoder_mid_block_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_conv1_bias]\n",
      "    %p_decoder_mid_block_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_norm2_weight]\n",
      "    %p_decoder_mid_block_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_norm2_bias]\n",
      "    %p_decoder_mid_block_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_conv2_weight]\n",
      "    %p_decoder_mid_block_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_0_conv2_bias]\n",
      "    %p_decoder_mid_block_attentions_0_group_norm_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_group_norm_weight]\n",
      "    %p_decoder_mid_block_attentions_0_group_norm_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_group_norm_bias]\n",
      "    %p_decoder_mid_block_attentions_0_to_q_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_q_weight]\n",
      "    %p_decoder_mid_block_attentions_0_to_q_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_q_bias]\n",
      "    %p_decoder_mid_block_attentions_0_to_k_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_k_weight]\n",
      "    %p_decoder_mid_block_attentions_0_to_k_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_k_bias]\n",
      "    %p_decoder_mid_block_attentions_0_to_v_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_v_weight]\n",
      "    %p_decoder_mid_block_attentions_0_to_v_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_v_bias]\n",
      "    %p_decoder_mid_block_attentions_0_to_out_0_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_out_0_weight]\n",
      "    %p_decoder_mid_block_attentions_0_to_out_0_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_attentions_0_to_out_0_bias]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight]\n",
      "    %p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_norm1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_norm1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_conv1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_conv1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_norm2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_norm2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_conv2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_0_conv2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_norm1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_norm1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_conv1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_conv1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_norm2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_norm2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_conv2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_1_conv2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_norm1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_norm1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_conv1_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_conv1_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_norm2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_norm2_bias]\n",
      "    %p_decoder_up_blocks_0_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_conv2_weight]\n",
      "    %p_decoder_up_blocks_0_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_resnets_2_conv2_bias]\n",
      "    %p_decoder_up_blocks_0_upsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_upsamplers_0_conv_weight]\n",
      "    %p_decoder_up_blocks_0_upsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_0_upsamplers_0_conv_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_norm1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_norm1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_conv1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_conv1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_norm2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_norm2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_conv2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_0_conv2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_norm1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_norm1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_conv1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_conv1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_norm2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_norm2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_conv2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_1_conv2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_norm1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_norm1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_conv1_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_conv1_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_norm2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_norm2_bias]\n",
      "    %p_decoder_up_blocks_1_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_conv2_weight]\n",
      "    %p_decoder_up_blocks_1_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_resnets_2_conv2_bias]\n",
      "    %p_decoder_up_blocks_1_upsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_upsamplers_0_conv_weight]\n",
      "    %p_decoder_up_blocks_1_upsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_1_upsamplers_0_conv_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_norm1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_norm1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_norm2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_norm2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_norm1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_norm1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_conv1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_conv1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_norm2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_norm2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_conv2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_1_conv2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_norm1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_norm1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_conv1_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_conv1_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_norm2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_norm2_bias]\n",
      "    %p_decoder_up_blocks_2_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_conv2_weight]\n",
      "    %p_decoder_up_blocks_2_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_resnets_2_conv2_bias]\n",
      "    %p_decoder_up_blocks_2_upsamplers_0_conv_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_upsamplers_0_conv_weight]\n",
      "    %p_decoder_up_blocks_2_upsamplers_0_conv_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_2_upsamplers_0_conv_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_norm1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_norm1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_norm2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_norm2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_1_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_norm1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_1_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_norm1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_1_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_conv1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_1_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_conv1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_1_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_norm2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_1_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_norm2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_1_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_conv2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_1_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_1_conv2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_2_norm1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_norm1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_2_norm1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_norm1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_2_conv1_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_conv1_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_2_conv1_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_conv1_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_2_norm2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_norm2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_2_norm2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_norm2_bias]\n",
      "    %p_decoder_up_blocks_3_resnets_2_conv2_weight : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_conv2_weight]\n",
      "    %p_decoder_up_blocks_3_resnets_2_conv2_bias : [num_users=1] = placeholder[target=p_decoder_up_blocks_3_resnets_2_conv2_bias]\n",
      "    %p_decoder_conv_norm_out_weight : [num_users=1] = placeholder[target=p_decoder_conv_norm_out_weight]\n",
      "    %p_decoder_conv_norm_out_bias : [num_users=1] = placeholder[target=p_decoder_conv_norm_out_bias]\n",
      "    %p_decoder_conv_out_weight : [num_users=1] = placeholder[target=p_decoder_conv_out_weight]\n",
      "    %p_decoder_conv_out_bias : [num_users=1] = placeholder[target=p_decoder_conv_out_bias]\n",
      "    %sample : [num_users=1] = placeholder[target=sample]\n",
      "    %conv2d : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%sample, %p_encoder_conv_in_weight, %p_encoder_conv_in_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d, 32, %p_encoder_down_blocks_0_resnets_0_norm1_weight, %p_encoder_down_blocks_0_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm,), kwargs = {})\n",
      "    %conv2d_1 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu, %p_encoder_down_blocks_0_resnets_0_conv1_weight, %p_encoder_down_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_1 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_1, 32, %p_encoder_down_blocks_0_resnets_0_norm2_weight, %p_encoder_down_blocks_0_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_1 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_1,), kwargs = {})\n",
      "    %dropout : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_1, 0.0, False), kwargs = {})\n",
      "    %conv2d_2 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout, %p_encoder_down_blocks_0_resnets_0_conv2_weight, %p_encoder_down_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d, %conv2d_2), kwargs = {})\n",
      "    %div : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add, 1.0), kwargs = {})\n",
      "    %group_norm_2 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div, 32, %p_encoder_down_blocks_0_resnets_1_norm1_weight, %p_encoder_down_blocks_0_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_2 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_2,), kwargs = {})\n",
      "    %conv2d_3 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_2, %p_encoder_down_blocks_0_resnets_1_conv1_weight, %p_encoder_down_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_3 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_3, 32, %p_encoder_down_blocks_0_resnets_1_norm2_weight, %p_encoder_down_blocks_0_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_3 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_3,), kwargs = {})\n",
      "    %dropout_1 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_3, 0.0, False), kwargs = {})\n",
      "    %conv2d_4 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_1, %p_encoder_down_blocks_0_resnets_1_conv2_weight, %p_encoder_down_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div, %conv2d_4), kwargs = {})\n",
      "    %div_1 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_1, 1.0), kwargs = {})\n",
      "    %pad : [num_users=1] = call_function[target=torch.ops.aten.pad.default](args = (%div_1, [0, 1, 0, 1], constant, 0.0), kwargs = {})\n",
      "    %conv2d_5 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%pad, %p_encoder_down_blocks_0_downsamplers_0_conv_weight, %p_encoder_down_blocks_0_downsamplers_0_conv_bias, [2, 2]), kwargs = {})\n",
      "    %group_norm_4 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_5, 32, %p_encoder_down_blocks_1_resnets_0_norm1_weight, %p_encoder_down_blocks_1_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_4 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_4,), kwargs = {})\n",
      "    %conv2d_6 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_4, %p_encoder_down_blocks_1_resnets_0_conv1_weight, %p_encoder_down_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_5 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_6, 32, %p_encoder_down_blocks_1_resnets_0_norm2_weight, %p_encoder_down_blocks_1_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_5 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_5,), kwargs = {})\n",
      "    %dropout_2 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_5, 0.0, False), kwargs = {})\n",
      "    %conv2d_7 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_2, %p_encoder_down_blocks_1_resnets_0_conv2_weight, %p_encoder_down_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_8 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_5, %p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight, %p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_8, %conv2d_7), kwargs = {})\n",
      "    %div_2 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_2, 1.0), kwargs = {})\n",
      "    %group_norm_6 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_2, 32, %p_encoder_down_blocks_1_resnets_1_norm1_weight, %p_encoder_down_blocks_1_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_6 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_6,), kwargs = {})\n",
      "    %conv2d_9 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_6, %p_encoder_down_blocks_1_resnets_1_conv1_weight, %p_encoder_down_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_7 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_9, 32, %p_encoder_down_blocks_1_resnets_1_norm2_weight, %p_encoder_down_blocks_1_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_7 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_7,), kwargs = {})\n",
      "    %dropout_3 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_7, 0.0, False), kwargs = {})\n",
      "    %conv2d_10 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_3, %p_encoder_down_blocks_1_resnets_1_conv2_weight, %p_encoder_down_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_3 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_2, %conv2d_10), kwargs = {})\n",
      "    %div_3 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_3, 1.0), kwargs = {})\n",
      "    %pad_1 : [num_users=1] = call_function[target=torch.ops.aten.pad.default](args = (%div_3, [0, 1, 0, 1], constant, 0.0), kwargs = {})\n",
      "    %conv2d_11 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%pad_1, %p_encoder_down_blocks_1_downsamplers_0_conv_weight, %p_encoder_down_blocks_1_downsamplers_0_conv_bias, [2, 2]), kwargs = {})\n",
      "    %group_norm_8 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_11, 32, %p_encoder_down_blocks_2_resnets_0_norm1_weight, %p_encoder_down_blocks_2_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_8 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_8,), kwargs = {})\n",
      "    %conv2d_12 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_8, %p_encoder_down_blocks_2_resnets_0_conv1_weight, %p_encoder_down_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_9 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_12, 32, %p_encoder_down_blocks_2_resnets_0_norm2_weight, %p_encoder_down_blocks_2_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_9 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_9,), kwargs = {})\n",
      "    %dropout_4 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_9, 0.0, False), kwargs = {})\n",
      "    %conv2d_13 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_4, %p_encoder_down_blocks_2_resnets_0_conv2_weight, %p_encoder_down_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_14 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_11, %p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight, %p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_4 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_14, %conv2d_13), kwargs = {})\n",
      "    %div_4 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_4, 1.0), kwargs = {})\n",
      "    %group_norm_10 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_4, 32, %p_encoder_down_blocks_2_resnets_1_norm1_weight, %p_encoder_down_blocks_2_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_10 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_10,), kwargs = {})\n",
      "    %conv2d_15 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_10, %p_encoder_down_blocks_2_resnets_1_conv1_weight, %p_encoder_down_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_11 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_15, 32, %p_encoder_down_blocks_2_resnets_1_norm2_weight, %p_encoder_down_blocks_2_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_11 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_11,), kwargs = {})\n",
      "    %dropout_5 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_11, 0.0, False), kwargs = {})\n",
      "    %conv2d_16 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_5, %p_encoder_down_blocks_2_resnets_1_conv2_weight, %p_encoder_down_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_5 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_4, %conv2d_16), kwargs = {})\n",
      "    %div_5 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_5, 1.0), kwargs = {})\n",
      "    %pad_2 : [num_users=1] = call_function[target=torch.ops.aten.pad.default](args = (%div_5, [0, 1, 0, 1], constant, 0.0), kwargs = {})\n",
      "    %conv2d_17 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%pad_2, %p_encoder_down_blocks_2_downsamplers_0_conv_weight, %p_encoder_down_blocks_2_downsamplers_0_conv_bias, [2, 2]), kwargs = {})\n",
      "    %group_norm_12 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_17, 32, %p_encoder_down_blocks_3_resnets_0_norm1_weight, %p_encoder_down_blocks_3_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_12 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_12,), kwargs = {})\n",
      "    %conv2d_18 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_12, %p_encoder_down_blocks_3_resnets_0_conv1_weight, %p_encoder_down_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_13 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_18, 32, %p_encoder_down_blocks_3_resnets_0_norm2_weight, %p_encoder_down_blocks_3_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_13 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_13,), kwargs = {})\n",
      "    %dropout_6 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_13, 0.0, False), kwargs = {})\n",
      "    %conv2d_19 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_6, %p_encoder_down_blocks_3_resnets_0_conv2_weight, %p_encoder_down_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_6 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_17, %conv2d_19), kwargs = {})\n",
      "    %div_6 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_6, 1.0), kwargs = {})\n",
      "    %group_norm_14 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_6, 32, %p_encoder_down_blocks_3_resnets_1_norm1_weight, %p_encoder_down_blocks_3_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_14 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_14,), kwargs = {})\n",
      "    %conv2d_20 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_14, %p_encoder_down_blocks_3_resnets_1_conv1_weight, %p_encoder_down_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_15 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_20, 32, %p_encoder_down_blocks_3_resnets_1_norm2_weight, %p_encoder_down_blocks_3_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_15 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_15,), kwargs = {})\n",
      "    %dropout_7 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_15, 0.0, False), kwargs = {})\n",
      "    %conv2d_21 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_7, %p_encoder_down_blocks_3_resnets_1_conv2_weight, %p_encoder_down_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_7 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_6, %conv2d_21), kwargs = {})\n",
      "    %div_7 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_7, 1.0), kwargs = {})\n",
      "    %group_norm_16 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_7, 32, %p_encoder_mid_block_resnets_0_norm1_weight, %p_encoder_mid_block_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_16 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_16,), kwargs = {})\n",
      "    %conv2d_22 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_16, %p_encoder_mid_block_resnets_0_conv1_weight, %p_encoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_17 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_22, 32, %p_encoder_mid_block_resnets_0_norm2_weight, %p_encoder_mid_block_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_17 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_17,), kwargs = {})\n",
      "    %dropout_8 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_17, 0.0, False), kwargs = {})\n",
      "    %conv2d_23 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_8, %p_encoder_mid_block_resnets_0_conv2_weight, %p_encoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_8 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_7, %conv2d_23), kwargs = {})\n",
      "    %div_8 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_8, 1), kwargs = {})\n",
      "    %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%div_8, [1, 512, 4096]), kwargs = {})\n",
      "    %transpose : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view, 1, 2), kwargs = {})\n",
      "    %transpose_1 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%transpose, 1, 2), kwargs = {})\n",
      "    %group_norm_18 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%transpose_1, 32, %p_encoder_mid_block_attentions_0_group_norm_weight, %p_encoder_mid_block_attentions_0_group_norm_bias, 1e-06), kwargs = {})\n",
      "    %transpose_2 : [num_users=3] = call_function[target=torch.ops.aten.transpose.int](args = (%group_norm_18, 1, 2), kwargs = {})\n",
      "    %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_2, %p_encoder_mid_block_attentions_0_to_q_weight, %p_encoder_mid_block_attentions_0_to_q_bias), kwargs = {})\n",
      "    %linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_2, %p_encoder_mid_block_attentions_0_to_k_weight, %p_encoder_mid_block_attentions_0_to_k_bias), kwargs = {})\n",
      "    %linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_2, %p_encoder_mid_block_attentions_0_to_v_weight, %p_encoder_mid_block_attentions_0_to_v_bias), kwargs = {})\n",
      "    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_3 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_1, 1, 2), kwargs = {})\n",
      "    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_1, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_4 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_2, 1, 2), kwargs = {})\n",
      "    %view_3 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_2, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_5 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_3, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_3, %transpose_4, %transpose_5), kwargs = {})\n",
      "    %transpose_6 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention, 1, 2), kwargs = {})\n",
      "    %view_4 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_6, [1, -1, 512]), kwargs = {})\n",
      "    %_to_copy : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_4,), kwargs = {dtype: torch.float32})\n",
      "    %linear_3 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy, %p_encoder_mid_block_attentions_0_to_out_0_weight, %p_encoder_mid_block_attentions_0_to_out_0_bias), kwargs = {})\n",
      "    %dropout_9 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_3, 0.0, False), kwargs = {})\n",
      "    %transpose_7 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%dropout_9, -1, -2), kwargs = {})\n",
      "    %view_5 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_7, [1, 512, 64, 64]), kwargs = {})\n",
      "    %add_9 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_5, %div_8), kwargs = {})\n",
      "    %div_9 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_9, 1), kwargs = {})\n",
      "    %group_norm_19 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_9, 32, %p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight, %p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_18 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_19,), kwargs = {})\n",
      "    %conv2d_24 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_18, %p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight, %p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_20 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_24, 32, %p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight, %p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_19 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_20,), kwargs = {})\n",
      "    %dropout_10 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_19, 0.0, False), kwargs = {})\n",
      "    %conv2d_25 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_10, %p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight, %p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_10 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_9, %conv2d_25), kwargs = {})\n",
      "    %div_10 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_10, 1), kwargs = {})\n",
      "    %group_norm_21 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_10, 32, %p_encoder_conv_norm_out_weight, %p_encoder_conv_norm_out_bias, 1e-06), kwargs = {})\n",
      "    %silu_20 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_21,), kwargs = {})\n",
      "    %conv2d_26 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_20, %p_encoder_conv_out_weight, %p_encoder_conv_out_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_27 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_26, %p_quant_conv_weight, %p_quant_conv_bias), kwargs = {})\n",
      "    %split : [num_users=1] = call_function[target=torch.ops.aten.split.Tensor](args = (%conv2d_27, 4, 1), kwargs = {})\n",
      "    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n",
      "    %conv2d_28 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%getitem, %p_post_quant_conv_weight, %p_post_quant_conv_bias), kwargs = {})\n",
      "    %conv2d_29 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_28, %p_decoder_conv_in_weight, %p_decoder_conv_in_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_22 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_29, 32, %p_decoder_mid_block_resnets_0_norm1_weight, %p_decoder_mid_block_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_21 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_22,), kwargs = {})\n",
      "    %conv2d_30 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_21, %p_decoder_mid_block_resnets_0_conv1_weight, %p_decoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_23 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_30, 32, %p_decoder_mid_block_resnets_0_norm2_weight, %p_decoder_mid_block_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_22 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_23,), kwargs = {})\n",
      "    %dropout_11 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_22, 0.0, False), kwargs = {})\n",
      "    %conv2d_31 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_11, %p_decoder_mid_block_resnets_0_conv2_weight, %p_decoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_11 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_29, %conv2d_31), kwargs = {})\n",
      "    %div_11 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_11, 1), kwargs = {})\n",
      "    %view_6 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%div_11, [1, 512, 4096]), kwargs = {})\n",
      "    %transpose_8 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_6, 1, 2), kwargs = {})\n",
      "    %transpose_9 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%transpose_8, 1, 2), kwargs = {})\n",
      "    %group_norm_24 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%transpose_9, 32, %p_decoder_mid_block_attentions_0_group_norm_weight, %p_decoder_mid_block_attentions_0_group_norm_bias, 1e-06), kwargs = {})\n",
      "    %transpose_10 : [num_users=3] = call_function[target=torch.ops.aten.transpose.int](args = (%group_norm_24, 1, 2), kwargs = {})\n",
      "    %linear_4 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_10, %p_decoder_mid_block_attentions_0_to_q_weight, %p_decoder_mid_block_attentions_0_to_q_bias), kwargs = {})\n",
      "    %linear_5 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_10, %p_decoder_mid_block_attentions_0_to_k_weight, %p_decoder_mid_block_attentions_0_to_k_bias), kwargs = {})\n",
      "    %linear_6 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_10, %p_decoder_mid_block_attentions_0_to_v_weight, %p_decoder_mid_block_attentions_0_to_v_bias), kwargs = {})\n",
      "    %view_7 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_4, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_11 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_7, 1, 2), kwargs = {})\n",
      "    %view_8 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_5, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_12 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_8, 1, 2), kwargs = {})\n",
      "    %view_9 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_6, [1, -1, 1, 512]), kwargs = {})\n",
      "    %transpose_13 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_9, 1, 2), kwargs = {})\n",
      "    %scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%transpose_11, %transpose_12, %transpose_13), kwargs = {})\n",
      "    %transpose_14 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%scaled_dot_product_attention_1, 1, 2), kwargs = {})\n",
      "    %view_10 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_14, [1, -1, 512]), kwargs = {})\n",
      "    %_to_copy_1 : [num_users=1] = call_function[target=torch.ops.aten._to_copy.default](args = (%view_10,), kwargs = {dtype: torch.float32})\n",
      "    %linear_7 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%_to_copy_1, %p_decoder_mid_block_attentions_0_to_out_0_weight, %p_decoder_mid_block_attentions_0_to_out_0_bias), kwargs = {})\n",
      "    %dropout_12 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%linear_7, 0.0, False), kwargs = {})\n",
      "    %transpose_15 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%dropout_12, -1, -2), kwargs = {})\n",
      "    %view_11 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_15, [1, 512, 64, 64]), kwargs = {})\n",
      "    %add_12 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_11, %div_11), kwargs = {})\n",
      "    %div_12 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_12, 1), kwargs = {})\n",
      "    %group_norm_25 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_12, 32, %p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight, %p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_23 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_25,), kwargs = {})\n",
      "    %conv2d_32 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_23, %p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight, %p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_26 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_32, 32, %p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight, %p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_24 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_26,), kwargs = {})\n",
      "    %dropout_13 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_24, 0.0, False), kwargs = {})\n",
      "    %conv2d_33 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_13, %p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight, %p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_13 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_12, %conv2d_33), kwargs = {})\n",
      "    %div_13 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_13, 1), kwargs = {})\n",
      "    %_to_copy_2 : [num_users=2] = call_function[target=torch.ops.aten._to_copy.default](args = (%div_13,), kwargs = {dtype: torch.float32})\n",
      "    %group_norm_27 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%_to_copy_2, 32, %p_decoder_up_blocks_0_resnets_0_norm1_weight, %p_decoder_up_blocks_0_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_25 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_27,), kwargs = {})\n",
      "    %conv2d_34 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_25, %p_decoder_up_blocks_0_resnets_0_conv1_weight, %p_decoder_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_28 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_34, 32, %p_decoder_up_blocks_0_resnets_0_norm2_weight, %p_decoder_up_blocks_0_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_26 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_28,), kwargs = {})\n",
      "    %dropout_14 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_26, 0.0, False), kwargs = {})\n",
      "    %conv2d_35 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_14, %p_decoder_up_blocks_0_resnets_0_conv2_weight, %p_decoder_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_14 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%_to_copy_2, %conv2d_35), kwargs = {})\n",
      "    %div_14 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_14, 1.0), kwargs = {})\n",
      "    %group_norm_29 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_14, 32, %p_decoder_up_blocks_0_resnets_1_norm1_weight, %p_decoder_up_blocks_0_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_27 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_29,), kwargs = {})\n",
      "    %conv2d_36 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_27, %p_decoder_up_blocks_0_resnets_1_conv1_weight, %p_decoder_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_30 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_36, 32, %p_decoder_up_blocks_0_resnets_1_norm2_weight, %p_decoder_up_blocks_0_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_28 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_30,), kwargs = {})\n",
      "    %dropout_15 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_28, 0.0, False), kwargs = {})\n",
      "    %conv2d_37 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_15, %p_decoder_up_blocks_0_resnets_1_conv2_weight, %p_decoder_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_15 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_14, %conv2d_37), kwargs = {})\n",
      "    %div_15 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_15, 1.0), kwargs = {})\n",
      "    %group_norm_31 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_15, 32, %p_decoder_up_blocks_0_resnets_2_norm1_weight, %p_decoder_up_blocks_0_resnets_2_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_29 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_31,), kwargs = {})\n",
      "    %conv2d_38 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_29, %p_decoder_up_blocks_0_resnets_2_conv1_weight, %p_decoder_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_32 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_38, 32, %p_decoder_up_blocks_0_resnets_2_norm2_weight, %p_decoder_up_blocks_0_resnets_2_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_30 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_32,), kwargs = {})\n",
      "    %dropout_16 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_30, 0.0, False), kwargs = {})\n",
      "    %conv2d_39 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_16, %p_decoder_up_blocks_0_resnets_2_conv2_weight, %p_decoder_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_16 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_15, %conv2d_39), kwargs = {})\n",
      "    %div_16 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_16, 1.0), kwargs = {})\n",
      "    %upsample_nearest2d : [num_users=1] = call_function[target=torch.ops.aten.upsample_nearest2d.vec](args = (%div_16, None, [2.0, 2.0]), kwargs = {})\n",
      "    %conv2d_40 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%upsample_nearest2d, %p_decoder_up_blocks_0_upsamplers_0_conv_weight, %p_decoder_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_33 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_40, 32, %p_decoder_up_blocks_1_resnets_0_norm1_weight, %p_decoder_up_blocks_1_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_31 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_33,), kwargs = {})\n",
      "    %conv2d_41 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_31, %p_decoder_up_blocks_1_resnets_0_conv1_weight, %p_decoder_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_34 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_41, 32, %p_decoder_up_blocks_1_resnets_0_norm2_weight, %p_decoder_up_blocks_1_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_32 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_34,), kwargs = {})\n",
      "    %dropout_17 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_32, 0.0, False), kwargs = {})\n",
      "    %conv2d_42 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_17, %p_decoder_up_blocks_1_resnets_0_conv2_weight, %p_decoder_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_17 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_40, %conv2d_42), kwargs = {})\n",
      "    %div_17 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_17, 1.0), kwargs = {})\n",
      "    %group_norm_35 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_17, 32, %p_decoder_up_blocks_1_resnets_1_norm1_weight, %p_decoder_up_blocks_1_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_33 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_35,), kwargs = {})\n",
      "    %conv2d_43 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_33, %p_decoder_up_blocks_1_resnets_1_conv1_weight, %p_decoder_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_36 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_43, 32, %p_decoder_up_blocks_1_resnets_1_norm2_weight, %p_decoder_up_blocks_1_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_34 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_36,), kwargs = {})\n",
      "    %dropout_18 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_34, 0.0, False), kwargs = {})\n",
      "    %conv2d_44 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_18, %p_decoder_up_blocks_1_resnets_1_conv2_weight, %p_decoder_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_18 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_17, %conv2d_44), kwargs = {})\n",
      "    %div_18 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_18, 1.0), kwargs = {})\n",
      "    %group_norm_37 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_18, 32, %p_decoder_up_blocks_1_resnets_2_norm1_weight, %p_decoder_up_blocks_1_resnets_2_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_35 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_37,), kwargs = {})\n",
      "    %conv2d_45 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_35, %p_decoder_up_blocks_1_resnets_2_conv1_weight, %p_decoder_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_38 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_45, 32, %p_decoder_up_blocks_1_resnets_2_norm2_weight, %p_decoder_up_blocks_1_resnets_2_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_36 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_38,), kwargs = {})\n",
      "    %dropout_19 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_36, 0.0, False), kwargs = {})\n",
      "    %conv2d_46 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_19, %p_decoder_up_blocks_1_resnets_2_conv2_weight, %p_decoder_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_19 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_18, %conv2d_46), kwargs = {})\n",
      "    %div_19 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_19, 1.0), kwargs = {})\n",
      "    %upsample_nearest2d_1 : [num_users=1] = call_function[target=torch.ops.aten.upsample_nearest2d.vec](args = (%div_19, None, [2.0, 2.0]), kwargs = {})\n",
      "    %conv2d_47 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%upsample_nearest2d_1, %p_decoder_up_blocks_1_upsamplers_0_conv_weight, %p_decoder_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_39 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_47, 32, %p_decoder_up_blocks_2_resnets_0_norm1_weight, %p_decoder_up_blocks_2_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_37 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_39,), kwargs = {})\n",
      "    %conv2d_48 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_37, %p_decoder_up_blocks_2_resnets_0_conv1_weight, %p_decoder_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_40 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_48, 32, %p_decoder_up_blocks_2_resnets_0_norm2_weight, %p_decoder_up_blocks_2_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_38 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_40,), kwargs = {})\n",
      "    %dropout_20 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_38, 0.0, False), kwargs = {})\n",
      "    %conv2d_49 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_20, %p_decoder_up_blocks_2_resnets_0_conv2_weight, %p_decoder_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%conv2d_47,), kwargs = {memory_format: torch.contiguous_format})\n",
      "    %conv2d_50 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%clone, %p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight, %p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_20 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_50, %conv2d_49), kwargs = {})\n",
      "    %div_20 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_20, 1.0), kwargs = {})\n",
      "    %group_norm_41 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_20, 32, %p_decoder_up_blocks_2_resnets_1_norm1_weight, %p_decoder_up_blocks_2_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_39 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_41,), kwargs = {})\n",
      "    %conv2d_51 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_39, %p_decoder_up_blocks_2_resnets_1_conv1_weight, %p_decoder_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_42 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_51, 32, %p_decoder_up_blocks_2_resnets_1_norm2_weight, %p_decoder_up_blocks_2_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_40 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_42,), kwargs = {})\n",
      "    %dropout_21 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_40, 0.0, False), kwargs = {})\n",
      "    %conv2d_52 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_21, %p_decoder_up_blocks_2_resnets_1_conv2_weight, %p_decoder_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_21 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_20, %conv2d_52), kwargs = {})\n",
      "    %div_21 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_21, 1.0), kwargs = {})\n",
      "    %group_norm_43 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_21, 32, %p_decoder_up_blocks_2_resnets_2_norm1_weight, %p_decoder_up_blocks_2_resnets_2_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_41 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_43,), kwargs = {})\n",
      "    %conv2d_53 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_41, %p_decoder_up_blocks_2_resnets_2_conv1_weight, %p_decoder_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_44 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_53, 32, %p_decoder_up_blocks_2_resnets_2_norm2_weight, %p_decoder_up_blocks_2_resnets_2_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_42 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_44,), kwargs = {})\n",
      "    %dropout_22 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_42, 0.0, False), kwargs = {})\n",
      "    %conv2d_54 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_22, %p_decoder_up_blocks_2_resnets_2_conv2_weight, %p_decoder_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_22 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_21, %conv2d_54), kwargs = {})\n",
      "    %div_22 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_22, 1.0), kwargs = {})\n",
      "    %upsample_nearest2d_2 : [num_users=1] = call_function[target=torch.ops.aten.upsample_nearest2d.vec](args = (%div_22, None, [2.0, 2.0]), kwargs = {})\n",
      "    %conv2d_55 : [num_users=2] = call_function[target=torch.ops.aten.conv2d.default](args = (%upsample_nearest2d_2, %p_decoder_up_blocks_2_upsamplers_0_conv_weight, %p_decoder_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_45 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_55, 32, %p_decoder_up_blocks_3_resnets_0_norm1_weight, %p_decoder_up_blocks_3_resnets_0_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_43 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_45,), kwargs = {})\n",
      "    %conv2d_56 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_43, %p_decoder_up_blocks_3_resnets_0_conv1_weight, %p_decoder_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_46 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_56, 32, %p_decoder_up_blocks_3_resnets_0_norm2_weight, %p_decoder_up_blocks_3_resnets_0_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_44 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_46,), kwargs = {})\n",
      "    %dropout_23 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_44, 0.0, False), kwargs = {})\n",
      "    %conv2d_57 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_23, %p_decoder_up_blocks_3_resnets_0_conv2_weight, %p_decoder_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %conv2d_58 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%conv2d_55, %p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight, %p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias), kwargs = {})\n",
      "    %add_23 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%conv2d_58, %conv2d_57), kwargs = {})\n",
      "    %div_23 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_23, 1.0), kwargs = {})\n",
      "    %group_norm_47 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_23, 32, %p_decoder_up_blocks_3_resnets_1_norm1_weight, %p_decoder_up_blocks_3_resnets_1_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_45 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_47,), kwargs = {})\n",
      "    %conv2d_59 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_45, %p_decoder_up_blocks_3_resnets_1_conv1_weight, %p_decoder_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_48 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_59, 32, %p_decoder_up_blocks_3_resnets_1_norm2_weight, %p_decoder_up_blocks_3_resnets_1_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_46 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_48,), kwargs = {})\n",
      "    %dropout_24 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_46, 0.0, False), kwargs = {})\n",
      "    %conv2d_60 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_24, %p_decoder_up_blocks_3_resnets_1_conv2_weight, %p_decoder_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_24 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_23, %conv2d_60), kwargs = {})\n",
      "    %div_24 : [num_users=2] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_24, 1.0), kwargs = {})\n",
      "    %group_norm_49 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_24, 32, %p_decoder_up_blocks_3_resnets_2_norm1_weight, %p_decoder_up_blocks_3_resnets_2_norm1_bias, 1e-06), kwargs = {})\n",
      "    %silu_47 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_49,), kwargs = {})\n",
      "    %conv2d_61 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_47, %p_decoder_up_blocks_3_resnets_2_conv1_weight, %p_decoder_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %group_norm_50 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%conv2d_61, 32, %p_decoder_up_blocks_3_resnets_2_norm2_weight, %p_decoder_up_blocks_3_resnets_2_norm2_bias, 1e-06), kwargs = {})\n",
      "    %silu_48 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_50,), kwargs = {})\n",
      "    %dropout_25 : [num_users=1] = call_function[target=torch.ops.aten.dropout.default](args = (%silu_48, 0.0, False), kwargs = {})\n",
      "    %conv2d_62 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%dropout_25, %p_decoder_up_blocks_3_resnets_2_conv2_weight, %p_decoder_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    %add_25 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%div_24, %conv2d_62), kwargs = {})\n",
      "    %div_25 : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%add_25, 1.0), kwargs = {})\n",
      "    %group_norm_51 : [num_users=1] = call_function[target=torch.ops.aten.group_norm.default](args = (%div_25, 32, %p_decoder_conv_norm_out_weight, %p_decoder_conv_norm_out_bias, 1e-06), kwargs = {})\n",
      "    %silu_49 : [num_users=1] = call_function[target=torch.ops.aten.silu.default](args = (%group_norm_51,), kwargs = {})\n",
      "    %conv2d_63 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%silu_49, %p_decoder_conv_out_weight, %p_decoder_conv_out_bias, [1, 1], [1, 1]), kwargs = {})\n",
      "    return (conv2d_63,)\n",
      "VAE exported ➜  vae_fp32.ep\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKL\n",
    "from torch.export import export\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "        \"prs-eth/marigold-depth-v1-1\", subfolder=\"vae\").cpu().eval()\n",
    "\n",
    "rgb = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "# AutoencoderKL.forward does: encode → (optionally sample) → decode\n",
    "gm_vae = export(vae, (rgb,))            # default sample_posterior = False\n",
    "gm_vae.graph.print_tabular()            # full encoder + decoder graph\n",
    "print(gm_vae.graph)\n",
    "\n",
    "save(gm_vae, \"vae_fp32.ep\")\n",
    "print(\"VAE exported ➜  vae_fp32.ep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dc2355",
   "metadata": {},
   "source": [
    "## this is for comaporing the graph module with the origional model (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428e44e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max |Δ| = tensor(0., grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "ep_loaded = load(\"vae_fp32.ep\")   # the ExportedProgram\n",
    "\n",
    "gm = ep_loaded.module()           # ← call with no args, returns GraphModule\n",
    "out_graph = gm(rgb)               # run the graph\n",
    "out_eager = vae(rgb)              # original eager result\n",
    "\n",
    "print(\"max |Δ| =\", (out_graph.sample - out_eager.sample).abs().max())  # ≈ 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d4abe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch : 2.4.1+cu121 |  CUDA runtime 12.1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision' has no attribute '_is_compiled_with_cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch :\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m__version__,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|  CUDA runtime\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtv    :\u001b[39m\u001b[38;5;124m\"\u001b[39m, torchvision\u001b[38;5;241m.\u001b[39m__version__, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|  compiled w/ CUDA:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 4\u001b[0m       torchvision\u001b[38;5;241m.\u001b[39m_is_tracing() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_compiled_with_cuda\u001b[49m())  \u001b[38;5;66;03m# True means CUDA kernels present\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU   :\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(), torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_capability())\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision' has no attribute '_is_compiled_with_cuda'"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(\"torch :\", torch.__version__,  \"|  CUDA runtime\", torch.version.cuda)\n",
    "print(\"tv    :\", torchvision.__version__, \"|  compiled w/ CUDA:\",\n",
    "      torchvision._is_tracing() is not None or torchvision._is_compiled_with_cuda())  # True means CUDA kernels present\n",
    "print(\"GPU   :\", torch.cuda.get_device_name(), torch.cuda.get_device_capability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62beef50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'prediction_type': 'depth', 'text_encoder': ['transformers', 'CLIPTextModel'], 'tokenizer': ['transformers', 'CLIPTokenizer']} were passed to MarigoldDepthPipeline, but are not expected and will be ignored. Please verify your model_index.json configuration file.\n",
      "Keyword arguments {'prediction_type': 'depth', 'text_encoder': ['transformers', 'CLIPTextModel'], 'tokenizer': ['transformers', 'CLIPTokenizer']} are not expected by MarigoldDepthPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 3/3 [00:00<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (768, 1024) (768, 1024) (768, 1024)\n",
      "dtype: float16\n",
      " any NaN: 786432  any Inf: 0\n",
      " valid-mask count: 786432 / 786432\n",
      "PRED min/max/mean/std: nan/nan/nan/nan\n",
      " GT  min/max/mean/std: 1.507/13.708/5.214/2.434\n",
      " MAE: nan   RMSE: nan\n",
      " δ<1.25: 0.00%\n",
      " δ<1.56: 0.00%\n",
      " δ<1.95: 0.00%\n",
      " Pearson r: nan\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "array type float16 is unsupported in linalg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# 9️⃣ LS‐alignment (as before)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     aligned, s, t \u001b[38;5;241m=\u001b[39m \u001b[43malign_depth_least_square\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgt_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_scale_shift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_resolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m960\u001b[39;49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malignment OK – scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshift\u001b[39m\u001b[38;5;124m\"\u001b[39m, t)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mLinAlgError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Marigold/src/util/alignment.py:73\u001b[0m, in \u001b[0;36malign_depth_least_square\u001b[0;34m(gt_arr, pred_arr, valid_mask_arr, return_scale_shift, max_resolution)\u001b[0m\n\u001b[1;32m     71\u001b[0m A \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([pred_masked, _ones], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_masked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mLinAlgError:\n\u001b[1;32m     75\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinAlgError in align_depth_least_square ‒ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD did not converge; falling back to identity scale-shift\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m     )\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/numpy/linalg/_linalg.py:2502\u001b[0m, in \u001b[0;36mlstsq\u001b[0;34m(a, b, rcond)\u001b[0m\n\u001b[1;32m   2499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m!=\u001b[39m m2:\n\u001b[1;32m   2500\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIncompatible dimensions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2502\u001b[0m t, result_t \u001b[38;5;241m=\u001b[39m \u001b[43m_commonType\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2503\u001b[0m result_real_t \u001b[38;5;241m=\u001b[39m _realType(result_t)\n\u001b[1;32m   2505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rcond \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/numpy/linalg/_linalg.py:162\u001b[0m, in \u001b[0;36m_commonType\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    159\u001b[0m         result_type \u001b[38;5;241m=\u001b[39m double\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m rt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;66;03m# unsupported inexact scalar\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is unsupported in linalg\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    163\u001b[0m                 (a\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mname,))\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     result_type \u001b[38;5;241m=\u001b[39m double\n",
      "\u001b[0;31mTypeError\u001b[0m: array type float16 is unsupported in linalg"
     ]
    }
   ],
   "source": [
    "import os, torch, numpy as np\n",
    "from marigold.marigold_depth_pipeline import MarigoldDepthPipeline\n",
    "from diffusers import DDIMScheduler\n",
    "from omegaconf import OmegaConf\n",
    "from src.dataset import DatasetMode, get_dataset\n",
    "from src.util.alignment import align_depth_least_square\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# 0️⃣ Pin to GPU 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "torch.cuda.set_device(1)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# 1️⃣ Load pipeline + fine‑tuned UNet\n",
    "ckpt_root  = \"/home/abradshaw/Marigold/output/train_marigold_depth/checkpoint/latest\"\n",
    "base_model = \"prs-eth/marigold-depth-v1-1\"\n",
    "dtype = torch.float16           # <-- only change\n",
    "\n",
    "pipe = MarigoldDepthPipeline.from_pretrained(base_model,\n",
    "                                             torch_dtype=dtype)\n",
    "\n",
    "pipe.unet = pipe.unet.__class__.from_pretrained(\n",
    "    os.path.join(ckpt_root, \"unet\"), torch_dtype=dtype\n",
    ")\n",
    "\n",
    "pipe.scheduler = DDIMScheduler.from_pretrained(\n",
    "    os.path.join(ckpt_root, \"scheduler\")\n",
    ")\n",
    "\n",
    "pipe = pipe.to(device, dtype=dtype)      # make sure VAE & UNet are fp16\n",
    "\n",
    "# 2️⃣ Load one sample\n",
    "cfg = OmegaConf.load(\"/home/abradshaw/Marigold/config/dataset_depth/data_hypersim_val.yaml\")\n",
    "ds  = get_dataset(cfg, base_data_dir=\"/home/abradshaw/marigold_data\", mode=DatasetMode.EVAL)\n",
    "sample     = ds[0]\n",
    "rgb_pil    = to_pil_image(sample[\"rgb_int\"].to(torch.uint8))\n",
    "gt_depth   = sample[\"depth_raw_linear\"].numpy()      # shape (1,H,W) or (H,W)\n",
    "valid_mask = sample[\"valid_mask_raw\"].numpy()         # (H,W) or (1,H,W)\n",
    "\n",
    "# 3️⃣ Inference\n",
    "with torch.no_grad():\n",
    "    pred = pipe(rgb_pil, show_progress_bar=False).depth_np  # shape (1,H,W)\n",
    "\n",
    " # ─────────── Squeeze to 2D ───────────\n",
    "pred_depth = np.squeeze(pred)        # (H, W)\n",
    "gt_depth   = np.squeeze(gt_depth)    # (H, W)  ← ADD THIS LINE\n",
    "valid_mask = np.squeeze(valid_mask)  # (H, W)\n",
    " # ──────────────────────────────────────\n",
    "\n",
    "print(\"shapes:\", pred_depth.shape, gt_depth.shape, valid_mask.shape)\n",
    "print(\"dtype:\", pred_depth.dtype)\n",
    "# 1) NaN / Inf counts\n",
    "print(\" any NaN:\", np.isnan(pred_depth).sum(),\n",
    "      \" any Inf:\", np.isinf(pred_depth).sum())\n",
    "# 2) how many valid pixels?\n",
    "print(\" valid-mask count:\", valid_mask.sum(), \"/\", valid_mask.size)\n",
    "\n",
    " # 4️⃣ Flatten over valid pixels\n",
    "assert pred_depth.shape == gt_depth.shape == valid_mask.shape, (\n",
    "    f\"shapes mismatch: pred {pred_depth.shape},  gt {gt_depth.shape},  mask {valid_mask.shape}\"\n",
    ")\n",
    "\n",
    "pred_vals = pred_depth[valid_mask]\n",
    "gt_vals   = gt_depth[valid_mask]\n",
    "\n",
    "# 5️⃣ Basic summary stats\n",
    "print(\"PRED min/max/mean/std:\",\n",
    "      f\"{pred_vals.min():.3f}/\"\n",
    "      f\"{pred_vals.max():.3f}/\"\n",
    "      f\"{pred_vals.mean():.3f}/\"\n",
    "      f\"{pred_vals.std():.3f}\")\n",
    "print(\" GT  min/max/mean/std:\",\n",
    "      f\"{gt_vals.min():.3f}/\"\n",
    "      f\"{gt_vals.max():.3f}/\"\n",
    "      f\"{gt_vals.mean():.3f}/\"\n",
    "      f\"{gt_vals.std():.3f}\")\n",
    "\n",
    "# 6️⃣ MAE & RMSE\n",
    "errors = pred_vals - gt_vals\n",
    "mae  = np.mean(np.abs(errors))\n",
    "rmse = np.sqrt(np.mean(errors**2))\n",
    "print(f\" MAE: {mae:.3f}   RMSE: {rmse:.3f}\")\n",
    "\n",
    "# 7️⃣ δ‐accuracies\n",
    "ratio = np.maximum(pred_vals/gt_vals, gt_vals/pred_vals)\n",
    "for δ in [1.25, 1.25**2, 1.25**3]:\n",
    "    print(f\" δ<{δ:.2f}:\", f\"{(ratio<δ).mean():.2%}\")\n",
    "\n",
    "# 8️⃣ Pearson r\n",
    "r, _ = pearsonr(gt_vals, pred_vals)\n",
    "print(f\" Pearson r: {r:.4f}\")\n",
    "\n",
    "# 9️⃣ LS‐alignment (as before)\n",
    "try:\n",
    "    aligned, s, t = align_depth_least_square(\n",
    "        gt_depth, pred_depth, valid_mask,\n",
    "        return_scale_shift=True, max_resolution=960\n",
    "    )\n",
    "    print(\"alignment OK – scale\", s, \"shift\", t)\n",
    "except np.linalg.LinAlgError as e:\n",
    "    print(\"LS failure:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e79075ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scheduler: <class 'diffusers.schedulers.scheduling_ddim.DDIMScheduler'>\n",
      "  rescale_betas_zero_snr = True\n",
      "  timestep_spacing       = trailing\n",
      "  betas[0:3]             = tensor([0.0009, 0.0009, 0.0009])\n",
      "unet conv_in: torch.Size([320, 8, 3, 3])\n",
      "unet dtype  : torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(\"scheduler:\", type(pipe.scheduler))\n",
    "print(\"  rescale_betas_zero_snr =\", pipe.scheduler.config.rescale_betas_zero_snr)\n",
    "print(\"  timestep_spacing       =\", pipe.scheduler.config.timestep_spacing)\n",
    "print(\"  betas[0:3]             =\", pipe.scheduler.betas[:3])\n",
    "\n",
    "print(\"unet conv_in:\", pipe.unet.conv_in.weight.shape)   # should be (320, 8, 3, 3)\n",
    "print(\"unet dtype  :\", pipe.unet.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a84ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning depth maps: 100%|██████████| 100/100 [00:04<00:00, 23.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"n_total\": 7370,\n",
      "  \"n_ok\": 100,\n",
      "  \"n_zero_mask\": 0,\n",
      "  \"n_nan_inf\": 0,\n",
      "  \"min_depth\": 0.23999999463558197,\n",
      "  \"max_depth\": 15.097999572753906\n",
      "}\n",
      "first 10 bad indices: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, torch, json, random\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "# now tell PyTorch “use the first visible device”\n",
    "torch.cuda.set_device(1)\n",
    "stats = {\n",
    "    \"n_total\"      : len(ds),\n",
    "    \"n_ok\"         : 0,\n",
    "    \"n_zero_mask\"  : 0,         # valid_mask.sum()==0\n",
    "    \"n_nan_inf\"    : 0,         # contains NaN / Inf\n",
    "    \"min_depth\"    :  1e9,\n",
    "    \"max_depth\"    : -1e9,\n",
    "}\n",
    "bad_indices = []\n",
    "\n",
    "for i in tqdm(range(100), desc=\"Scanning depth maps\"):\n",
    "    s          = ds[i]\n",
    "    depth_np   = s[\"depth_raw_linear\"].numpy()\n",
    "    valid_mask = s[\"valid_mask_raw\"].numpy()\n",
    "\n",
    "    # 1. check finite values\n",
    "    if not np.isfinite(depth_np).all():\n",
    "        stats[\"n_nan_inf\"] += 1\n",
    "        bad_indices.append((i,\"nan/inf\"))\n",
    "        continue\n",
    "\n",
    "    # 2. check mask\n",
    "    if valid_mask.sum() == 0:\n",
    "        stats[\"n_zero_mask\"] += 1\n",
    "        bad_indices.append((i,\"zero_mask\"))\n",
    "        continue\n",
    "\n",
    "    stats[\"n_ok\"] += 1\n",
    "    stats[\"min_depth\"] = min(stats[\"min_depth\"], float(depth_np[valid_mask].min()))\n",
    "    stats[\"max_depth\"] = max(stats[\"max_depth\"], float(depth_np[valid_mask].max()))\n",
    "\n",
    "print(json.dumps(stats, indent=2))\n",
    "print(\"first 10 bad indices:\", bad_indices[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3357ea6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'prediction_type': 'depth', 'text_encoder': ['transformers', 'CLIPTextModel'], 'tokenizer': ['transformers', 'CLIPTokenizer']} were passed to MarigoldDepthPipeline, but are not expected and will be ignored. Please verify your model_index.json configuration file.\n",
      "Keyword arguments {'prediction_type': 'depth', 'text_encoder': ['transformers', 'CLIPTextModel'], 'tokenizer': ['transformers', 'CLIPTokenizer']} are not expected by MarigoldDepthPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 3/3 [00:00<00:00, 45.49it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m pred_depth \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(pred)                             \u001b[38;5;66;03m# shape (H, W)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 5️⃣ Mask & Flatten\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m pred_depth\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m gt_depth\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m valid_mask\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     49\u001b[0m pred_vals \u001b[38;5;241m=\u001b[39m pred_depth[valid_mask]\n\u001b[1;32m     50\u001b[0m gt_vals   \u001b[38;5;241m=\u001b[39m gt_depth[valid_mask]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Single‐Cell Evaluation & Metrics Visualization\n",
    "# Runs one validation sample through your fine‐tuned Marigold pipeline,\n",
    "# then computes a battery of depth‐error metrics and visualizations.\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from marigold.marigold_depth_pipeline import MarigoldDepthPipeline\n",
    "from diffusers import DDIMScheduler\n",
    "from omegaconf import OmegaConf\n",
    "from src.dataset import DatasetMode, get_dataset\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# 1️⃣ Environment & Device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "torch.cuda.set_device(1)                              # first visible device → GPU 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2️⃣ Load Pipeline & Fine‑Tuned Checkpoint\n",
    "base_model = \"prs-eth/marigold-depth-v1-1\"\n",
    "ckpt_root  = \"/home/abradshaw/Marigold/output/train_marigold_depth/checkpoint/latest\"\n",
    "pipe = MarigoldDepthPipeline.from_pretrained(base_model, torch_dtype=torch.float32)\n",
    "pipe.unet = pipe.unet.__class__.from_pretrained(\n",
    "    os.path.join(ckpt_root, \"unet\"),\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "pipe.scheduler = DDIMScheduler.from_pretrained(os.path.join(ckpt_root, \"scheduler\"))\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# 3️⃣ Prepare One Validation Sample\n",
    "cfg = OmegaConf.load(\"/home/abradshaw/Marigold/config/dataset_depth/data_hypersim_val.yaml\")\n",
    "ds  = get_dataset(cfg, base_data_dir=\"/home/abradshaw/marigold_data\", mode=DatasetMode.EVAL)\n",
    "sample      = ds[0]\n",
    "rgb_pil     = to_pil_image(sample[\"rgb_int\"].to(torch.uint8))\n",
    "gt_depth    = sample[\"depth_raw_linear\"].numpy()       # (H, W)\n",
    "valid_mask  = sample[\"valid_mask_raw\"].numpy().astype(bool)\n",
    "\n",
    "# 4️⃣ Run Inference\n",
    "with torch.no_grad():\n",
    "    pred = pipe(rgb_pil, show_progress_bar=False).depth_np  # shape (1,H,W)\n",
    "pred_depth = np.squeeze(pred)                             # shape (H, W)\n",
    "\n",
    "# 5️⃣ Mask & Flatten\n",
    "assert pred_depth.shape == gt_depth.shape == valid_mask.shape\n",
    "pred_vals = pred_depth[valid_mask]\n",
    "gt_vals   = gt_depth[valid_mask]\n",
    "\n",
    "# 6️⃣ Basic Summary Statistics\n",
    "print(f\"PRED  → min {pred_vals.min():.3f}, max {pred_vals.max():.3f}, \"\n",
    "      f\"mean {pred_vals.mean():.3f}, std {pred_vals.std():.3f}\")\n",
    "print(f\"GT    → min {gt_vals.min():.3f}, max {gt_vals.max():.3f}, \"\n",
    "      f\"mean {gt_vals.mean():.3f}, std {gt_vals.std():.3f}\")\n",
    "\n",
    "# 7️⃣ Percentiles\n",
    "for p in [1, 5, 25, 50, 75, 95, 99]:\n",
    "    print(f\"PRED {p}th percentile: {np.percentile(pred_vals, p):.3f}\")\n",
    "\n",
    "# 8️⃣ Correlation\n",
    "r, _ = pearsonr(gt_vals, pred_vals)\n",
    "print(f\"Pearson r: {r:.4f}\")\n",
    "\n",
    "# 9️⃣ Error Metrics: MAE, RMSE, SILog\n",
    "errors = pred_vals - gt_vals\n",
    "mae  = np.mean(np.abs(errors))\n",
    "rmse = np.sqrt(np.mean(errors**2))\n",
    "d    = np.log(pred_vals + 1e-6) - np.log(gt_vals + 1e-6)\n",
    "silog= np.sqrt(np.mean(d**2) - 0.85*(np.mean(d))**2)\n",
    "print(f\"MAE: {mae:.3f} m,  RMSE: {rmse:.3f} m,  SILog: {silog:.3f}\")\n",
    "\n",
    "# 1️⃣0️⃣ Threshold Accuracies δ<1.25,1.25²,1.25³\n",
    "ratio = np.maximum(pred_vals/gt_vals, gt_vals/pred_vals)\n",
    "for d in [1.25, 1.25**2, 1.25**3]:\n",
    "    print(f\"δ<{d:.3f}: {np.mean(ratio<d):.3%}\")\n",
    "\n",
    "# 1️⃣1️⃣ Plots\n",
    "\n",
    "# a) Scatter plot GT vs PRED\n",
    "idx = np.random.choice(len(pred_vals), size=5000, replace=False)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(gt_vals[idx], pred_vals[idx], s=1, alpha=0.1)\n",
    "mn, mx = gt_vals.min(), gt_vals.max()\n",
    "plt.plot([mn,mx], [mn,mx], 'r--', lw=1)\n",
    "plt.xlabel(\"GT depth (m)\"); plt.ylabel(\"Pred depth (m)\")\n",
    "plt.title(\"Scatter: Pred vs GT\")\n",
    "plt.show()\n",
    "\n",
    "# b) Error histogram\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(errors, bins=100, range=(-5,5))\n",
    "plt.title(\"Error Distribution (Pred - GT)\"); plt.xlabel(\"Error (m)\"); plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# c) Visual maps of GT, Pred, and Error\n",
    "vmin, vmax = 0.0, 15.0\n",
    "fig, axes = plt.subplots(1,3,figsize=(15,5))\n",
    "axes[0].imshow(gt_depth,   vmin=vmin, vmax=vmax, cmap='inferno'); axes[0].set_title(\"GT Depth\")\n",
    "axes[1].imshow(pred_depth, vmin=vmin, vmax=vmax, cmap='inferno'); axes[1].set_title(\"Predicted Depth\")\n",
    "axes[2].imshow(errors,     vmin=-5,   vmax=5,     cmap='bwr');     axes[2].set_title(\"Error (m)\")\n",
    "for ax in axes: ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c2b29b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m########################################################################\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 0) choose a dataset item and its prediction\u001b[39;00m\n\u001b[1;32m      8\u001b[0m idx  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m                      \u001b[38;5;66;03m# <-- change if needed\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m[idx]                 \u001b[38;5;66;03m# ds is your BaseDepthDataset\u001b[39;00m\n\u001b[1;32m     10\u001b[0m gt   \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth_raw_linear\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     11\u001b[0m mask \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_mask_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "# ================= ONE-SHOT DEPTH / LS DIAGNOSTICS =================\n",
    "import numpy as np, torch, json, matplotlib.pyplot as plt, seaborn as sns\n",
    "from numpy.linalg import LinAlgError\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "########################################################################\n",
    "# 0) choose a dataset item and its prediction\n",
    "idx  = 0                      # <-- change if needed\n",
    "item = ds[idx]                 # ds is your BaseDepthDataset\n",
    "gt   = item[\"depth_raw_linear\"].numpy()\n",
    "mask = item[\"valid_mask_raw\"].numpy().squeeze()\n",
    "\n",
    "\n",
    "\n",
    "# generate or reuse a prediction for this rgb\n",
    "rgb_pil   = torch.clip((item[\"rgb_norm\"] + 1) / 2, 0, 1)\n",
    "rgb_pil   = torch.permute(rgb_pil, (1,2,0)).mul(255).byte().cpu().numpy()\n",
    "from PIL import Image\n",
    "pred      = pipe(Image.fromarray(rgb_pil), show_progress_bar=False).depth_np.squeeze()\n",
    "\n",
    "########################################################################\n",
    "def ls_basic(g,p,m):\n",
    "    A = np.c_[p[m], np.ones(np.count_nonzero(m))]\n",
    "    try:\n",
    "        X = np.linalg.lstsq(A, g[m,None], rcond=None)[0].ravel()\n",
    "        return X[0], X[1], None\n",
    "    except LinAlgError as e:\n",
    "        return 1.0, 0.0, e\n",
    "\n",
    "scale, shift, err = ls_basic(gt, pred, mask)\n",
    "\n",
    "########################################################################\n",
    "# 1) quick stats\n",
    "n_pix = int(mask.sum())\n",
    "print(f\"[idx {idx}] pixels used for LS : {n_pix}\")\n",
    "print(\"GT   min/max/std :\", gt[mask].min(), gt[mask].max(), gt[mask].std())\n",
    "print(\"Pred min/max/std :\", pred[mask].min(), pred[mask].max(), pred[mask].std())\n",
    "print(\"LS result        : scale\", scale, \"shift\", shift, \" error:\", err)\n",
    "\n",
    "########################################################################\n",
    "# 2) singular values & condition #\n",
    "A = np.c_[pred[mask], np.ones(n_pix)]\n",
    "_, S, _ = np.linalg.svd(A, full_matrices=False)\n",
    "print(\"singular values  :\", S)\n",
    "print(\"condition number :\", S.max()/S.min())\n",
    "\n",
    "########################################################################\n",
    "# 3) scatter and LS line (only first 5k points for speed)\n",
    "keep = slice(None) if n_pix < 5000 else slice(None, None, n_pix//5000)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(pred[mask][keep], gt[mask][keep], s=4, alpha=.25)\n",
    "x_ = np.linspace(pred[mask].min(), pred[mask].max(), 2)\n",
    "plt.plot(x_, scale*x_ + shift, 'r')\n",
    "plt.xlabel(\"Prediction\"), plt.ylabel(\"GT\")\n",
    "plt.title(f\"idx {idx}\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "########################################################################\n",
    "# 4) ridge attempt if basic LS failed\n",
    "if err is not None:\n",
    "    lam = 1e-6\n",
    "    ATA = A.T@A + lam*np.eye(2)\n",
    "    ATb = A.T@gt[mask,None]\n",
    "    scale_r, shift_r = np.linalg.solve(ATA, ATb).ravel()\n",
    "    print(\"ridge (λ=1e-6)    : scale\", scale_r, \"shift\", shift_r)\n",
    "\n",
    "########################################################################\n",
    "print(\"Done.\")\n",
    "# ====================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5576e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Running on: NVIDIA GeForce RTX 4090 (GPU 0)\n",
      "⏱️ Inference time: 2.27 seconds for 100 matmuls\n",
      "⚙️ SM Clock:     2250 MHz\n",
      "🌡️ Temp:         62 °C\n",
      "🔋 Power Usage:  445.18 W / 450.00 W\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "gpu_id = 0\n",
    "\n",
    "torch.cuda.set_device(gpu_id)\n",
    "device = torch.device(f\"cuda:{gpu_id}\")\n",
    "print(f\"\\n🧠 Running on: {torch.cuda.get_device_name(device)} (GPU {gpu_id})\")\n",
    "\n",
    "iters = 100\n",
    "x = torch.randn(8192, 8192, device=device)\n",
    "y = torch.randn(8192, 8192, device=device)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "for _ in range(iters):\n",
    "    z = torch.matmul(x, y)\n",
    "torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "\n",
    "print(f\"⏱️ Inference time: {end - start:.2f} seconds for {iters} matmuls\")\n",
    "\n",
    "def run_nvidia_smi_query():\n",
    "    fields = \"clocks.sm,temperature.gpu,power.draw,power.limit\"\n",
    "    cmd = f\"nvidia-smi --id={gpu_id} --query-gpu={fields} --format=csv,noheader,nounits\"\n",
    "    try:\n",
    "        output = subprocess.check_output(cmd.split()).decode().strip()\n",
    "        sm_clock, temp, power_draw, power_limit = output.split(\", \")\n",
    "        print(f\"⚙️ SM Clock:     {sm_clock} MHz\")\n",
    "        print(f\"🌡️ Temp:         {temp} °C\")\n",
    "        print(f\"🔋 Power Usage:  {power_draw} W / {power_limit} W\")\n",
    "    except Exception as e:\n",
    "        print(\"Error running nvidia-smi:\", e)\n",
    "\n",
    "run_nvidia_smi_query()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184bd807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | file                                 | shape    | dtype   |         min |      max |   mean |   std |     % zeros |   % NaNs |\n",
      "|----|--------------------------------------|----------|---------|-------------|----------|--------|-------|-------------|----------|\n",
      "|  0 | pred_00019_00183_indoors_000_010.npy | 768×1024 | float16 | 0.0889893   | 0.902344 |    inf |   inf | 0           |        0 |\n",
      "|  1 | pred_00019_00183_indoors_000_040.npy | 768×1024 | float16 | 0.000213623 | 0.938477 |    inf |   inf | 0           |        0 |\n",
      "|  2 | pred_00019_00183_indoors_010_000.npy | 768×1024 | float16 | 0           | 0.787598 |    inf |   inf | 0.286865    |        0 |\n",
      "|  3 | pred_00019_00183_indoors_010_020.npy | 768×1024 | float16 | 0.0256958   | 0.948242 |    inf |   inf | 0           |        0 |\n",
      "|  4 | pred_00019_00183_indoors_020_030.npy | 768×1024 | float16 | 0           | 0.876465 |    inf |   inf | 0.00305176  |        0 |\n",
      "|  5 | pred_00019_00183_indoors_020_050.npy | 768×1024 | float16 | 0.112976    | 0.677246 |    inf |   inf | 0           |        0 |\n",
      "|  6 | pred_00019_00183_indoors_030_010.npy | 768×1024 | float16 | 0.0765991   | 0.909668 |    inf |   inf | 0           |        0 |\n",
      "|  7 | pred_00019_00183_indoors_030_040.npy | 768×1024 | float16 | 0.00256348  | 0.824219 |    inf |   inf | 0           |        0 |\n",
      "|  8 | pred_00019_00183_indoors_040_000.npy | 768×1024 | float16 | 0.000244141 | 0.914062 |    inf |   inf | 0           |        0 |\n",
      "|  9 | pred_00019_00183_indoors_040_030.npy | 768×1024 | float16 | 0           | 0.931641 |    inf |   inf | 0.0864665   |        0 |\n",
      "| 10 | pred_00019_00183_indoors_050_010.npy | 768×1024 | float16 | 0.0247192   | 0.887695 |    inf |   inf | 0           |        0 |\n",
      "| 11 | pred_00019_00183_indoors_060_000.npy | 768×1024 | float16 | 0.0196228   | 0.945801 |    inf |   inf | 0           |        0 |\n",
      "| 12 | pred_00019_00183_indoors_080_000.npy | 768×1024 | float16 | 0.154663    | 0.905273 |    inf |   inf | 0           |        0 |\n",
      "| 13 | pred_00019_00183_indoors_110_000.npy | 768×1024 | float16 | 0.0071106   | 0.929688 |    inf |   inf | 0           |        0 |\n",
      "| 14 | pred_00019_00183_indoors_110_050.npy | 768×1024 | float16 | 0           | 0.705078 |    inf |   inf | 0.000508626 |        0 |\n",
      "| 15 | pred_00019_00183_indoors_120_010.npy | 768×1024 | float16 | 0.0106659   | 0.936523 |    inf |   inf | 0           |        0 |\n",
      "| 16 | pred_00019_00183_indoors_120_030.npy | 768×1024 | float16 | 0.00650787  | 0.930176 |    inf |   inf | 0           |        0 |\n",
      "| 17 | pred_00019_00183_indoors_130_000.npy | 768×1024 | float16 | 0.000732422 | 0.943359 |    inf |   inf | 0           |        0 |\n",
      "| 18 | pred_00019_00183_indoors_130_020.npy | 768×1024 | float16 | 0           | 0.969727 |    inf |   inf | 0.0167847   |        0 |\n",
      "| 19 | pred_00019_00183_indoors_130_040.npy | 768×1024 | float16 | 0.0237732   | 0.945312 |    inf |   inf | 0           |        0 |\n",
      "| 20 | pred_00019_00183_indoors_140_010.npy | 768×1024 | float16 | 0.017334    | 0.979492 |    inf |   inf | 0           |        0 |\n",
      "| 21 | pred_00019_00183_indoors_140_030.npy | 768×1024 | float16 | 0.0695801   | 0.935547 |    inf |   inf | 0           |        0 |\n",
      "| 22 | pred_00019_00183_indoors_140_050.npy | 768×1024 | float16 | 0.0350037   | 0.877441 |    inf |   inf | 0           |        0 |\n",
      "| 23 | pred_00019_00183_indoors_150_000.npy | 768×1024 | float16 | 0           | 0.99707  |    inf |   inf | 0.134786    |        0 |\n",
      "| 24 | pred_00019_00183_indoors_150_020.npy | 768×1024 | float16 | 0.0682373   | 0.979492 |    inf |   inf | 0           |        0 |\n",
      "| 25 | pred_00019_00183_indoors_160_010.npy | 768×1024 | float16 | 0.0225525   | 0.963867 |    inf |   inf | 0           |        0 |\n",
      "| 26 | pred_00019_00183_indoors_160_030.npy | 768×1024 | float16 | 0.332031    | 0.761719 |    inf |   inf | 0           |        0 |\n",
      "| 27 | pred_00019_00183_indoors_160_050.npy | 768×1024 | float16 | 0.0297852   | 0.727539 |    inf |   inf | 0           |        0 |\n",
      "| 28 | pred_00019_00183_indoors_170_000.npy | 768×1024 | float16 | 0.00248718  | 0.967773 |    inf |   inf | 0           |        0 |\n",
      "| 29 | pred_00019_00183_indoors_170_020.npy | 768×1024 | float16 | 0.0625      | 0.946289 |    inf |   inf | 0           |        0 |\n",
      "| 30 | pred_00019_00183_indoors_170_040.npy | 768×1024 | float16 | 0           | 0.833496 |    inf |   inf | 0.000508626 |        0 |\n",
      "| 31 | pred_00019_00183_indoors_180_010.npy | 768×1024 | float16 | 0.00682068  | 0.970703 |    inf |   inf | 0           |        0 |\n",
      "| 32 | pred_00019_00183_indoors_180_030.npy | 768×1024 | float16 | 0.0588379   | 0.955078 |    inf |   inf | 0           |        0 |\n",
      "| 33 | pred_00019_00183_indoors_180_050.npy | 768×1024 | float16 | 0.00268555  | 0.833984 |    inf |   inf | 0           |        0 |\n",
      "| 34 | pred_00019_00183_indoors_190_040.npy | 768×1024 | float16 | 0.0385742   | 0.975098 |    inf |   inf | 0           |        0 |\n",
      "| 35 | pred_00019_00183_indoors_200_020.npy | 768×1024 | float16 | 0.0156555   | 0.978516 |    inf |   inf | 0           |        0 |\n",
      "| 36 | pred_00019_00183_indoors_200_050.npy | 768×1024 | float16 | 0.00732422  | 0.96582  |    inf |   inf | 0           |        0 |\n",
      "| 37 | pred_00019_00183_indoors_210_030.npy | 768×1024 | float16 | 0.0078125   | 0.85791  |    inf |   inf | 0           |        0 |\n",
      "| 38 | pred_00019_00183_indoors_220_020.npy | 768×1024 | float16 | 0.00894165  | 0.983398 |    inf |   inf | 0           |        0 |\n",
      "| 39 | pred_00019_00183_indoors_220_040.npy | 768×1024 | float16 | 0           | 0.959473 |    inf |   inf | 0.000508626 |        0 |\n",
      "| 40 | pred_00019_00183_indoors_230_010.npy | 768×1024 | float16 | 0           | 0.970703 |    inf |   inf | 0.0467936   |        0 |\n",
      "| 41 | pred_00019_00183_indoors_230_030.npy | 768×1024 | float16 | 0.0119553   | 0.931641 |    inf |   inf | 0           |        0 |\n",
      "| 42 | pred_00019_00183_indoors_230_050.npy | 768×1024 | float16 | 0           | 0.930664 |    inf |   inf | 0.000508626 |        0 |\n",
      "| 43 | pred_00019_00183_indoors_240_000.npy | 768×1024 | float16 | 0.00585938  | 0.933594 |    inf |   inf | 0           |        0 |\n",
      "| 44 | pred_00019_00183_indoors_240_020.npy | 768×1024 | float16 | 0.0193329   | 0.916016 |    inf |   inf | 0           |        0 |\n",
      "| 45 | pred_00019_00183_indoors_240_040.npy | 768×1024 | float16 | 0.0467834   | 0.850586 |    inf |   inf | 0           |        0 |\n",
      "| 46 | pred_00019_00183_indoors_250_010.npy | 768×1024 | float16 | 0           | 0.993164 |    inf |   inf | 0.00254313  |        0 |\n",
      "| 47 | pred_00019_00183_indoors_250_030.npy | 768×1024 | float16 | 0.121521    | 0.842285 |    inf |   inf | 0           |        0 |\n",
      "| 48 | pred_00019_00183_indoors_260_000.npy | 768×1024 | float16 | 0.0101929   | 0.902344 |    inf |   inf | 0           |        0 |\n",
      "| 49 | pred_00019_00183_indoors_260_020.npy | 768×1024 | float16 | 0.0537109   | 0.944824 |    inf |   inf | 0           |        0 |\n",
      "| 50 | pred_00019_00183_indoors_260_050.npy | 768×1024 | float16 | 0.0917358   | 0.838867 |    inf |   inf | 0           |        0 |\n",
      "| 51 | pred_00019_00183_indoors_270_010.npy | 768×1024 | float16 | 0           | 0.999023 |    inf |   inf | 0.0717163   |        0 |\n",
      "| 52 | pred_00019_00183_indoors_270_040.npy | 768×1024 | float16 | 0.271973    | 0.902832 |    inf |   inf | 0           |        0 |\n",
      "| 53 | pred_00019_00183_indoors_280_020.npy | 768×1024 | float16 | 0.0222473   | 0.88623  |    inf |   inf | 0           |        0 |\n",
      "| 54 | pred_00019_00183_indoors_280_050.npy | 768×1024 | float16 | 0.18396     | 0.843262 |    inf |   inf | 0           |        0 |\n",
      "| 55 | pred_00019_00183_indoors_290_000.npy | 768×1024 | float16 | 0.00878906  | 0.984375 |    inf |   inf | 0           |        0 |\n",
      "| 56 | pred_00019_00183_indoors_290_040.npy | 768×1024 | float16 | 0.398926    | 0.855957 |    inf |   inf | 0           |        0 |\n",
      "| 57 | pred_00019_00183_indoors_300_010.npy | 768×1024 | float16 | 0.00817871  | 0.942383 |    inf |   inf | 0           |        0 |\n",
      "| 58 | pred_00019_00183_indoors_300_030.npy | 768×1024 | float16 | 0.0754395   | 0.796875 |    inf |   inf | 0           |        0 |\n",
      "| 59 | pred_00019_00183_indoors_300_050.npy | 768×1024 | float16 | 0.111633    | 0.860352 |    inf |   inf | 0           |        0 |\n",
      "| 60 | pred_00019_00183_indoors_310_000.npy | 768×1024 | float16 | 0           | 0.969727 |    inf |   inf | 0.000508626 |        0 |\n",
      "| 61 | pred_00019_00183_indoors_310_020.npy | 768×1024 | float16 | 0.0111389   | 0.959473 |    inf |   inf | 0           |        0 |\n",
      "| 62 | pred_00019_00183_indoors_310_040.npy | 768×1024 | float16 | 0.0209351   | 0.932617 |    inf |   inf | 0           |        0 |\n",
      "| 63 | pred_00019_00183_indoors_320_010.npy | 768×1024 | float16 | 0.00759888  | 0.963867 |    inf |   inf | 0           |        0 |\n",
      "| 64 | pred_00019_00183_indoors_320_030.npy | 768×1024 | float16 | 0.0118103   | 0.879883 |    inf |   inf | 0           |        0 |\n",
      "| 65 | pred_00019_00183_indoors_330_000.npy | 768×1024 | float16 | 0.00634766  | 0.964844 |    inf |   inf | 0           |        0 |\n",
      "| 66 | pred_00019_00183_indoors_330_020.npy | 768×1024 | float16 | 0.0360718   | 0.972656 |    inf |   inf | 0           |        0 |\n",
      "| 67 | pred_00019_00183_indoors_330_050.npy | 768×1024 | float16 | 0           | 0.960449 |    inf |   inf | 0.00152588  |        0 |\n",
      "| 68 | pred_00019_00183_indoors_340_010.npy | 768×1024 | float16 | 0           | 0.938965 |    inf |   inf | 0.000508626 |        0 |\n",
      "| 69 | pred_00019_00183_indoors_340_040.npy | 768×1024 | float16 | 0.00558472  | 0.950684 |    inf |   inf | 0           |        0 |\n",
      "| 70 | pred_00019_00183_indoors_350_000.npy | 768×1024 | float16 | 0.00674057  | 0.980469 |    inf |   inf | 0           |        0 |\n",
      "| 71 | pred_00019_00183_indoors_350_020.npy | 768×1024 | float16 | 0.0927734   | 0.916016 |    inf |   inf | 0           |        0 |\n",
      "| 72 | pred_00019_00183_indoors_350_050.npy | 768×1024 | float16 | 0.0134583   | 0.926758 |    inf |   inf | 0           |        0 |\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, glob, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "#from ace_tools import display_dataframe_to_user   # available inside ChatGPT notebooks; ignore if not\n",
    "\n",
    "# 🔧 1 ‑‑‑‑‑ Edit this to point at the folder that holds your *.npy depth predictions\n",
    "directory_path = \"/tmp/marigold_smoke/indoors1/scene_00019/scan_00183\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2 ‑‑‑‑‑ No changes needed below this line\n",
    "# ---------------------------------------------------------------------------\n",
    "file_pattern = os.path.join(directory_path, \"*.npy\")\n",
    "file_list   = sorted(glob.glob(file_pattern))\n",
    "if not file_list:\n",
    "    raise FileNotFoundError(f\"No .npy files found in {directory_path}\")\n",
    "\n",
    "rows = []\n",
    "for f in file_list:\n",
    "    arr_f16 = np.load(f)\n",
    "    arr     = arr_f16.astype(np.float16)\n",
    "    rows.append(\n",
    "        {\n",
    "            \"file\": os.path.basename(f),\n",
    "            \"shape\": \"×\".join(map(str, arr.shape)),\n",
    "            \"dtype\": str(arr.dtype),\n",
    "            \"min\":   float(np.nanmin(arr)),\n",
    "            \"max\":   float(np.nanmax(arr)),\n",
    "            \"mean\":  float(np.nanmean(arr)),\n",
    "            \"std\":   float(np.nanstd(arr)),\n",
    "            \"% zeros\": float((arr == 0).sum()  / arr.size * 100),\n",
    "            \"% NaNs\":  float(np.isnan(arr).sum() / arr.size * 100),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "try:\n",
    "    display_dataframe_to_user(\"Prediction statistics\", df)\n",
    "except NameError:\n",
    "    # fall back to plain print if not running inside ChatGPT / ace_tools\n",
    "    from tabulate import tabulate\n",
    "    print(tabulate(df, headers=\"keys\", tablefmt=\"github\"))\n",
    "\n",
    "# quick‑look histogram of the first file\n",
    "sample = np.load(file_list[0])\n",
    "plt.hist(sample.flatten(), bins=100)\n",
    "plt.title(f\"Histogram – {os.path.basename(file_list[0])}\")\n",
    "plt.xlabel(\"depth value\");  plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae1959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype = float32 |  min = 0.08898926 |  max = 0.90234375 |  mean = 0.6008287 |  std = 0.21301217\n"
     ]
    }
   ],
   "source": [
    "f0 = file_list[0]\n",
    "arr16 = np.load(f0)                 # original float16\n",
    "arr32 = arr16.astype(np.float32)    # up‑cast\n",
    "\n",
    "print(\"dtype =\", arr32.dtype,\n",
    "      \"|  min =\", arr32.min(),\n",
    "      \"|  max =\", arr32.max(),\n",
    "      \"|  mean =\", arr32.mean(),\n",
    "      \"|  std =\",  arr32.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c7178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 3/3 [00:00<00:00, 51.05it/s]\n",
      "WARNING:root:The loaded `DDIMScheduler` is configured with `timestep_spacing=\"leading\"`; the recommended setting is `\"trailing\"`. This change is backward-compatible and yields better results. Consider using `prs-eth/marigold-depth-v1-1` for the best experience.\n",
      "WARNING:root:The loaded `DDIMScheduler` is configured with `rescale_betas_zero_snr=False`; the recommended setting is True. Consider using `prs-eth/marigold-depth-v1-1` for the best experience.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs : 0\n",
      "range: 0.0 1.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch, numpy as np\n",
    "from PIL import Image\n",
    "from diffusers import DDIMScheduler\n",
    "from marigold.marigold_depth_pipeline import MarigoldDepthPipeline\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "\n",
    "# -------- config ---------\n",
    "ckpt_dir = Path(\"/home/abradshaw/marigold_checkpoints/stable-diffusion-2\")\n",
    "img_path = Path(\"/datasets/abradshaw/marigold_data/hypersim/val/ai_003_010/rgb_cam_00_fr0000.png\")\n",
    "device   = \"cuda\"                                  # or \"cpu\"\n",
    "dtype    = torch.float32\n",
    "denoise_steps = 4\n",
    "# -------------------------\n",
    "\n",
    "# --- optional: use DDIM instead of PNDM -------------\n",
    "ddim = DDIMScheduler.from_pretrained(ckpt_dir, subfolder=\"scheduler\")\n",
    "\n",
    "# --- build pipeline (UNet still expects 4ch here) ---\n",
    "pipe = MarigoldDepthPipeline.from_pretrained(\n",
    "    ckpt_dir,\n",
    "    scheduler=ddim,           # drop this arg to stay with PNDM\n",
    ").to(device, dtype=dtype)\n",
    "\n",
    "# ------- widen conv_in from 4 → 8 channels ----------\n",
    "old = pipe.unet.conv_in                        # Conv2d(4 → 320)\n",
    "w, b = old.weight.data, old.bias.data          # shapes [320,4,3,3] & [320]\n",
    "\n",
    "new_conv = nn.Conv2d(\n",
    "    in_channels = 8,\n",
    "    out_channels= old.out_channels,\n",
    "    kernel_size = old.kernel_size,\n",
    "    stride      = old.stride,\n",
    "    padding     = old.padding,\n",
    ")\n",
    "new_conv.weight = Parameter(w.repeat(1, 2, 1, 1) * 0.5)  # duplicate & halve\n",
    "new_conv.bias   = Parameter(b.clone())\n",
    "\n",
    "pipe.unet.conv_in = new_conv\n",
    "pipe.unet.config[\"in_channels\"] = 8\n",
    "pipe.unet.eval(); pipe.vae.eval()\n",
    "\n",
    "# ------------------ inference -----------------------\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "out = pipe(img,\n",
    "           denoising_steps=denoise_steps,\n",
    "           processing_res=0,            # keep native res\n",
    "           show_progress_bar=False)\n",
    "\n",
    "print(\"NaNs :\", np.isnan(out.depth_np).sum())\n",
    "print(\"range:\", out.depth_np.min(), out.depth_np.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02176e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (480, 640)\n",
      "Min depth: 0.0000\n",
      "Max depth: 1.0000\n",
      "Mean depth: 0.4578\n",
      "Median depth: 0.3917\n",
      "Std dev: 0.2675\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Marigold depth prediction\n",
    "depth_path = \"/home/abradshaw/Marigold/output/eval/nyu_test/prediction/test/living_room_0053/pred_1291.npy\"\n",
    "depth = np.load(depth_path)\n",
    "\n",
    "# Check shape and confirm it's a depth map\n",
    "print(f\"Shape: {depth.shape}\")\n",
    "assert depth.shape == (480, 640), \"Unexpected depth shape!\"\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"Min depth: {depth.min():.4f}\")\n",
    "print(f\"Max depth: {depth.max():.4f}\")\n",
    "print(f\"Mean depth: {depth.mean():.4f}\")\n",
    "print(f\"Median depth: {np.median(depth):.4f}\")\n",
    "print(f\"Std dev: {depth.std():.4f}\")\n",
    "\n",
    "# Optional: visualize the depth map\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(depth, cmap='plasma')\n",
    "plt.title(\"Predicted Depth Map\")\n",
    "plt.colorbar(label='Depth')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43288f1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /home/abradshaw/Marigold/output/iter_000000 is not the path to a directory containing a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/diffusers/configuration_utils.py:390\u001b[0m, in \u001b[0;36mConfigMixin.load_config\u001b[0;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/abradshaw/Marigold/output/iter_000000'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m EXP \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/abradshaw/Marigold/output/iter_000000\u001b[39m\u001b[38;5;124m\"\u001b[39m          \u001b[38;5;66;03m# <- point to the UNet folder you saved\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ref \u001b[38;5;241m=\u001b[39m UNet2DConditionModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(REF, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m exp \u001b[38;5;241m=\u001b[39m \u001b[43mUNet2DConditionModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEXP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m mismatch \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (n_r, p_r), (n_e, p_e) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ref\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mitems(), exp\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mitems()):\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/diffusers/models/modeling_utils.py:1014\u001b[0m, in \u001b[0;36mModelMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m config_path \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;66;03m# load config\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m config, unused_kwargs, commit_hash \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdduf_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdduf_entries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;66;03m# no in-place modification of the original config.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/diffusers/configuration_utils.py:426\u001b[0m, in \u001b[0;36mConfigMixin.load_config\u001b[0;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt connect to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to load this model, couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find it\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the cached files and it looks like \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not the path to a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m directory containing a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCheckout your internet connection or see how to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m run the library in offline mode at\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/diffusers/installation#offline-mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load config for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like /home/abradshaw/Marigold/output/iter_000000 is not the path to a directory containing a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "import torch, json, pathlib, pprint\n",
    "\n",
    "REF = \"prs-eth/marigold-depth-v1-1\"\n",
    "EXP = \"/home/abradshaw/Marigold/output/iter_000000\"          # <- point to the UNet folder you saved\n",
    "\n",
    "ref = UNet2DConditionModel.from_pretrained(REF, subfolder=\"unet\")\n",
    "exp = UNet2DConditionModel.from_pretrained(EXP, subfolder=\"unet\")\n",
    "\n",
    "mismatch = {}\n",
    "for (n_r, p_r), (n_e, p_e) in zip(ref.state_dict().items(), exp.state_dict().items()):\n",
    "    if p_r.shape != p_e.shape:\n",
    "        mismatch[n_r] = (tuple(p_r.shape), tuple(p_e.shape))\n",
    "\n",
    "print(f\"# mismatched tensors: {len(mismatch)}\")\n",
    "pprint.pp(mismatch)        # empty dict == good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c5bbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Loading checkpoints …\n",
      "hash(conv_in.weight[:5])  reference = 8c014793d95052c4a422c3bd30a1412d\n",
      "hash(conv_in.weight[:5])  new       = a250c0a6364a712d9ffa93ba20cf9f3d\n",
      "✅  UNet hash differs. Overall Δ (relative L2): 0.3070\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# ------------ 0. Edit these two lines ----------------------------------\n",
    "SNAP = \"9571e7123e258cf052b4e54241f17971c290e9a8\" \n",
    "REF_CKPT = f\"/home/abradshaw/.cache/huggingface/hub/\" \\\n",
    "           f\"models--prs-eth--marigold-depth-v1-1/snapshots/{SNAP}\"\n",
    "NEW_UNET  = \"./output/train_marigold_depth/checkpoint/latest/unet/\" \\\n",
    "            \"diffusion_pytorch_model.safetensors\"       # trainer‑written weights\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "import hashlib, torch, numpy as np\n",
    "from diffusers import UNet2DConditionModel\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "\n",
    "def md5(t):                # quick fingerprint\n",
    "    return hashlib.md5(t.detach().cpu().numpy().tobytes()).hexdigest()\n",
    "\n",
    "print(\"⚠️  Loading checkpoints …\")\n",
    "ref_unet  = UNet2DConditionModel.from_pretrained(f\"{REF_CKPT}/unet\",\n",
    "                                                 torch_dtype=torch.float32,\n",
    "                                                 local_files_only=True)\n",
    "new_state = load_safetensors(NEW_UNET, device=\"cpu\")\n",
    "\n",
    "# ── 1) simple hash comparison ──────────────────────────────────────────\n",
    "h_ref = md5(ref_unet.state_dict()[\"conv_in.weight\"][:5])\n",
    "h_new = md5(new_state[\"conv_in.weight\"][:5])\n",
    "print(f\"hash(conv_in.weight[:5])  reference = {h_ref}\")\n",
    "print(f\"hash(conv_in.weight[:5])  new       = {h_new}\")\n",
    "\n",
    "# ── 2) global change metric ────────────────────────────────────────────\n",
    "if h_ref == h_new:\n",
    "    print(\"🔥  The saved UNet is **bit‑identical** to the reference!  \"\n",
    "          \"Either training hasn’t updated the weights yet, or you copied \"\n",
    "          \"the wrong folder.\")\n",
    "else:\n",
    "    l2_diff, l2_ref = 0.0, 0.0\n",
    "    for k, w_ref in ref_unet.state_dict().items():\n",
    "        w_new = new_state[k]\n",
    "        l2_diff += torch.sum((w_ref - w_new) ** 2).item()\n",
    "        l2_ref  += torch.sum(w_ref ** 2).item()\n",
    "    rel_change = (l2_diff / l2_ref) ** 0.5\n",
    "    print(f\"✅  UNet hash differs. Overall Δ (relative L2): {rel_change:.4f}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a899bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Suspicious layers =================================================\n",
      "conv_in.weight                                                ⚠ big change\n",
      "conv_in.bias                                                  ⚠ big change\n",
      "time_embedding.linear_1.weight                                ⚠ big change\n",
      "time_embedding.linear_1.bias                                  ⚠ big change\n",
      "time_embedding.linear_2.weight                                ⚠ big change\n",
      "time_embedding.linear_2.bias                                  ⚠ big change\n",
      "down_blocks.0.attentions.0.norm.weight                        ⚠ big change\n",
      "down_blocks.0.attentions.0.norm.bias                          ⚠ big change\n",
      "down_blocks.0.attentions.0.proj_in.weight                     ⚠ big change\n",
      "down_blocks.0.attentions.0.proj_in.bias                       ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias    ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias    ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias    ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias  ⚠ big change\n",
      "down_blocks.0.attentions.0.proj_out.weight                    ⚠ big change\n",
      "down_blocks.0.attentions.0.proj_out.bias                      ⚠ big change\n",
      "down_blocks.0.attentions.1.norm.weight                        ⚠ big change\n",
      "down_blocks.0.attentions.1.norm.bias                          ⚠ big change\n",
      "down_blocks.0.attentions.1.proj_in.weight                     ⚠ big change\n",
      "down_blocks.0.attentions.1.proj_in.bias                       ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias    ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias    ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias    ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias  ⚠ big change\n",
      "down_blocks.0.attentions.1.proj_out.weight                    ⚠ big change\n",
      "down_blocks.0.attentions.1.proj_out.bias                      ⚠ big change\n",
      "down_blocks.0.resnets.0.norm1.weight                          ⚠ big change\n",
      "down_blocks.0.resnets.0.norm1.bias                            ⚠ big change\n",
      "down_blocks.0.resnets.0.conv1.weight                          ⚠ big change\n",
      "down_blocks.0.resnets.0.conv1.bias                            ⚠ big change\n",
      "down_blocks.0.resnets.0.time_emb_proj.weight                  ⚠ big change\n",
      "down_blocks.0.resnets.0.time_emb_proj.bias                    ⚠ big change\n",
      "down_blocks.0.resnets.0.norm2.weight                          ⚠ high σ, ⚠ big change\n",
      "down_blocks.0.resnets.0.norm2.bias                            ⚠ big change\n",
      "down_blocks.0.resnets.0.conv2.weight                          ⚠ big change\n",
      "down_blocks.0.resnets.0.conv2.bias                            ⚠ big change\n",
      "down_blocks.0.resnets.1.norm1.weight                          ⚠ big change\n",
      "down_blocks.0.resnets.1.norm1.bias                            ⚠ big change\n",
      "down_blocks.0.resnets.1.conv1.weight                          ⚠ big change\n",
      "down_blocks.0.resnets.1.conv1.bias                            ⚠ big change\n",
      "down_blocks.0.resnets.1.time_emb_proj.weight                  ⚠ big change\n",
      "down_blocks.0.resnets.1.time_emb_proj.bias                    ⚠ big change\n",
      "down_blocks.0.resnets.1.norm2.weight                          ⚠ high σ, ⚠ big change\n",
      "down_blocks.0.resnets.1.norm2.bias                            ⚠ big change\n",
      "down_blocks.0.resnets.1.conv2.weight                          ⚠ big change\n",
      "down_blocks.0.resnets.1.conv2.bias                            ⚠ big change\n",
      "down_blocks.0.downsamplers.0.conv.weight                      ⚠ big change\n",
      "down_blocks.0.downsamplers.0.conv.bias                        ⚠ big change\n",
      "down_blocks.1.attentions.0.norm.weight                        ⚠ big change\n",
      "down_blocks.1.attentions.0.norm.bias                          ⚠ big change\n",
      "down_blocks.1.attentions.0.proj_in.weight                     ⚠ big change\n",
      "down_blocks.1.attentions.0.proj_in.bias                       ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias    ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias    ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias    ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias  ⚠ big change\n",
      "down_blocks.1.attentions.0.proj_out.weight                    ⚠ big change\n",
      "down_blocks.1.attentions.0.proj_out.bias                      ⚠ big change\n",
      "down_blocks.1.attentions.1.norm.weight                        ⚠ big change\n",
      "down_blocks.1.attentions.1.norm.bias                          ⚠ big change\n",
      "down_blocks.1.attentions.1.proj_in.weight                     ⚠ big change\n",
      "down_blocks.1.attentions.1.proj_in.bias                       ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias    ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias    ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias    ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias  ⚠ big change\n",
      "down_blocks.1.attentions.1.proj_out.weight                    ⚠ big change\n",
      "down_blocks.1.attentions.1.proj_out.bias                      ⚠ big change\n",
      "down_blocks.1.resnets.0.norm1.weight                          ⚠ big change\n",
      "down_blocks.1.resnets.0.norm1.bias                            ⚠ big change\n",
      "down_blocks.1.resnets.0.conv1.weight                          ⚠ big change\n",
      "down_blocks.1.resnets.0.conv1.bias                            ⚠ big change\n",
      "down_blocks.1.resnets.0.time_emb_proj.weight                  ⚠ big change\n",
      "down_blocks.1.resnets.0.time_emb_proj.bias                    ⚠ big change\n",
      "down_blocks.1.resnets.0.norm2.weight                          ⚠ high σ, ⚠ big change\n",
      "down_blocks.1.resnets.0.norm2.bias                            ⚠ big change\n",
      "down_blocks.1.resnets.0.conv2.weight                          ⚠ big change\n",
      "down_blocks.1.resnets.0.conv2.bias                            ⚠ big change\n",
      "down_blocks.1.resnets.0.conv_shortcut.weight                  ⚠ big change\n",
      "down_blocks.1.resnets.0.conv_shortcut.bias                    ⚠ big change\n",
      "down_blocks.1.resnets.1.norm1.weight                          ⚠ big change\n",
      "down_blocks.1.resnets.1.norm1.bias                            ⚠ big change\n",
      "down_blocks.1.resnets.1.conv1.weight                          ⚠ big change\n",
      "down_blocks.1.resnets.1.conv1.bias                            ⚠ big change\n",
      "down_blocks.1.resnets.1.time_emb_proj.weight                  ⚠ big change\n",
      "down_blocks.1.resnets.1.time_emb_proj.bias                    ⚠ big change\n",
      "down_blocks.1.resnets.1.norm2.weight                          ⚠ high σ, ⚠ big change\n",
      "down_blocks.1.resnets.1.norm2.bias                            ⚠ high σ, ⚠ big change\n",
      "down_blocks.1.resnets.1.conv2.weight                          ⚠ big change\n",
      "down_blocks.1.resnets.1.conv2.bias                            ⚠ big change\n",
      "down_blocks.1.downsamplers.0.conv.weight                      ⚠ big change\n",
      "down_blocks.1.downsamplers.0.conv.bias                        ⚠ big change\n",
      "down_blocks.2.attentions.0.norm.weight                        ⚠ big change\n",
      "down_blocks.2.attentions.0.norm.bias                          ⚠ big change\n",
      "down_blocks.2.attentions.0.proj_in.weight                     ⚠ big change\n",
      "down_blocks.2.attentions.0.proj_in.bias                       ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias    ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias    ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias    ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias  ⚠ big change\n",
      "down_blocks.2.attentions.0.proj_out.weight                    ⚠ big change\n",
      "down_blocks.2.attentions.0.proj_out.bias                      ⚠ big change\n",
      "down_blocks.2.attentions.1.norm.weight                        ⚠ big change\n",
      "down_blocks.2.attentions.1.norm.bias                          ⚠ big change\n",
      "down_blocks.2.attentions.1.proj_in.weight                     ⚠ big change\n",
      "down_blocks.2.attentions.1.proj_in.bias                       ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias    ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias    ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias    ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias  ⚠ big change\n",
      "down_blocks.2.attentions.1.proj_out.weight                    ⚠ big change\n",
      "down_blocks.2.attentions.1.proj_out.bias                      ⚠ big change\n",
      "down_blocks.2.resnets.0.norm1.weight                          ⚠ big change\n",
      "down_blocks.2.resnets.0.norm1.bias                            ⚠ big change\n",
      "down_blocks.2.resnets.0.conv1.weight                          ⚠ big change\n",
      "down_blocks.2.resnets.0.conv1.bias                            ⚠ big change\n",
      "down_blocks.2.resnets.0.time_emb_proj.weight                  ⚠ big change\n",
      "down_blocks.2.resnets.0.time_emb_proj.bias                    ⚠ big change\n",
      "down_blocks.2.resnets.0.norm2.weight                          ⚠ high σ, ⚠ big change\n",
      "down_blocks.2.resnets.0.norm2.bias                            ⚠ high σ, ⚠ big change\n",
      "down_blocks.2.resnets.0.conv2.weight                          ⚠ big change\n",
      "down_blocks.2.resnets.0.conv2.bias                            ⚠ big change\n",
      "down_blocks.2.resnets.0.conv_shortcut.weight                  ⚠ big change\n",
      "down_blocks.2.resnets.0.conv_shortcut.bias                    ⚠ big change\n",
      "down_blocks.2.resnets.1.norm1.weight                          ⚠ big change\n",
      "down_blocks.2.resnets.1.norm1.bias                            ⚠ big change\n",
      "down_blocks.2.resnets.1.conv1.weight                          ⚠ big change\n",
      "down_blocks.2.resnets.1.conv1.bias                            ⚠ big change\n",
      "down_blocks.2.resnets.1.time_emb_proj.weight                  ⚠ big change\n",
      "down_blocks.2.resnets.1.time_emb_proj.bias                    ⚠ big change\n",
      "down_blocks.2.resnets.1.norm2.weight                          ⚠ high σ, ⚠ big change\n",
      "down_blocks.2.resnets.1.norm2.bias                            ⚠ high σ, ⚠ big change\n",
      "down_blocks.2.resnets.1.conv2.weight                          ⚠ big change\n",
      "down_blocks.2.resnets.1.conv2.bias                            ⚠ big change\n",
      "down_blocks.2.downsamplers.0.conv.weight                      ⚠ big change\n",
      "down_blocks.2.downsamplers.0.conv.bias                        ⚠ big change\n",
      "down_blocks.3.resnets.0.norm1.weight                          ⚠ big change\n",
      "down_blocks.3.resnets.0.norm1.bias                            ⚠ high σ, ⚠ big change\n",
      "down_blocks.3.resnets.0.conv1.weight                          ⚠ big change\n",
      "down_blocks.3.resnets.0.conv1.bias                            ⚠ big change\n",
      "down_blocks.3.resnets.0.time_emb_proj.weight                  ⚠ big change\n",
      "down_blocks.3.resnets.0.time_emb_proj.bias                    ⚠ big change\n",
      "down_blocks.3.resnets.0.norm2.weight                          ⚠ high σ, ⚠ big change\n",
      "down_blocks.3.resnets.0.norm2.bias                            ⚠ big change\n",
      "down_blocks.3.resnets.0.conv2.weight                          ⚠ big change\n",
      "down_blocks.3.resnets.0.conv2.bias                            ⚠ big change\n",
      "down_blocks.3.resnets.1.norm1.weight                          ⚠ big change\n",
      "down_blocks.3.resnets.1.norm1.bias                            ⚠ big change\n",
      "down_blocks.3.resnets.1.conv1.weight                          ⚠ big change\n",
      "down_blocks.3.resnets.1.conv1.bias                            ⚠ big change\n",
      "down_blocks.3.resnets.1.time_emb_proj.weight                  ⚠ big change\n",
      "down_blocks.3.resnets.1.time_emb_proj.bias                    ⚠ big change\n",
      "down_blocks.3.resnets.1.norm2.weight                          ⚠ high σ, ⚠ big change\n",
      "down_blocks.3.resnets.1.norm2.bias                            ⚠ big change\n",
      "down_blocks.3.resnets.1.conv2.weight                          ⚠ big change\n",
      "down_blocks.3.resnets.1.conv2.bias                            ⚠ big change\n",
      "up_blocks.0.resnets.0.norm1.weight                            ⚠ big change\n",
      "up_blocks.0.resnets.0.norm1.bias                              ⚠ big change\n",
      "up_blocks.0.resnets.0.conv1.weight                            ⚠ big change\n",
      "up_blocks.0.resnets.0.conv1.bias                              ⚠ big change\n",
      "up_blocks.0.resnets.0.time_emb_proj.weight                    ⚠ big change\n",
      "up_blocks.0.resnets.0.time_emb_proj.bias                      ⚠ big change\n",
      "up_blocks.0.resnets.0.norm2.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.0.resnets.0.norm2.bias                              ⚠ big change\n",
      "up_blocks.0.resnets.0.conv2.weight                            ⚠ big change\n",
      "up_blocks.0.resnets.0.conv2.bias                              ⚠ big change\n",
      "up_blocks.0.resnets.0.conv_shortcut.weight                    ⚠ big change\n",
      "up_blocks.0.resnets.0.conv_shortcut.bias                      ⚠ big change\n",
      "up_blocks.0.resnets.1.norm1.weight                            ⚠ big change\n",
      "up_blocks.0.resnets.1.norm1.bias                              ⚠ big change\n",
      "up_blocks.0.resnets.1.conv1.weight                            ⚠ big change\n",
      "up_blocks.0.resnets.1.conv1.bias                              ⚠ big change\n",
      "up_blocks.0.resnets.1.time_emb_proj.weight                    ⚠ big change\n",
      "up_blocks.0.resnets.1.time_emb_proj.bias                      ⚠ big change\n",
      "up_blocks.0.resnets.1.norm2.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.0.resnets.1.norm2.bias                              ⚠ big change\n",
      "up_blocks.0.resnets.1.conv2.weight                            ⚠ big change\n",
      "up_blocks.0.resnets.1.conv2.bias                              ⚠ big change\n",
      "up_blocks.0.resnets.1.conv_shortcut.weight                    ⚠ big change\n",
      "up_blocks.0.resnets.1.conv_shortcut.bias                      ⚠ big change\n",
      "up_blocks.0.resnets.2.norm1.weight                            ⚠ big change\n",
      "up_blocks.0.resnets.2.norm1.bias                              ⚠ high σ, ⚠ big change\n",
      "up_blocks.0.resnets.2.conv1.weight                            ⚠ big change\n",
      "up_blocks.0.resnets.2.conv1.bias                              ⚠ big change\n",
      "up_blocks.0.resnets.2.time_emb_proj.weight                    ⚠ big change\n",
      "up_blocks.0.resnets.2.time_emb_proj.bias                      ⚠ big change\n",
      "up_blocks.0.resnets.2.norm2.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.0.resnets.2.norm2.bias                              ⚠ high σ, ⚠ big change\n",
      "up_blocks.0.resnets.2.conv2.weight                            ⚠ big change\n",
      "up_blocks.0.resnets.2.conv2.bias                              ⚠ big change\n",
      "up_blocks.0.resnets.2.conv_shortcut.weight                    ⚠ big change\n",
      "up_blocks.0.resnets.2.conv_shortcut.bias                      ⚠ big change\n",
      "up_blocks.0.upsamplers.0.conv.weight                          ⚠ big change\n",
      "up_blocks.0.upsamplers.0.conv.bias                            ⚠ big change\n",
      "up_blocks.1.attentions.0.norm.weight                          ⚠ big change\n",
      "up_blocks.1.attentions.0.norm.bias                            ⚠ big change\n",
      "up_blocks.1.attentions.0.proj_in.weight                       ⚠ big change\n",
      "up_blocks.1.attentions.0.proj_in.bias                         ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight    ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias      ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight    ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias      ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight    ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias      ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias   ⚠ big change\n",
      "up_blocks.1.attentions.0.proj_out.weight                      ⚠ big change\n",
      "up_blocks.1.attentions.0.proj_out.bias                        ⚠ big change\n",
      "up_blocks.1.attentions.1.norm.weight                          ⚠ big change\n",
      "up_blocks.1.attentions.1.norm.bias                            ⚠ big change\n",
      "up_blocks.1.attentions.1.proj_in.weight                       ⚠ big change\n",
      "up_blocks.1.attentions.1.proj_in.bias                         ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight    ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias      ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight    ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias      ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight    ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias      ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias   ⚠ big change\n",
      "up_blocks.1.attentions.1.proj_out.weight                      ⚠ big change\n",
      "up_blocks.1.attentions.1.proj_out.bias                        ⚠ big change\n",
      "up_blocks.1.attentions.2.norm.weight                          ⚠ big change\n",
      "up_blocks.1.attentions.2.norm.bias                            ⚠ big change\n",
      "up_blocks.1.attentions.2.proj_in.weight                       ⚠ big change\n",
      "up_blocks.1.attentions.2.proj_in.bias                         ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight    ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias      ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight    ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias      ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight    ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias      ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias   ⚠ big change\n",
      "up_blocks.1.attentions.2.proj_out.weight                      ⚠ big change\n",
      "up_blocks.1.attentions.2.proj_out.bias                        ⚠ big change\n",
      "up_blocks.1.resnets.0.norm1.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.1.resnets.0.norm1.bias                              ⚠ high σ, ⚠ big change\n",
      "up_blocks.1.resnets.0.conv1.weight                            ⚠ big change\n",
      "up_blocks.1.resnets.0.conv1.bias                              ⚠ big change\n",
      "up_blocks.1.resnets.0.time_emb_proj.weight                    ⚠ big change\n",
      "up_blocks.1.resnets.0.time_emb_proj.bias                      ⚠ big change\n",
      "up_blocks.1.resnets.0.norm2.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.1.resnets.0.norm2.bias                              ⚠ high σ, ⚠ big change\n",
      "up_blocks.1.resnets.0.conv2.weight                            ⚠ big change\n",
      "up_blocks.1.resnets.0.conv2.bias                              ⚠ big change\n",
      "up_blocks.1.resnets.0.conv_shortcut.weight                    ⚠ big change\n",
      "up_blocks.1.resnets.0.conv_shortcut.bias                      ⚠ big change\n",
      "up_blocks.1.resnets.1.norm1.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.1.resnets.1.norm1.bias                              ⚠ high σ, ⚠ big change\n",
      "up_blocks.1.resnets.1.conv1.weight                            ⚠ big change\n",
      "up_blocks.1.resnets.1.conv1.bias                              ⚠ big change\n",
      "up_blocks.1.resnets.1.time_emb_proj.weight                    ⚠ big change\n",
      "up_blocks.1.resnets.1.time_emb_proj.bias                      ⚠ big change\n",
      "up_blocks.1.resnets.1.norm2.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.1.resnets.1.norm2.bias                              ⚠ high σ, ⚠ big change\n",
      "up_blocks.1.resnets.1.conv2.weight                            ⚠ big change\n",
      "up_blocks.1.resnets.1.conv2.bias                              ⚠ big change\n",
      "up_blocks.1.resnets.1.conv_shortcut.weight                    ⚠ big change\n",
      "up_blocks.1.resnets.1.conv_shortcut.bias                      ⚠ big change\n",
      "up_blocks.1.resnets.2.norm1.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.1.resnets.2.norm1.bias                              ⚠ high σ, ⚠ big change\n",
      "up_blocks.1.resnets.2.conv1.weight                            ⚠ big change\n",
      "up_blocks.1.resnets.2.conv1.bias                              ⚠ big change\n",
      "up_blocks.1.resnets.2.time_emb_proj.weight                    ⚠ big change\n",
      "up_blocks.1.resnets.2.time_emb_proj.bias                      ⚠ big change\n",
      "up_blocks.1.resnets.2.norm2.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.1.resnets.2.norm2.bias                              ⚠ high σ, ⚠ big change\n",
      "up_blocks.1.resnets.2.conv2.weight                            ⚠ big change\n",
      "up_blocks.1.resnets.2.conv2.bias                              ⚠ big change\n",
      "up_blocks.1.resnets.2.conv_shortcut.weight                    ⚠ big change\n",
      "up_blocks.1.resnets.2.conv_shortcut.bias                      ⚠ big change\n",
      "up_blocks.1.upsamplers.0.conv.weight                          ⚠ big change\n",
      "up_blocks.1.upsamplers.0.conv.bias                            ⚠ big change\n",
      "up_blocks.2.attentions.0.norm.weight                          ⚠ big change\n",
      "up_blocks.2.attentions.0.norm.bias                            ⚠ big change\n",
      "up_blocks.2.attentions.0.proj_in.weight                       ⚠ big change\n",
      "up_blocks.2.attentions.0.proj_in.bias                         ⚠ high σ, ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight    ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias      ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight    ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias      ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight    ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias      ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias   ⚠ big change\n",
      "up_blocks.2.attentions.0.proj_out.weight                      ⚠ big change\n",
      "up_blocks.2.attentions.0.proj_out.bias                        ⚠ big change\n",
      "up_blocks.2.attentions.1.norm.weight                          ⚠ big change\n",
      "up_blocks.2.attentions.1.norm.bias                            ⚠ big change\n",
      "up_blocks.2.attentions.1.proj_in.weight                       ⚠ big change\n",
      "up_blocks.2.attentions.1.proj_in.bias                         ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight    ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias      ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight    ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias      ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight    ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias      ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias   ⚠ big change\n",
      "up_blocks.2.attentions.1.proj_out.weight                      ⚠ big change\n",
      "up_blocks.2.attentions.1.proj_out.bias                        ⚠ big change\n",
      "up_blocks.2.attentions.2.norm.weight                          ⚠ big change\n",
      "up_blocks.2.attentions.2.norm.bias                            ⚠ big change\n",
      "up_blocks.2.attentions.2.proj_in.weight                       ⚠ big change\n",
      "up_blocks.2.attentions.2.proj_in.bias                         ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight    ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias      ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight    ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias      ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight    ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias      ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias   ⚠ big change\n",
      "up_blocks.2.attentions.2.proj_out.weight                      ⚠ big change\n",
      "up_blocks.2.attentions.2.proj_out.bias                        ⚠ big change\n",
      "up_blocks.2.resnets.0.norm1.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.2.resnets.0.norm1.bias                              ⚠ high σ, ⚠ big change\n",
      "up_blocks.2.resnets.0.conv1.weight                            ⚠ big change\n",
      "up_blocks.2.resnets.0.conv1.bias                              ⚠ big change\n",
      "up_blocks.2.resnets.0.time_emb_proj.weight                    ⚠ big change\n",
      "up_blocks.2.resnets.0.time_emb_proj.bias                      ⚠ big change\n",
      "up_blocks.2.resnets.0.norm2.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.2.resnets.0.norm2.bias                              ⚠ high σ, ⚠ big change\n",
      "up_blocks.2.resnets.0.conv2.weight                            ⚠ big change\n",
      "up_blocks.2.resnets.0.conv2.bias                              ⚠ big change\n",
      "up_blocks.2.resnets.0.conv_shortcut.weight                    ⚠ big change\n",
      "up_blocks.2.resnets.0.conv_shortcut.bias                      ⚠ big change\n",
      "up_blocks.2.resnets.1.norm1.weight                            ⚠ big change\n",
      "up_blocks.2.resnets.1.norm1.bias                              ⚠ big change\n",
      "up_blocks.2.resnets.1.conv1.weight                            ⚠ big change\n",
      "up_blocks.2.resnets.1.conv1.bias                              ⚠ big change\n",
      "up_blocks.2.resnets.1.time_emb_proj.weight                    ⚠ big change\n",
      "up_blocks.2.resnets.1.time_emb_proj.bias                      ⚠ big change\n",
      "up_blocks.2.resnets.1.norm2.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.2.resnets.1.norm2.bias                              ⚠ big change\n",
      "up_blocks.2.resnets.1.conv2.weight                            ⚠ big change\n",
      "up_blocks.2.resnets.1.conv2.bias                              ⚠ big change\n",
      "up_blocks.2.resnets.1.conv_shortcut.weight                    ⚠ big change\n",
      "up_blocks.2.resnets.1.conv_shortcut.bias                      ⚠ big change\n",
      "up_blocks.2.resnets.2.norm1.weight                            ⚠ big change\n",
      "up_blocks.2.resnets.2.norm1.bias                              ⚠ big change\n",
      "up_blocks.2.resnets.2.conv1.weight                            ⚠ big change\n",
      "up_blocks.2.resnets.2.conv1.bias                              ⚠ big change\n",
      "up_blocks.2.resnets.2.time_emb_proj.weight                    ⚠ big change\n",
      "up_blocks.2.resnets.2.time_emb_proj.bias                      ⚠ big change\n",
      "up_blocks.2.resnets.2.norm2.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.2.resnets.2.norm2.bias                              ⚠ big change\n",
      "up_blocks.2.resnets.2.conv2.weight                            ⚠ big change\n",
      "up_blocks.2.resnets.2.conv2.bias                              ⚠ big change\n",
      "up_blocks.2.resnets.2.conv_shortcut.weight                    ⚠ big change\n",
      "up_blocks.2.resnets.2.conv_shortcut.bias                      ⚠ big change\n",
      "up_blocks.2.upsamplers.0.conv.weight                          ⚠ big change\n",
      "up_blocks.2.upsamplers.0.conv.bias                            ⚠ big change\n",
      "up_blocks.3.attentions.0.norm.weight                          ⚠ big change\n",
      "up_blocks.3.attentions.0.norm.bias                            ⚠ big change\n",
      "up_blocks.3.attentions.0.proj_in.weight                       ⚠ big change\n",
      "up_blocks.3.attentions.0.proj_in.bias                         ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight    ⚠ high σ, ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias      ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight    ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias      ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight    ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias      ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias   ⚠ big change\n",
      "up_blocks.3.attentions.0.proj_out.weight                      ⚠ big change\n",
      "up_blocks.3.attentions.0.proj_out.bias                        ⚠ big change\n",
      "up_blocks.3.attentions.1.norm.weight                          ⚠ big change\n",
      "up_blocks.3.attentions.1.norm.bias                            ⚠ big change\n",
      "up_blocks.3.attentions.1.proj_in.weight                       ⚠ big change\n",
      "up_blocks.3.attentions.1.proj_in.bias                         ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight    ⚠ high σ, ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias      ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight  ⚠ high σ, ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight    ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias      ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight    ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias      ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias   ⚠ big change\n",
      "up_blocks.3.attentions.1.proj_out.weight                      ⚠ big change\n",
      "up_blocks.3.attentions.1.proj_out.bias                        ⚠ big change\n",
      "up_blocks.3.attentions.2.norm.weight                          ⚠ big change\n",
      "up_blocks.3.attentions.2.norm.bias                            ⚠ big change\n",
      "up_blocks.3.attentions.2.proj_in.weight                       ⚠ big change\n",
      "up_blocks.3.attentions.2.proj_in.bias                         ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight    ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias      ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight    ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias      ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight    ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias      ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight  ⚠ big change\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias   ⚠ big change\n",
      "up_blocks.3.attentions.2.proj_out.weight                      ⚠ big change\n",
      "up_blocks.3.attentions.2.proj_out.bias                        ⚠ big change\n",
      "up_blocks.3.resnets.0.norm1.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.3.resnets.0.norm1.bias                              ⚠ high σ, ⚠ big change\n",
      "up_blocks.3.resnets.0.conv1.weight                            ⚠ big change\n",
      "up_blocks.3.resnets.0.conv1.bias                              ⚠ big change\n",
      "up_blocks.3.resnets.0.time_emb_proj.weight                    ⚠ big change\n",
      "up_blocks.3.resnets.0.time_emb_proj.bias                      ⚠ big change\n",
      "up_blocks.3.resnets.0.norm2.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.3.resnets.0.norm2.bias                              ⚠ high σ, ⚠ big change\n",
      "up_blocks.3.resnets.0.conv2.weight                            ⚠ big change\n",
      "up_blocks.3.resnets.0.conv2.bias                              ⚠ big change\n",
      "up_blocks.3.resnets.0.conv_shortcut.weight                    ⚠ big change\n",
      "up_blocks.3.resnets.0.conv_shortcut.bias                      ⚠ big change\n",
      "up_blocks.3.resnets.1.norm1.weight                            ⚠ big change\n",
      "up_blocks.3.resnets.1.norm1.bias                              ⚠ big change\n",
      "up_blocks.3.resnets.1.conv1.weight                            ⚠ big change\n",
      "up_blocks.3.resnets.1.conv1.bias                              ⚠ big change\n",
      "up_blocks.3.resnets.1.time_emb_proj.weight                    ⚠ big change\n",
      "up_blocks.3.resnets.1.time_emb_proj.bias                      ⚠ big change\n",
      "up_blocks.3.resnets.1.norm2.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.3.resnets.1.norm2.bias                              ⚠ big change\n",
      "up_blocks.3.resnets.1.conv2.weight                            ⚠ big change\n",
      "up_blocks.3.resnets.1.conv2.bias                              ⚠ big change\n",
      "up_blocks.3.resnets.1.conv_shortcut.weight                    ⚠ big change\n",
      "up_blocks.3.resnets.1.conv_shortcut.bias                      ⚠ big change\n",
      "up_blocks.3.resnets.2.norm1.weight                            ⚠ big change\n",
      "up_blocks.3.resnets.2.norm1.bias                              ⚠ big change\n",
      "up_blocks.3.resnets.2.conv1.weight                            ⚠ big change\n",
      "up_blocks.3.resnets.2.conv1.bias                              ⚠ big change\n",
      "up_blocks.3.resnets.2.time_emb_proj.weight                    ⚠ big change\n",
      "up_blocks.3.resnets.2.time_emb_proj.bias                      ⚠ big change\n",
      "up_blocks.3.resnets.2.norm2.weight                            ⚠ high σ, ⚠ big change\n",
      "up_blocks.3.resnets.2.norm2.bias                              ⚠ big change\n",
      "up_blocks.3.resnets.2.conv2.weight                            ⚠ big change\n",
      "up_blocks.3.resnets.2.conv2.bias                              ⚠ big change\n",
      "up_blocks.3.resnets.2.conv_shortcut.weight                    ⚠ big change\n",
      "up_blocks.3.resnets.2.conv_shortcut.bias                      ⚠ big change\n",
      "mid_block.attentions.0.norm.weight                            ⚠ big change\n",
      "mid_block.attentions.0.norm.bias                              ⚠ big change\n",
      "mid_block.attentions.0.proj_in.weight                         ⚠ big change\n",
      "mid_block.attentions.0.proj_in.bias                           ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1.weight      ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1.bias        ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight  ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight  ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight  ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight  ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias  ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2.weight      ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2.bias        ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight  ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight  ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight  ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight  ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias  ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3.weight      ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3.bias        ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight  ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias  ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight   ⚠ big change\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias     ⚠ big change\n",
      "mid_block.attentions.0.proj_out.weight                        ⚠ big change\n",
      "mid_block.attentions.0.proj_out.bias                          ⚠ big change\n",
      "mid_block.resnets.0.norm1.weight                              ⚠ big change\n",
      "mid_block.resnets.0.norm1.bias                                ⚠ big change\n",
      "mid_block.resnets.0.conv1.weight                              ⚠ big change\n",
      "mid_block.resnets.0.conv1.bias                                ⚠ big change\n",
      "mid_block.resnets.0.time_emb_proj.weight                      ⚠ big change\n",
      "mid_block.resnets.0.time_emb_proj.bias                        ⚠ big change\n",
      "mid_block.resnets.0.norm2.weight                              ⚠ high σ, ⚠ big change\n",
      "mid_block.resnets.0.norm2.bias                                ⚠ big change\n",
      "mid_block.resnets.0.conv2.weight                              ⚠ big change\n",
      "mid_block.resnets.0.conv2.bias                                ⚠ big change\n",
      "mid_block.resnets.1.norm1.weight                              ⚠ big change\n",
      "mid_block.resnets.1.norm1.bias                                ⚠ big change\n",
      "mid_block.resnets.1.conv1.weight                              ⚠ big change\n",
      "mid_block.resnets.1.conv1.bias                                ⚠ big change\n",
      "mid_block.resnets.1.time_emb_proj.weight                      ⚠ big change\n",
      "mid_block.resnets.1.time_emb_proj.bias                        ⚠ big change\n",
      "mid_block.resnets.1.norm2.weight                              ⚠ high σ, ⚠ big change\n",
      "mid_block.resnets.1.norm2.bias                                ⚠ big change\n",
      "mid_block.resnets.1.conv2.weight                              ⚠ big change\n",
      "mid_block.resnets.1.conv2.bias                                ⚠ big change\n",
      "conv_norm_out.weight                                          ⚠ high σ, ⚠ big change\n",
      "conv_norm_out.bias                                            ⚠ high σ, ⚠ big change\n",
      "conv_out.weight                                               ⚠ big change\n",
      "conv_out.bias                                                 ⚠ big change\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>param_count</th>\n",
       "      <th>μ_ref</th>\n",
       "      <th>σ_ref</th>\n",
       "      <th>min_ref</th>\n",
       "      <th>max_ref</th>\n",
       "      <th>μ_new</th>\n",
       "      <th>σ_new</th>\n",
       "      <th>min_new</th>\n",
       "      <th>max_new</th>\n",
       "      <th>μ_|Δ|</th>\n",
       "      <th>σ_|Δ|</th>\n",
       "      <th>min_|Δ|</th>\n",
       "      <th>max_|Δ|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>up_blocks.3.attentions.2.transformer_blocks.0....</td>\n",
       "      <td>320</td>\n",
       "      <td>0.024018</td>\n",
       "      <td>0.133561</td>\n",
       "      <td>-0.303386</td>\n",
       "      <td>1.579281</td>\n",
       "      <td>0.012923</td>\n",
       "      <td>0.057234</td>\n",
       "      <td>-0.216770</td>\n",
       "      <td>0.213553</td>\n",
       "      <td>0.033848</td>\n",
       "      <td>0.135310</td>\n",
       "      <td>2.714805e-06</td>\n",
       "      <td>1.790283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>up_blocks.3.attentions.1.transformer_blocks.0....</td>\n",
       "      <td>320</td>\n",
       "      <td>-0.031759</td>\n",
       "      <td>0.161955</td>\n",
       "      <td>-1.620620</td>\n",
       "      <td>1.262874</td>\n",
       "      <td>-0.018471</td>\n",
       "      <td>0.094012</td>\n",
       "      <td>-0.385624</td>\n",
       "      <td>0.281963</td>\n",
       "      <td>0.048523</td>\n",
       "      <td>0.138026</td>\n",
       "      <td>3.356487e-05</td>\n",
       "      <td>1.665616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>up_blocks.3.attentions.2.transformer_blocks.0....</td>\n",
       "      <td>102400</td>\n",
       "      <td>-0.000386</td>\n",
       "      <td>0.161379</td>\n",
       "      <td>-1.459340</td>\n",
       "      <td>1.528667</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>0.111513</td>\n",
       "      <td>-0.650104</td>\n",
       "      <td>0.545257</td>\n",
       "      <td>0.053955</td>\n",
       "      <td>0.075010</td>\n",
       "      <td>1.244247e-06</td>\n",
       "      <td>1.542064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>up_blocks.3.attentions.2.transformer_blocks.0....</td>\n",
       "      <td>102400</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.127889</td>\n",
       "      <td>-1.720424</td>\n",
       "      <td>1.710349</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>-0.977135</td>\n",
       "      <td>0.952446</td>\n",
       "      <td>0.029480</td>\n",
       "      <td>0.042302</td>\n",
       "      <td>3.352761e-08</td>\n",
       "      <td>1.012626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>up_blocks.1.resnets.0.time_emb_proj.weight</td>\n",
       "      <td>1638400</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>-3.386188</td>\n",
       "      <td>0.859903</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.008133</td>\n",
       "      <td>-2.853139</td>\n",
       "      <td>0.537161</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.741688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>up_blocks.3.attentions.1.transformer_blocks.0....</td>\n",
       "      <td>102400</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.135544</td>\n",
       "      <td>-1.308942</td>\n",
       "      <td>1.009944</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.119611</td>\n",
       "      <td>-0.718262</td>\n",
       "      <td>0.736607</td>\n",
       "      <td>0.026399</td>\n",
       "      <td>0.031781</td>\n",
       "      <td>7.776543e-07</td>\n",
       "      <td>0.739738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>up_blocks.3.attentions.1.transformer_blocks.0....</td>\n",
       "      <td>102400</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.157942</td>\n",
       "      <td>-0.898885</td>\n",
       "      <td>0.970055</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.126508</td>\n",
       "      <td>-0.549355</td>\n",
       "      <td>0.635191</td>\n",
       "      <td>0.046284</td>\n",
       "      <td>0.047726</td>\n",
       "      <td>5.811453e-07</td>\n",
       "      <td>0.610804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>conv_norm_out.bias</td>\n",
       "      <td>320</td>\n",
       "      <td>-0.168942</td>\n",
       "      <td>0.690046</td>\n",
       "      <td>-4.560919</td>\n",
       "      <td>0.118398</td>\n",
       "      <td>-0.186653</td>\n",
       "      <td>0.751935</td>\n",
       "      <td>-5.129172</td>\n",
       "      <td>0.129156</td>\n",
       "      <td>0.024259</td>\n",
       "      <td>0.068947</td>\n",
       "      <td>1.616031e-05</td>\n",
       "      <td>0.568253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>conv_norm_out.weight</td>\n",
       "      <td>320</td>\n",
       "      <td>0.286581</td>\n",
       "      <td>0.324463</td>\n",
       "      <td>-0.002230</td>\n",
       "      <td>2.663415</td>\n",
       "      <td>0.307049</td>\n",
       "      <td>0.358599</td>\n",
       "      <td>-0.000237</td>\n",
       "      <td>3.189737</td>\n",
       "      <td>0.024896</td>\n",
       "      <td>0.047364</td>\n",
       "      <td>3.640354e-05</td>\n",
       "      <td>0.526322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>down_blocks.2.resnets.1.time_emb_proj.weight</td>\n",
       "      <td>1638400</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.009281</td>\n",
       "      <td>-2.231521</td>\n",
       "      <td>0.413346</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.007330</td>\n",
       "      <td>-1.829977</td>\n",
       "      <td>0.318263</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>0.004223</td>\n",
       "      <td>9.313226e-10</td>\n",
       "      <td>0.401544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>up_blocks.3.attentions.2.transformer_blocks.0....</td>\n",
       "      <td>320</td>\n",
       "      <td>0.690222</td>\n",
       "      <td>0.126282</td>\n",
       "      <td>0.281554</td>\n",
       "      <td>1.013661</td>\n",
       "      <td>0.621511</td>\n",
       "      <td>0.095250</td>\n",
       "      <td>0.223552</td>\n",
       "      <td>0.842499</td>\n",
       "      <td>0.070565</td>\n",
       "      <td>0.051127</td>\n",
       "      <td>1.951456e-04</td>\n",
       "      <td>0.376749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>up_blocks.0.resnets.1.conv2.weight</td>\n",
       "      <td>14745600</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>0.024542</td>\n",
       "      <td>-1.217586</td>\n",
       "      <td>0.731385</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>0.021726</td>\n",
       "      <td>-1.091634</td>\n",
       "      <td>0.743323</td>\n",
       "      <td>0.007037</td>\n",
       "      <td>0.007601</td>\n",
       "      <td>2.328306e-10</td>\n",
       "      <td>0.352708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>up_blocks.1.resnets.0.conv1.weight</td>\n",
       "      <td>29491200</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.031210</td>\n",
       "      <td>-1.539783</td>\n",
       "      <td>0.699562</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.030176</td>\n",
       "      <td>-1.671510</td>\n",
       "      <td>0.697009</td>\n",
       "      <td>0.006710</td>\n",
       "      <td>0.007804</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.324784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>down_blocks.1.resnets.0.time_emb_proj.weight</td>\n",
       "      <td>819200</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>0.010487</td>\n",
       "      <td>-2.323008</td>\n",
       "      <td>0.347598</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.009154</td>\n",
       "      <td>-2.587089</td>\n",
       "      <td>0.339662</td>\n",
       "      <td>0.005936</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>5.587935e-09</td>\n",
       "      <td>0.318105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>up_blocks.0.resnets.2.conv1.weight</td>\n",
       "      <td>29491200</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>-1.632170</td>\n",
       "      <td>0.615485</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.018807</td>\n",
       "      <td>-1.750897</td>\n",
       "      <td>0.598570</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>4.656613e-10</td>\n",
       "      <td>0.316378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>up_blocks.3.attentions.1.transformer_blocks.0....</td>\n",
       "      <td>320</td>\n",
       "      <td>0.835837</td>\n",
       "      <td>0.178427</td>\n",
       "      <td>0.329702</td>\n",
       "      <td>1.286550</td>\n",
       "      <td>0.745813</td>\n",
       "      <td>0.154075</td>\n",
       "      <td>0.333057</td>\n",
       "      <td>1.043224</td>\n",
       "      <td>0.090045</td>\n",
       "      <td>0.045314</td>\n",
       "      <td>3.355265e-03</td>\n",
       "      <td>0.315127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>down_blocks.2.resnets.0.time_emb_proj.weight</td>\n",
       "      <td>1638400</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.011868</td>\n",
       "      <td>-4.931323</td>\n",
       "      <td>0.786368</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.009986</td>\n",
       "      <td>-5.231844</td>\n",
       "      <td>0.612580</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.300521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>up_blocks.3.resnets.2.conv2.weight</td>\n",
       "      <td>921600</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.054184</td>\n",
       "      <td>-0.513139</td>\n",
       "      <td>0.417763</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.054086</td>\n",
       "      <td>-0.561327</td>\n",
       "      <td>0.440836</td>\n",
       "      <td>0.012924</td>\n",
       "      <td>0.011567</td>\n",
       "      <td>8.149073e-10</td>\n",
       "      <td>0.298445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>up_blocks.0.resnets.0.conv2.weight</td>\n",
       "      <td>14745600</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>0.018410</td>\n",
       "      <td>-0.614533</td>\n",
       "      <td>0.742789</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>0.015940</td>\n",
       "      <td>-0.561271</td>\n",
       "      <td>0.704862</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>1.746230e-10</td>\n",
       "      <td>0.272725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>up_blocks.1.attentions.1.transformer_blocks.0....</td>\n",
       "      <td>1280</td>\n",
       "      <td>-0.001185</td>\n",
       "      <td>0.067531</td>\n",
       "      <td>-1.347583</td>\n",
       "      <td>0.660052</td>\n",
       "      <td>-0.001047</td>\n",
       "      <td>0.075030</td>\n",
       "      <td>-1.618890</td>\n",
       "      <td>0.735407</td>\n",
       "      <td>0.005425</td>\n",
       "      <td>0.008734</td>\n",
       "      <td>9.699725e-07</td>\n",
       "      <td>0.271307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 layer  param_count     μ_ref  \\\n",
       "579  up_blocks.3.attentions.2.transformer_blocks.0....          320  0.024018   \n",
       "553  up_blocks.3.attentions.1.transformer_blocks.0....          320 -0.031759   \n",
       "580  up_blocks.3.attentions.2.transformer_blocks.0....       102400 -0.000386   \n",
       "581  up_blocks.3.attentions.2.transformer_blocks.0....       102400  0.000459   \n",
       "372         up_blocks.1.resnets.0.time_emb_proj.weight      1638400  0.000016   \n",
       "555  up_blocks.3.attentions.1.transformer_blocks.0....       102400  0.000256   \n",
       "554  up_blocks.3.attentions.1.transformer_blocks.0....       102400  0.000220   \n",
       "683                                 conv_norm_out.bias          320 -0.168942   \n",
       "682                               conv_norm_out.weight          320  0.286581   \n",
       "224       down_blocks.2.resnets.1.time_emb_proj.weight      1638400  0.000120   \n",
       "578  up_blocks.3.attentions.2.transformer_blocks.0....          320  0.690222   \n",
       "272                 up_blocks.0.resnets.1.conv2.weight     14745600 -0.000201   \n",
       "370                 up_blocks.1.resnets.0.conv1.weight     29491200  0.000583   \n",
       "136       down_blocks.1.resnets.0.time_emb_proj.weight       819200 -0.000020   \n",
       "278                 up_blocks.0.resnets.2.conv1.weight     29491200  0.000007   \n",
       "552  up_blocks.3.attentions.1.transformer_blocks.0....          320  0.835837   \n",
       "212       down_blocks.2.resnets.0.time_emb_proj.weight      1638400  0.000068   \n",
       "632                 up_blocks.3.resnets.2.conv2.weight       921600  0.000243   \n",
       "260                 up_blocks.0.resnets.0.conv2.weight     14745600 -0.000268   \n",
       "326  up_blocks.1.attentions.1.transformer_blocks.0....         1280 -0.001185   \n",
       "\n",
       "        σ_ref   min_ref   max_ref     μ_new     σ_new   min_new   max_new  \\\n",
       "579  0.133561 -0.303386  1.579281  0.012923  0.057234 -0.216770  0.213553   \n",
       "553  0.161955 -1.620620  1.262874 -0.018471  0.094012 -0.385624  0.281963   \n",
       "580  0.161379 -1.459340  1.528667 -0.000151  0.111513 -0.650104  0.545257   \n",
       "581  0.127889 -1.720424  1.710349  0.000108  0.101500 -0.977135  0.952446   \n",
       "372  0.009888 -3.386188  0.859903  0.000011  0.008133 -2.853139  0.537161   \n",
       "555  0.135544 -1.308942  1.009944  0.000029  0.119611 -0.718262  0.736607   \n",
       "554  0.157942 -0.898885  0.970055  0.000230  0.126508 -0.549355  0.635191   \n",
       "683  0.690046 -4.560919  0.118398 -0.186653  0.751935 -5.129172  0.129156   \n",
       "682  0.324463 -0.002230  2.663415  0.307049  0.358599 -0.000237  3.189737   \n",
       "224  0.009281 -2.231521  0.413346  0.000158  0.007330 -1.829977  0.318263   \n",
       "578  0.126282  0.281554  1.013661  0.621511  0.095250  0.223552  0.842499   \n",
       "272  0.024542 -1.217586  0.731385 -0.000209  0.021726 -1.091634  0.743323   \n",
       "370  0.031210 -1.539783  0.699562  0.000492  0.030176 -1.671510  0.697009   \n",
       "136  0.010487 -2.323008  0.347598 -0.000010  0.009154 -2.587089  0.339662   \n",
       "278  0.021305 -1.632170  0.615485 -0.000050  0.018807 -1.750897  0.598570   \n",
       "552  0.178427  0.329702  1.286550  0.745813  0.154075  0.333057  1.043224   \n",
       "212  0.011868 -4.931323  0.786368  0.000074  0.009986 -5.231844  0.612580   \n",
       "632  0.054184 -0.513139  0.417763 -0.000037  0.054086 -0.561327  0.440836   \n",
       "260  0.018410 -0.614533  0.742789 -0.000249  0.015940 -0.561271  0.704862   \n",
       "326  0.067531 -1.347583  0.660052 -0.001047  0.075030 -1.618890  0.735407   \n",
       "\n",
       "        μ_|Δ|     σ_|Δ|       min_|Δ|   max_|Δ|  \n",
       "579  0.033848  0.135310  2.714805e-06  1.790283  \n",
       "553  0.048523  0.138026  3.356487e-05  1.665616  \n",
       "580  0.053955  0.075010  1.244247e-06  1.542064  \n",
       "581  0.029480  0.042302  3.352761e-08  1.012626  \n",
       "372  0.004040  0.004058  0.000000e+00  0.741688  \n",
       "555  0.026399  0.031781  7.776543e-07  0.739738  \n",
       "554  0.046284  0.047726  5.811453e-07  0.610804  \n",
       "683  0.024259  0.068947  1.616031e-05  0.568253  \n",
       "682  0.024896  0.047364  3.640354e-05  0.526322  \n",
       "224  0.004792  0.004223  9.313226e-10  0.401544  \n",
       "578  0.070565  0.051127  1.951456e-04  0.376749  \n",
       "272  0.007037  0.007601  2.328306e-10  0.352708  \n",
       "370  0.006710  0.007804  0.000000e+00  0.324784  \n",
       "136  0.005936  0.004834  5.587935e-09  0.318105  \n",
       "278  0.007899  0.007906  4.656613e-10  0.316378  \n",
       "552  0.090045  0.045314  3.355265e-03  0.315127  \n",
       "212  0.005894  0.004962  0.000000e+00  0.300521  \n",
       "632  0.012924  0.011567  8.149073e-10  0.298445  \n",
       "260  0.004461  0.005841  1.746230e-10  0.272725  \n",
       "326  0.005425  0.008734  9.699725e-07  0.271307  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────\n",
    "SNAP        = \"9571e7123e258cf052b4e54241f17971c290e9a8\"\n",
    "REF_UNET    = f\"/home/abradshaw/.cache/huggingface/hub/\" \\\n",
    "              f\"models--prs-eth--marigold-depth-v1-1/snapshots/{SNAP}/unet\"\n",
    "NEW_UNET    = \"./output/train_marigold_depth/checkpoint/latest/unet/\" \\\n",
    "              \"diffusion_pytorch_model.safetensors\"\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import torch, numpy as np, pandas as pd\n",
    "from diffusers import UNet2DConditionModel\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "\n",
    "# 1) load weights  ────────────────────────────────────────────────────────\n",
    "ref = UNet2DConditionModel.from_pretrained(\n",
    "        REF_UNET, torch_dtype=torch.float32, local_files_only=True\n",
    "      ).state_dict()\n",
    "\n",
    "new = load_safetensors(NEW_UNET, device=\"cpu\")\n",
    "\n",
    "# 2) per‑layer stats  ─────────────────────────────────────────────────────\n",
    "rows = []\n",
    "for name, w_ref in ref.items():\n",
    "    w_new  = new[name]\n",
    "    delta  = (w_ref - w_new).float()\n",
    "\n",
    "    def stats(t):\n",
    "        return [t.mean().item(), t.std().item(),\n",
    "                t.min().item(), t.max().item()]\n",
    "\n",
    "    rows.append([\n",
    "        name, w_ref.numel(),\n",
    "        *stats(w_ref),            # μ σ min max  (reference)\n",
    "        *stats(w_new),            # μ σ min max  (new)\n",
    "        *stats(delta.abs())       # μ σ min max  (|Δ|)\n",
    "    ])\n",
    "\n",
    "cols = [\"layer\", \"param_count\",\n",
    "        \"μ_ref\", \"σ_ref\", \"min_ref\", \"max_ref\",\n",
    "        \"μ_new\", \"σ_new\", \"min_new\", \"max_new\",\n",
    "        \"μ_|Δ|\", \"σ_|Δ|\", \"min_|Δ|\", \"max_|Δ|\"]\n",
    "\n",
    "df = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "# 3) flag suspicious layers  ─────────────────────────────────────────────\n",
    "SIGMA_LIMIT = 2      # > 2× global σ is unusual\n",
    "DELTA_LIMIT = 1e-2   # any element that moves by >1e‑2\n",
    "ABS_LIMIT   = 1e1    # weights with |value| > 10\n",
    "\n",
    "global_sigma = df[\"σ_ref\"].mean()\n",
    "flags = []\n",
    "\n",
    "for _, r in df.iterrows():\n",
    "    issues = []\n",
    "    if abs(r[\"max_new\"]) > ABS_LIMIT or abs(r[\"min_new\"]) > ABS_LIMIT:\n",
    "        issues.append(\"⚠ large magnitude\")\n",
    "    if r[\"σ_new\"] > SIGMA_LIMIT * global_sigma:\n",
    "        issues.append(\"⚠ high σ\")\n",
    "    if r[\"max_|Δ|\"] > DELTA_LIMIT:\n",
    "        issues.append(\"⚠ big change\")\n",
    "    if np.isnan(r[[\"μ_new\",\"σ_new\",\"min_new\",\"max_new\"]].astype(float).to_numpy()).any():\n",
    "        issues.append(\"❌ NaN/Inf\")\n",
    "    if issues:\n",
    "        flags.append((r[\"layer\"], \", \".join(issues)))\n",
    "\n",
    "print(\"=== Suspicious layers =================================================\")\n",
    "for name, why in flags:\n",
    "    print(f\"{name:60s}  {why}\")\n",
    "print(\"======================================================================\\n\")\n",
    "\n",
    "# 4) show 20 biggest single‑element changes  ─────────────────────────────\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "display(df.sort_values(\"max_|Δ|\", ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 layer  param_count  \\\n",
      "286         up_blocks.0.resnets.2.conv_shortcut.weight      3276800   \n",
      "579  up_blocks.3.attentions.2.transformer_blocks.0....          320   \n",
      "274         up_blocks.0.resnets.1.conv_shortcut.weight      3276800   \n",
      "685                                      conv_out.bias            4   \n",
      "553  up_blocks.3.attentions.1.transformer_blocks.0....          320   \n",
      "148       down_blocks.1.resnets.1.time_emb_proj.weight       819200   \n",
      "4                       time_embedding.linear_2.weight      1638400   \n",
      "72        down_blocks.0.resnets.1.time_emb_proj.weight       409600   \n",
      "616         up_blocks.3.resnets.1.time_emb_proj.weight       409600   \n",
      "512         up_blocks.2.resnets.2.time_emb_proj.weight       819200   \n",
      "604         up_blocks.3.resnets.0.time_emb_proj.weight       409600   \n",
      "500         up_blocks.2.resnets.1.time_emb_proj.weight       819200   \n",
      "628         up_blocks.3.resnets.2.time_emb_proj.weight       409600   \n",
      "288               up_blocks.0.upsamplers.0.conv.weight     14745600   \n",
      "136       down_blocks.1.resnets.0.time_emb_proj.weight       819200   \n",
      "\n",
      "     rel_L2_change  frac_|Δ|>0.0001  \n",
      "286         1.2041           0.9838  \n",
      "579         1.0279           0.9969  \n",
      "274         0.9482           0.9867  \n",
      "685         0.9282           1.0000  \n",
      "553         0.8866           1.0000  \n",
      "148         0.8683           0.9908  \n",
      "4           0.8671           0.9624  \n",
      "72          0.8594           0.9868  \n",
      "616         0.8465           0.9815  \n",
      "512         0.8098           0.9852  \n",
      "604         0.8068           0.9865  \n",
      "500         0.7967           0.9867  \n",
      "628         0.7866           0.9829  \n",
      "288         0.7732           0.9883  \n",
      "136         0.7300           0.9891  \n",
      "                                                 layer  param_count  \\\n",
      "286         up_blocks.0.resnets.2.conv_shortcut.weight      3276800   \n",
      "579  up_blocks.3.attentions.2.transformer_blocks.0....          320   \n",
      "274         up_blocks.0.resnets.1.conv_shortcut.weight      3276800   \n",
      "685                                      conv_out.bias            4   \n",
      "553  up_blocks.3.attentions.1.transformer_blocks.0....          320   \n",
      "148       down_blocks.1.resnets.1.time_emb_proj.weight       819200   \n",
      "4                       time_embedding.linear_2.weight      1638400   \n",
      "72        down_blocks.0.resnets.1.time_emb_proj.weight       409600   \n",
      "616         up_blocks.3.resnets.1.time_emb_proj.weight       409600   \n",
      "512         up_blocks.2.resnets.2.time_emb_proj.weight       819200   \n",
      "604         up_blocks.3.resnets.0.time_emb_proj.weight       409600   \n",
      "500         up_blocks.2.resnets.1.time_emb_proj.weight       819200   \n",
      "628         up_blocks.3.resnets.2.time_emb_proj.weight       409600   \n",
      "288               up_blocks.0.upsamplers.0.conv.weight     14745600   \n",
      "136       down_blocks.1.resnets.0.time_emb_proj.weight       819200   \n",
      "\n",
      "     rel_L2_change  frac_|Δ|>0.0001  \n",
      "286         1.2041           0.9838  \n",
      "579         1.0279           0.9969  \n",
      "274         0.9482           0.9867  \n",
      "685         0.9282           1.0000  \n",
      "553         0.8866           1.0000  \n",
      "148         0.8683           0.9908  \n",
      "4           0.8671           0.9624  \n",
      "72          0.8594           0.9868  \n",
      "616         0.8465           0.9815  \n",
      "512         0.8098           0.9852  \n",
      "604         0.8068           0.9865  \n",
      "500         0.7967           0.9867  \n",
      "628         0.7866           0.9829  \n",
      "288         0.7732           0.9883  \n",
      "136         0.7300           0.9891  \n",
      "\n",
      "Absolute |Δ| statistics across the whole UNet\n",
      " min   0.000000\n",
      " 25%   0.002497\n",
      " median 0.006380\n",
      " 75%   0.012803\n",
      " 90%   0.020498\n",
      " 95%   0.025827\n",
      " 99%   0.037258\n",
      " max   1.790372\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ╭─────────────────────────── EDIT PATHS IF NEEDED ─────────────────────╮\n",
    "SNAP  = \"9571e7123e258cf052b4e54241f17971c290e9a8\"\n",
    "REF   = f\"/home/abradshaw/.cache/huggingface/hub/\" \\\n",
    "        f\"models--prs-eth--marigold-depth-v1-1/snapshots/{SNAP}/unet\"\n",
    "NEW   = \"./output/train_marigold_depth/checkpoint/latest/unet/\" \\\n",
    "        \"diffusion_pytorch_model.safetensors\"\n",
    "THRESH = 1e-5        # any element that moved by >THRESH counts as 'changed'\n",
    "# ╰───────────────────────────────────────────────────────────────────────╯\n",
    "\n",
    "import torch, numpy as np, pandas as pd\n",
    "from diffusers import UNet2DConditionModel\n",
    "from safetensors.torch import load_file as load_st\n",
    "\n",
    "ref = UNet2DConditionModel.from_pretrained(REF, torch_dtype=torch.float32,\n",
    "                                           local_files_only=True).state_dict()\n",
    "new = load_st(NEW, device=\"cpu\")\n",
    "\n",
    "rows = []\n",
    "for k, w_ref in ref.items():\n",
    "    w_new = new[k]\n",
    "    diff  = (w_ref - w_new).float()\n",
    "    rel_l2 = torch.linalg.vector_norm(diff) / torch.linalg.vector_norm(w_ref)\n",
    "    frac_moved = (diff.abs() > THRESH).sum().item() / diff.numel()\n",
    "    rows.append((k, w_ref.numel(), rel_l2.item(), frac_moved))\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"layer\", \"param_count\",\n",
    "                                 \"rel_L2_change\", f\"frac_|Δ|>{THRESH}\"])\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "print(df.sort_values(\"rel_L2_change\", ascending=False).head(15))\n",
    "\n",
    "\n",
    "# ── print the 15 most‑changed layers (already in your code) ─────────────\n",
    "print(df.sort_values(\"rel_L2_change\", ascending=False).head(15))\n",
    "\n",
    "# ╭───────────────── NEW: DISTRIBUTION COMPARISON ─────────────────╮\n",
    "# gather *all* absolute weight differences into one big vector\n",
    "all_diffs = torch.cat([(ref[k] - new[k]).view(-1).abs() for k in ref])\n",
    "\n",
    "# quick numeric summary\n",
    "q = np.percentile(all_diffs.numpy(), [0, 25, 50, 75, 90, 95, 99, 100])\n",
    "print(\"\\nAbsolute |Δ| statistics across the whole UNet\")\n",
    "print(\" min  {:9.6f}\".format(q[0]))\n",
    "print(\" 25%  {:9.6f}\".format(q[1]))\n",
    "print(\" median{:9.6f}\".format(q[2]))\n",
    "print(\" 75%  {:9.6f}\".format(q[3]))\n",
    "print(\" 90%  {:9.6f}\".format(q[4]))\n",
    "print(\" 95%  {:9.6f}\".format(q[5]))\n",
    "print(\" 99%  {:9.6f}\".format(q[6]))\n",
    "print(\" max  {:9.6f}\".format(q[7]))\n",
    "\n",
    "# optional: visualise with a histogram  (one plot, no colour spec)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(all_diffs.numpy(), bins=100, log=True)\n",
    "plt.xlabel(\"|Δ| weight value\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Distribution of absolute weight changes\\n(new – reference)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# ╰────────────────────────────────────────────────────────────────╯\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d8779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x450 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch, numpy as np, matplotlib.pyplot as plt\n",
    "from diffusers import UNet2DConditionModel\n",
    "from safetensors.torch import load_file as load_st\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "SNAP  = \"9571e7123e258cf052b4e54241f17971c290e9a8\"\n",
    "REF   = f\"/home/abradshaw/.cache/huggingface/hub/\" \\\n",
    "        f\"models--prs-eth--marigold-depth-v1-1/snapshots/{SNAP}/unet\"\n",
    "NEW   = \"./output/train_marigold_depth/checkpoint/latest/unet/\" \\\n",
    "        \"diffusion_pytorch_model.safetensors\"\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "ref = UNet2DConditionModel.from_pretrained(REF, local_files_only=True).state_dict()\n",
    "new = load_st(NEW, device=\"cpu\")\n",
    "\n",
    "abs_ref  = torch.cat([w.abs().view(-1)             for w in ref.values()]).numpy()\n",
    "abs_diff = torch.cat([(w - new[k]).abs().view(-1)  for k, w in ref.items()]).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4.5))\n",
    "ax.hist(abs_ref , bins=200, alpha=.5, label=\"|weight| in v1‑1\")\n",
    "ax.hist(abs_diff, bins=200, alpha=.5, label=\"|Δ| after 50 iters\")\n",
    "ax.set_xscale(\"log\"); ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"magnitude\"); ax.set_ylabel(\"#parameters\")\n",
    "ax.set_title(\"Parameter distribution: reference vs. changes\")\n",
    "ax.legend(); plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdcef352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoints …\n",
      "Δ_B→Y  =  0.002   (# steps done so far)\n",
      "Δ_M→Y  =  0.307   (how far from v1‑1)\n",
      "Δ_B→M  =  0.307   (full journey v1‑1 made)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mΔ_M→Y  = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrel_L2(your,\u001b[38;5;250m \u001b[39mmari)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m   (how far from v1‑1)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mΔ_B→M  = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrel_L2(mari,\u001b[38;5;250m \u001b[39msd\u001b[38;5;250m  \u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m   (full journey v1‑1 made)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mΔ_B→M  = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43ml2_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmari\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43msd2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m, in \u001b[0;36ml2_distance\u001b[0;34m(state_A, state_B)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, wB \u001b[38;5;129;01min\u001b[39;00m state_B\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     11\u001b[0m     wA \u001b[38;5;241m=\u001b[39m state_A[k]\n\u001b[0;32m---> 12\u001b[0m     d \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((\u001b[43mwA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwB\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     13\u001b[0m     s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(wB \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (d\u001b[38;5;241m/\u001b[39ms) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch, hashlib\n",
    "from diffusers import UNet2DConditionModel\n",
    "from safetensors.torch import load_file as load_st\n",
    "\n",
    "\n",
    "# ----- edit paths -----------------------------------------------------\n",
    "BASE = \"/home/abradshaw/marigold_checkpoints/stable-diffusion-2/unet/\"\n",
    "MARI = REF\n",
    "YOUR = NEW\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "ADAPT_CONV_IN = True          # False → just skip conv_in, True → widen it\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def widen_sd_conv4_to8(w):\n",
    "    \"\"\"duplicate 4‑ch → 8‑ch and halve magnitude\"\"\"\n",
    "    return (w.repeat(1, 2, 1, 1) * 0.5).clone()\n",
    "\n",
    "def rel_L2(A, B):\n",
    "    num, den = 0.0, 0.0\n",
    "    for k, wB in B.items():\n",
    "        wA = A.get(k)\n",
    "        if wA is None:                       # should not happen\n",
    "            continue\n",
    "        if wA.shape != wB.shape:\n",
    "            if \"conv_in.weight\" in k and ADAPT_CONV_IN:\n",
    "                wB = widen_sd_conv4_to8(wB)  # make shapes match\n",
    "            else:\n",
    "                # shapes differ and we don't adapt → skip this tensor\n",
    "                continue\n",
    "        num += torch.sum((wA - wB) ** 2).item()\n",
    "        den += torch.sum(wB ** 2).item()\n",
    "    return (num / den) ** 0.5\n",
    "\n",
    "print(\"Loading checkpoints …\")\n",
    "sd   = UNet2DConditionModel.from_pretrained(BASE, local_files_only=True).state_dict()\n",
    "mari = UNet2DConditionModel.from_pretrained(MARI, local_files_only=True).state_dict()\n",
    "your = load_st(YOUR, device=\"cpu\")\n",
    "\n",
    "print(f\"Δ_B→Y  = {rel_L2(your, sd  ):6.3f}   (# steps done so far)\")\n",
    "print(f\"Δ_M→Y  = {rel_L2(your, mari):6.3f}   (how far from v1‑1)\")\n",
    "print(f\"Δ_B→M  = {rel_L2(mari, sd  ):6.3f}   (full journey v1‑1 made)\")\n",
    "print(f\"Δ_B→M  = {l2_distance(mari, sd2):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842f7516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# ---------- 0.  plumbing -------------------------------------------------\u001b[39;00m\n\u001b[1;32m     11\u001b[0m device   \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m trainer  \u001b[38;5;241m=\u001b[39m MarigoldDepthTrainer(\u001b[43mcfg\u001b[49m, model, train_dataloader, device, out_dir_ckpt, out_dir_eval, out_dir_vis, accumulation_steps)\n\u001b[1;32m     13\u001b[0m vae      \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m dl       \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain_loader      \u001b[38;5;66;03m# pick any loader you like\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from marigold.marigold_depth_pipeline import MarigoldDepthPipeline\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "from src.trainer.marigold_depth_trainer import MarigoldDepthTrainer\n",
    "\n",
    "\n",
    "# ---------- 0.  plumbing -------------------------------------------------\n",
    "device   = \"cuda\"\n",
    "trainer  = MarigoldDepthTrainer(cfg, model, train_dataloader, device, out_dir_ckpt, out_dir_eval, out_dir_vis, accumulation_steps)\n",
    "vae      = trainer.model.vae.to(device)\n",
    "dl       = trainer.train_loader      # pick any loader you like\n",
    "\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    # ---------- 1.  grab one batch --------------------------------------\n",
    "    batch          = next(iter(dl))\n",
    "    rgb_norm       = batch[\"rgb_norm\"].to(device)         # [-1,1]\n",
    "    depth_raw      = batch[trainer.gt_depth_type].to(device)\n",
    "    valid_mask     = batch[trainer.gt_mask_type].to(device)\n",
    "\n",
    "    print(\"raw depth stats :\", depth_raw.min().item(),\n",
    "                              depth_raw.max().item(),\n",
    "                              torch.median(depth_raw).item())\n",
    "\n",
    "    # ---------- 2.  depth normalisation ---------------------------------\n",
    "    q02, q98 = torch.quantile(depth_raw[valid_mask], q=(.02, .98))\n",
    "    depth_norm = (depth_raw - q02) / (q98 - q02)          # → [0,1]\n",
    "    depth_norm = depth_norm.clamp(0, 1) * 2 - 1           # → [-1,1]\n",
    "\n",
    "    print(\"depth_norm  ∈ [{:.2f},{:.2f}]\".format(\n",
    "            depth_norm.min().item(), depth_norm.max().item()))\n",
    "\n",
    "    # ---------- 3.  VAE round‑trip error -------------------------------\n",
    "    def vae_encode(img):                          # img ∈ [-1,1]\n",
    "        h      = vae.encoder(img)\n",
    "        mean   = vae.quant_conv(h).chunk(2, dim=1)[0]\n",
    "        return mean * trainer.model.latent_scale_factor\n",
    "\n",
    "    def vae_decode(lat):                          # lat in latent space\n",
    "        z      = vae.post_quant_conv(lat / trainer.model.latent_scale_factor)\n",
    "        rec    = vae.decoder(z).mean(1, keepdim=True)\n",
    "        return rec\n",
    "\n",
    "    depth_lat   = vae_encode(depth_norm)\n",
    "    depth_rec   = vae_decode(depth_lat)\n",
    "    l1_err      = (depth_rec - depth_norm).abs()[valid_mask].mean().item()\n",
    "\n",
    "    print(f\"VAE round‑trip L1 error : {l1_err:.4e}\")\n",
    "\n",
    "    # ---------- 4.  UNet+scheduler sanity on *this* batch ---------------\n",
    "    # build the same latents the trainer uses\n",
    "    rgb_lat    = vae_encode(rgb_norm)\n",
    "    noise      = torch.randn_like(depth_lat)\n",
    "    t          = torch.randint(0,\n",
    "                               trainer.model.scheduler.num_train_timesteps,\n",
    "                               (1,), device=device)\n",
    "\n",
    "    noisy_lat  = trainer.training_noise_scheduler.add_noise(depth_lat,\n",
    "                                                            noise, t)\n",
    "    cat_lat    = torch.cat([rgb_lat, noisy_lat], 1).float()\n",
    "\n",
    "    # forward UNet in full FP‑32 to see the raw output distribution\n",
    "    with torch.autocast(\"cuda\", enabled=False):\n",
    "        pred = trainer.model.unet(cat_lat, t,\n",
    "                                  trainer.model.fixed_embed.expand(1,-1,-1)\n",
    "                                  ).sample\n",
    "    print(\"UNet pred   stats :\", pred.min().item(),\n",
    "                                pred.max().item(),\n",
    "                                torch.mean(torch.isfinite(pred).float()).item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marigold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
