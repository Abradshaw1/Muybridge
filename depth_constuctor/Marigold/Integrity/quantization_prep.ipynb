{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3590ea",
   "metadata": {},
   "source": [
    "## This notebook is used to prepare the quantization data for the Marigold depth estimation model. Focusing on the preperation of the Unet and VAE models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d03c57",
   "metadata": {},
   "source": [
    "## UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719756e",
   "metadata": {},
   "source": [
    "### 1. Load the model and save as a torch.export graph for op inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c13ba2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                                                                     target                                                                   args                                                                                                                                                           kwargs\n",
      "-------------  -----------------------------------------------------------------------  -----------------------------------------------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------  ---------------------------------------------------------------------------\n",
      "placeholder    p_time_embedding_linear_1_weight                                         p_time_embedding_linear_1_weight                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_time_embedding_linear_1_bias                                           p_time_embedding_linear_1_bias                                           ()                                                                                                                                                             {}\n",
      "placeholder    p_time_embedding_linear_2_weight                                         p_time_embedding_linear_2_weight                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_time_embedding_linear_2_bias                                           p_time_embedding_linear_2_bias                                           ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_in_weight                                                         p_conv_in_weight                                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_in_bias                                                           p_conv_in_bias                                                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_norm1_weight                                   p_down_blocks_0_resnets_0_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_norm1_bias                                     p_down_blocks_0_resnets_0_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_conv1_weight                                   p_down_blocks_0_resnets_0_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_conv1_bias                                     p_down_blocks_0_resnets_0_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_time_emb_proj_weight                           p_down_blocks_0_resnets_0_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_time_emb_proj_bias                             p_down_blocks_0_resnets_0_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_norm2_weight                                   p_down_blocks_0_resnets_0_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_norm2_bias                                     p_down_blocks_0_resnets_0_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_conv2_weight                                   p_down_blocks_0_resnets_0_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_0_conv2_bias                                     p_down_blocks_0_resnets_0_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_norm_weight                                 p_down_blocks_0_attentions_0_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_norm_bias                                   p_down_blocks_0_attentions_0_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_proj_in_weight                              p_down_blocks_0_attentions_0_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_proj_in_bias                                p_down_blocks_0_attentions_0_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight           p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias             p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight           p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias             p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight           p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias             p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight        p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias          p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_proj_out_weight                             p_down_blocks_0_attentions_0_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_0_proj_out_bias                               p_down_blocks_0_attentions_0_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_norm1_weight                                   p_down_blocks_0_resnets_1_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_norm1_bias                                     p_down_blocks_0_resnets_1_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_conv1_weight                                   p_down_blocks_0_resnets_1_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_conv1_bias                                     p_down_blocks_0_resnets_1_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_time_emb_proj_weight                           p_down_blocks_0_resnets_1_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_time_emb_proj_bias                             p_down_blocks_0_resnets_1_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_norm2_weight                                   p_down_blocks_0_resnets_1_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_norm2_bias                                     p_down_blocks_0_resnets_1_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_conv2_weight                                   p_down_blocks_0_resnets_1_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_resnets_1_conv2_bias                                     p_down_blocks_0_resnets_1_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_norm_weight                                 p_down_blocks_0_attentions_1_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_norm_bias                                   p_down_blocks_0_attentions_1_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_proj_in_weight                              p_down_blocks_0_attentions_1_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_proj_in_bias                                p_down_blocks_0_attentions_1_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight           p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias             p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight           p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias             p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight           p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias             p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight        p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias          p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_proj_out_weight                             p_down_blocks_0_attentions_1_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_attentions_1_proj_out_bias                               p_down_blocks_0_attentions_1_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_downsamplers_0_conv_weight                               p_down_blocks_0_downsamplers_0_conv_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_0_downsamplers_0_conv_bias                                 p_down_blocks_0_downsamplers_0_conv_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_norm1_weight                                   p_down_blocks_1_resnets_0_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_norm1_bias                                     p_down_blocks_1_resnets_0_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv1_weight                                   p_down_blocks_1_resnets_0_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv1_bias                                     p_down_blocks_1_resnets_0_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_time_emb_proj_weight                           p_down_blocks_1_resnets_0_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_time_emb_proj_bias                             p_down_blocks_1_resnets_0_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_norm2_weight                                   p_down_blocks_1_resnets_0_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_norm2_bias                                     p_down_blocks_1_resnets_0_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv2_weight                                   p_down_blocks_1_resnets_0_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv2_bias                                     p_down_blocks_1_resnets_0_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv_shortcut_weight                           p_down_blocks_1_resnets_0_conv_shortcut_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_0_conv_shortcut_bias                             p_down_blocks_1_resnets_0_conv_shortcut_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_norm_weight                                 p_down_blocks_1_attentions_0_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_norm_bias                                   p_down_blocks_1_attentions_0_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_proj_in_weight                              p_down_blocks_1_attentions_0_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_proj_in_bias                                p_down_blocks_1_attentions_0_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight           p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias             p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight           p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias             p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight           p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias             p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight        p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias          p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_proj_out_weight                             p_down_blocks_1_attentions_0_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_0_proj_out_bias                               p_down_blocks_1_attentions_0_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_norm1_weight                                   p_down_blocks_1_resnets_1_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_norm1_bias                                     p_down_blocks_1_resnets_1_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_conv1_weight                                   p_down_blocks_1_resnets_1_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_conv1_bias                                     p_down_blocks_1_resnets_1_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_time_emb_proj_weight                           p_down_blocks_1_resnets_1_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_time_emb_proj_bias                             p_down_blocks_1_resnets_1_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_norm2_weight                                   p_down_blocks_1_resnets_1_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_norm2_bias                                     p_down_blocks_1_resnets_1_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_conv2_weight                                   p_down_blocks_1_resnets_1_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_resnets_1_conv2_bias                                     p_down_blocks_1_resnets_1_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_norm_weight                                 p_down_blocks_1_attentions_1_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_norm_bias                                   p_down_blocks_1_attentions_1_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_proj_in_weight                              p_down_blocks_1_attentions_1_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_proj_in_bias                                p_down_blocks_1_attentions_1_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight           p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias             p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight           p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias             p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight           p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias             p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight        p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias          p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_proj_out_weight                             p_down_blocks_1_attentions_1_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_attentions_1_proj_out_bias                               p_down_blocks_1_attentions_1_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_downsamplers_0_conv_weight                               p_down_blocks_1_downsamplers_0_conv_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_1_downsamplers_0_conv_bias                                 p_down_blocks_1_downsamplers_0_conv_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_norm1_weight                                   p_down_blocks_2_resnets_0_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_norm1_bias                                     p_down_blocks_2_resnets_0_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv1_weight                                   p_down_blocks_2_resnets_0_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv1_bias                                     p_down_blocks_2_resnets_0_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_time_emb_proj_weight                           p_down_blocks_2_resnets_0_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_time_emb_proj_bias                             p_down_blocks_2_resnets_0_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_norm2_weight                                   p_down_blocks_2_resnets_0_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_norm2_bias                                     p_down_blocks_2_resnets_0_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv2_weight                                   p_down_blocks_2_resnets_0_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv2_bias                                     p_down_blocks_2_resnets_0_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv_shortcut_weight                           p_down_blocks_2_resnets_0_conv_shortcut_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_0_conv_shortcut_bias                             p_down_blocks_2_resnets_0_conv_shortcut_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_norm_weight                                 p_down_blocks_2_attentions_0_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_norm_bias                                   p_down_blocks_2_attentions_0_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_proj_in_weight                              p_down_blocks_2_attentions_0_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_proj_in_bias                                p_down_blocks_2_attentions_0_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight           p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias             p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight           p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias             p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight           p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias             p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight        p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias          p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_proj_out_weight                             p_down_blocks_2_attentions_0_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_0_proj_out_bias                               p_down_blocks_2_attentions_0_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_norm1_weight                                   p_down_blocks_2_resnets_1_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_norm1_bias                                     p_down_blocks_2_resnets_1_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_conv1_weight                                   p_down_blocks_2_resnets_1_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_conv1_bias                                     p_down_blocks_2_resnets_1_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_time_emb_proj_weight                           p_down_blocks_2_resnets_1_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_time_emb_proj_bias                             p_down_blocks_2_resnets_1_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_norm2_weight                                   p_down_blocks_2_resnets_1_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_norm2_bias                                     p_down_blocks_2_resnets_1_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_conv2_weight                                   p_down_blocks_2_resnets_1_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_resnets_1_conv2_bias                                     p_down_blocks_2_resnets_1_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_norm_weight                                 p_down_blocks_2_attentions_1_norm_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_norm_bias                                   p_down_blocks_2_attentions_1_norm_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_proj_in_weight                              p_down_blocks_2_attentions_1_proj_in_weight                              ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_proj_in_bias                                p_down_blocks_2_attentions_1_proj_in_bias                                ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight           p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias             p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight           p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias             p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight      p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight  ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias    ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight           p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias             p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight        p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias          p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias          ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_proj_out_weight                             p_down_blocks_2_attentions_1_proj_out_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_attentions_1_proj_out_bias                               p_down_blocks_2_attentions_1_proj_out_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_downsamplers_0_conv_weight                               p_down_blocks_2_downsamplers_0_conv_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_2_downsamplers_0_conv_bias                                 p_down_blocks_2_downsamplers_0_conv_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_norm1_weight                                   p_down_blocks_3_resnets_0_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_norm1_bias                                     p_down_blocks_3_resnets_0_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_conv1_weight                                   p_down_blocks_3_resnets_0_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_conv1_bias                                     p_down_blocks_3_resnets_0_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_time_emb_proj_weight                           p_down_blocks_3_resnets_0_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_time_emb_proj_bias                             p_down_blocks_3_resnets_0_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_norm2_weight                                   p_down_blocks_3_resnets_0_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_norm2_bias                                     p_down_blocks_3_resnets_0_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_conv2_weight                                   p_down_blocks_3_resnets_0_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_0_conv2_bias                                     p_down_blocks_3_resnets_0_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_norm1_weight                                   p_down_blocks_3_resnets_1_norm1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_norm1_bias                                     p_down_blocks_3_resnets_1_norm1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_conv1_weight                                   p_down_blocks_3_resnets_1_conv1_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_conv1_bias                                     p_down_blocks_3_resnets_1_conv1_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_time_emb_proj_weight                           p_down_blocks_3_resnets_1_time_emb_proj_weight                           ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_time_emb_proj_bias                             p_down_blocks_3_resnets_1_time_emb_proj_bias                             ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_norm2_weight                                   p_down_blocks_3_resnets_1_norm2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_norm2_bias                                     p_down_blocks_3_resnets_1_norm2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_conv2_weight                                   p_down_blocks_3_resnets_1_conv2_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_down_blocks_3_resnets_1_conv2_bias                                     p_down_blocks_3_resnets_1_conv2_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_norm1_weight                                       p_mid_block_resnets_0_norm1_weight                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_norm1_bias                                         p_mid_block_resnets_0_norm1_bias                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_conv1_weight                                       p_mid_block_resnets_0_conv1_weight                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_conv1_bias                                         p_mid_block_resnets_0_conv1_bias                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_time_emb_proj_weight                               p_mid_block_resnets_0_time_emb_proj_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_time_emb_proj_bias                                 p_mid_block_resnets_0_time_emb_proj_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_norm2_weight                                       p_mid_block_resnets_0_norm2_weight                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_norm2_bias                                         p_mid_block_resnets_0_norm2_bias                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_conv2_weight                                       p_mid_block_resnets_0_conv2_weight                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_0_conv2_bias                                         p_mid_block_resnets_0_conv2_bias                                         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_norm_weight                                     p_mid_block_attentions_0_norm_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_norm_bias                                       p_mid_block_attentions_0_norm_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_proj_in_weight                                  p_mid_block_attentions_0_proj_in_weight                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_proj_in_bias                                    p_mid_block_attentions_0_proj_in_bias                                    ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm1_weight               p_mid_block_attentions_0_transformer_blocks_0_norm1_weight               ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm1_bias                 p_mid_block_attentions_0_transformer_blocks_0_norm1_bias                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight          p_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight          p_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight          p_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight      p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias        p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias        ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm2_weight               p_mid_block_attentions_0_transformer_blocks_0_norm2_weight               ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm2_bias                 p_mid_block_attentions_0_transformer_blocks_0_norm2_bias                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight          p_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight          p_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight          p_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight      p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight      ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias        p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias        ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm3_weight               p_mid_block_attentions_0_transformer_blocks_0_norm3_weight               ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_norm3_bias                 p_mid_block_attentions_0_transformer_blocks_0_norm3_bias                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight       p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight       ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias         p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight            p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight            ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias              p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias              ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_proj_out_weight                                 p_mid_block_attentions_0_proj_out_weight                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_attentions_0_proj_out_bias                                   p_mid_block_attentions_0_proj_out_bias                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_norm1_weight                 p_mid_block_resnets_slice_1__none__none___0_norm1_weight                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_norm1_bias                   p_mid_block_resnets_slice_1__none__none___0_norm1_bias                   ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_conv1_weight                 p_mid_block_resnets_slice_1__none__none___0_conv1_weight                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_conv1_bias                   p_mid_block_resnets_slice_1__none__none___0_conv1_bias                   ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_weight         p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_weight         ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_bias           p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_bias           ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_norm2_weight                 p_mid_block_resnets_slice_1__none__none___0_norm2_weight                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_norm2_bias                   p_mid_block_resnets_slice_1__none__none___0_norm2_bias                   ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_conv2_weight                 p_mid_block_resnets_slice_1__none__none___0_conv2_weight                 ()                                                                                                                                                             {}\n",
      "placeholder    p_mid_block_resnets_slice_1__none__none___0_conv2_bias                   p_mid_block_resnets_slice_1__none__none___0_conv2_bias                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_norm1_weight                                     p_up_blocks_0_resnets_0_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_norm1_bias                                       p_up_blocks_0_resnets_0_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv1_weight                                     p_up_blocks_0_resnets_0_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv1_bias                                       p_up_blocks_0_resnets_0_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_time_emb_proj_weight                             p_up_blocks_0_resnets_0_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_time_emb_proj_bias                               p_up_blocks_0_resnets_0_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_norm2_weight                                     p_up_blocks_0_resnets_0_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_norm2_bias                                       p_up_blocks_0_resnets_0_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv2_weight                                     p_up_blocks_0_resnets_0_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv2_bias                                       p_up_blocks_0_resnets_0_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv_shortcut_weight                             p_up_blocks_0_resnets_0_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_0_conv_shortcut_bias                               p_up_blocks_0_resnets_0_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_norm1_weight                                     p_up_blocks_0_resnets_1_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_norm1_bias                                       p_up_blocks_0_resnets_1_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv1_weight                                     p_up_blocks_0_resnets_1_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv1_bias                                       p_up_blocks_0_resnets_1_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_time_emb_proj_weight                             p_up_blocks_0_resnets_1_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_time_emb_proj_bias                               p_up_blocks_0_resnets_1_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_norm2_weight                                     p_up_blocks_0_resnets_1_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_norm2_bias                                       p_up_blocks_0_resnets_1_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv2_weight                                     p_up_blocks_0_resnets_1_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv2_bias                                       p_up_blocks_0_resnets_1_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv_shortcut_weight                             p_up_blocks_0_resnets_1_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_1_conv_shortcut_bias                               p_up_blocks_0_resnets_1_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_norm1_weight                                     p_up_blocks_0_resnets_2_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_norm1_bias                                       p_up_blocks_0_resnets_2_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv1_weight                                     p_up_blocks_0_resnets_2_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv1_bias                                       p_up_blocks_0_resnets_2_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_time_emb_proj_weight                             p_up_blocks_0_resnets_2_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_time_emb_proj_bias                               p_up_blocks_0_resnets_2_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_norm2_weight                                     p_up_blocks_0_resnets_2_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_norm2_bias                                       p_up_blocks_0_resnets_2_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv2_weight                                     p_up_blocks_0_resnets_2_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv2_bias                                       p_up_blocks_0_resnets_2_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv_shortcut_weight                             p_up_blocks_0_resnets_2_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_resnets_2_conv_shortcut_bias                               p_up_blocks_0_resnets_2_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_upsamplers_0_conv_weight                                   p_up_blocks_0_upsamplers_0_conv_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_0_upsamplers_0_conv_bias                                     p_up_blocks_0_upsamplers_0_conv_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_norm1_weight                                     p_up_blocks_1_resnets_0_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_norm1_bias                                       p_up_blocks_1_resnets_0_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv1_weight                                     p_up_blocks_1_resnets_0_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv1_bias                                       p_up_blocks_1_resnets_0_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_time_emb_proj_weight                             p_up_blocks_1_resnets_0_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_time_emb_proj_bias                               p_up_blocks_1_resnets_0_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_norm2_weight                                     p_up_blocks_1_resnets_0_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_norm2_bias                                       p_up_blocks_1_resnets_0_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv2_weight                                     p_up_blocks_1_resnets_0_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv2_bias                                       p_up_blocks_1_resnets_0_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv_shortcut_weight                             p_up_blocks_1_resnets_0_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_0_conv_shortcut_bias                               p_up_blocks_1_resnets_0_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_norm_weight                                   p_up_blocks_1_attentions_0_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_norm_bias                                     p_up_blocks_1_attentions_0_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_proj_in_weight                                p_up_blocks_1_attentions_0_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_proj_in_bias                                  p_up_blocks_1_attentions_0_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight             p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias               p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight             p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias               p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight             p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias               p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight          p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias            p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_proj_out_weight                               p_up_blocks_1_attentions_0_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_0_proj_out_bias                                 p_up_blocks_1_attentions_0_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_norm1_weight                                     p_up_blocks_1_resnets_1_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_norm1_bias                                       p_up_blocks_1_resnets_1_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv1_weight                                     p_up_blocks_1_resnets_1_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv1_bias                                       p_up_blocks_1_resnets_1_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_time_emb_proj_weight                             p_up_blocks_1_resnets_1_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_time_emb_proj_bias                               p_up_blocks_1_resnets_1_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_norm2_weight                                     p_up_blocks_1_resnets_1_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_norm2_bias                                       p_up_blocks_1_resnets_1_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv2_weight                                     p_up_blocks_1_resnets_1_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv2_bias                                       p_up_blocks_1_resnets_1_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv_shortcut_weight                             p_up_blocks_1_resnets_1_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_1_conv_shortcut_bias                               p_up_blocks_1_resnets_1_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_norm_weight                                   p_up_blocks_1_attentions_1_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_norm_bias                                     p_up_blocks_1_attentions_1_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_proj_in_weight                                p_up_blocks_1_attentions_1_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_proj_in_bias                                  p_up_blocks_1_attentions_1_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight             p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias               p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight             p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias               p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight             p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias               p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight          p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias            p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_proj_out_weight                               p_up_blocks_1_attentions_1_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_1_proj_out_bias                                 p_up_blocks_1_attentions_1_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_norm1_weight                                     p_up_blocks_1_resnets_2_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_norm1_bias                                       p_up_blocks_1_resnets_2_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv1_weight                                     p_up_blocks_1_resnets_2_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv1_bias                                       p_up_blocks_1_resnets_2_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_time_emb_proj_weight                             p_up_blocks_1_resnets_2_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_time_emb_proj_bias                               p_up_blocks_1_resnets_2_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_norm2_weight                                     p_up_blocks_1_resnets_2_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_norm2_bias                                       p_up_blocks_1_resnets_2_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv2_weight                                     p_up_blocks_1_resnets_2_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv2_bias                                       p_up_blocks_1_resnets_2_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv_shortcut_weight                             p_up_blocks_1_resnets_2_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_resnets_2_conv_shortcut_bias                               p_up_blocks_1_resnets_2_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_norm_weight                                   p_up_blocks_1_attentions_2_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_norm_bias                                     p_up_blocks_1_attentions_2_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_proj_in_weight                                p_up_blocks_1_attentions_2_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_proj_in_bias                                  p_up_blocks_1_attentions_2_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight             p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias               p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight             p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias               p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight             p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias               p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight          p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias            p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_proj_out_weight                               p_up_blocks_1_attentions_2_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_attentions_2_proj_out_bias                                 p_up_blocks_1_attentions_2_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_upsamplers_0_conv_weight                                   p_up_blocks_1_upsamplers_0_conv_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_1_upsamplers_0_conv_bias                                     p_up_blocks_1_upsamplers_0_conv_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_norm1_weight                                     p_up_blocks_2_resnets_0_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_norm1_bias                                       p_up_blocks_2_resnets_0_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv1_weight                                     p_up_blocks_2_resnets_0_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv1_bias                                       p_up_blocks_2_resnets_0_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_time_emb_proj_weight                             p_up_blocks_2_resnets_0_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_time_emb_proj_bias                               p_up_blocks_2_resnets_0_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_norm2_weight                                     p_up_blocks_2_resnets_0_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_norm2_bias                                       p_up_blocks_2_resnets_0_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv2_weight                                     p_up_blocks_2_resnets_0_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv2_bias                                       p_up_blocks_2_resnets_0_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv_shortcut_weight                             p_up_blocks_2_resnets_0_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_0_conv_shortcut_bias                               p_up_blocks_2_resnets_0_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_norm_weight                                   p_up_blocks_2_attentions_0_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_norm_bias                                     p_up_blocks_2_attentions_0_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_proj_in_weight                                p_up_blocks_2_attentions_0_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_proj_in_bias                                  p_up_blocks_2_attentions_0_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight             p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias               p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight             p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias               p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight             p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias               p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight          p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias            p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_proj_out_weight                               p_up_blocks_2_attentions_0_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_0_proj_out_bias                                 p_up_blocks_2_attentions_0_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_norm1_weight                                     p_up_blocks_2_resnets_1_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_norm1_bias                                       p_up_blocks_2_resnets_1_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv1_weight                                     p_up_blocks_2_resnets_1_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv1_bias                                       p_up_blocks_2_resnets_1_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_time_emb_proj_weight                             p_up_blocks_2_resnets_1_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_time_emb_proj_bias                               p_up_blocks_2_resnets_1_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_norm2_weight                                     p_up_blocks_2_resnets_1_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_norm2_bias                                       p_up_blocks_2_resnets_1_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv2_weight                                     p_up_blocks_2_resnets_1_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv2_bias                                       p_up_blocks_2_resnets_1_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv_shortcut_weight                             p_up_blocks_2_resnets_1_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_1_conv_shortcut_bias                               p_up_blocks_2_resnets_1_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_norm_weight                                   p_up_blocks_2_attentions_1_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_norm_bias                                     p_up_blocks_2_attentions_1_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_proj_in_weight                                p_up_blocks_2_attentions_1_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_proj_in_bias                                  p_up_blocks_2_attentions_1_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight             p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias               p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight             p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias               p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight             p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias               p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight          p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias            p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_proj_out_weight                               p_up_blocks_2_attentions_1_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_1_proj_out_bias                                 p_up_blocks_2_attentions_1_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_norm1_weight                                     p_up_blocks_2_resnets_2_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_norm1_bias                                       p_up_blocks_2_resnets_2_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv1_weight                                     p_up_blocks_2_resnets_2_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv1_bias                                       p_up_blocks_2_resnets_2_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_time_emb_proj_weight                             p_up_blocks_2_resnets_2_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_time_emb_proj_bias                               p_up_blocks_2_resnets_2_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_norm2_weight                                     p_up_blocks_2_resnets_2_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_norm2_bias                                       p_up_blocks_2_resnets_2_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv2_weight                                     p_up_blocks_2_resnets_2_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv2_bias                                       p_up_blocks_2_resnets_2_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv_shortcut_weight                             p_up_blocks_2_resnets_2_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_resnets_2_conv_shortcut_bias                               p_up_blocks_2_resnets_2_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_norm_weight                                   p_up_blocks_2_attentions_2_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_norm_bias                                     p_up_blocks_2_attentions_2_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_proj_in_weight                                p_up_blocks_2_attentions_2_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_proj_in_bias                                  p_up_blocks_2_attentions_2_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight             p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias               p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight             p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias               p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight             p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias               p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight          p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias            p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_proj_out_weight                               p_up_blocks_2_attentions_2_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_attentions_2_proj_out_bias                                 p_up_blocks_2_attentions_2_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_upsamplers_0_conv_weight                                   p_up_blocks_2_upsamplers_0_conv_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_2_upsamplers_0_conv_bias                                     p_up_blocks_2_upsamplers_0_conv_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_norm1_weight                                     p_up_blocks_3_resnets_0_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_norm1_bias                                       p_up_blocks_3_resnets_0_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv1_weight                                     p_up_blocks_3_resnets_0_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv1_bias                                       p_up_blocks_3_resnets_0_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_time_emb_proj_weight                             p_up_blocks_3_resnets_0_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_time_emb_proj_bias                               p_up_blocks_3_resnets_0_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_norm2_weight                                     p_up_blocks_3_resnets_0_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_norm2_bias                                       p_up_blocks_3_resnets_0_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv2_weight                                     p_up_blocks_3_resnets_0_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv2_bias                                       p_up_blocks_3_resnets_0_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv_shortcut_weight                             p_up_blocks_3_resnets_0_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_0_conv_shortcut_bias                               p_up_blocks_3_resnets_0_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_norm_weight                                   p_up_blocks_3_attentions_0_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_norm_bias                                     p_up_blocks_3_attentions_0_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_proj_in_weight                                p_up_blocks_3_attentions_0_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_proj_in_bias                                  p_up_blocks_3_attentions_0_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight             p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias               p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight             p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias               p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight             p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias               p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight          p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias            p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_proj_out_weight                               p_up_blocks_3_attentions_0_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_0_proj_out_bias                                 p_up_blocks_3_attentions_0_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_norm1_weight                                     p_up_blocks_3_resnets_1_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_norm1_bias                                       p_up_blocks_3_resnets_1_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv1_weight                                     p_up_blocks_3_resnets_1_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv1_bias                                       p_up_blocks_3_resnets_1_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_time_emb_proj_weight                             p_up_blocks_3_resnets_1_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_time_emb_proj_bias                               p_up_blocks_3_resnets_1_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_norm2_weight                                     p_up_blocks_3_resnets_1_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_norm2_bias                                       p_up_blocks_3_resnets_1_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv2_weight                                     p_up_blocks_3_resnets_1_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv2_bias                                       p_up_blocks_3_resnets_1_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv_shortcut_weight                             p_up_blocks_3_resnets_1_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_1_conv_shortcut_bias                               p_up_blocks_3_resnets_1_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_norm_weight                                   p_up_blocks_3_attentions_1_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_norm_bias                                     p_up_blocks_3_attentions_1_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_proj_in_weight                                p_up_blocks_3_attentions_1_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_proj_in_bias                                  p_up_blocks_3_attentions_1_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight             p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias               p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight             p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias               p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight             p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias               p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight          p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias            p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_proj_out_weight                               p_up_blocks_3_attentions_1_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_1_proj_out_bias                                 p_up_blocks_3_attentions_1_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_norm1_weight                                     p_up_blocks_3_resnets_2_norm1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_norm1_bias                                       p_up_blocks_3_resnets_2_norm1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv1_weight                                     p_up_blocks_3_resnets_2_conv1_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv1_bias                                       p_up_blocks_3_resnets_2_conv1_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_time_emb_proj_weight                             p_up_blocks_3_resnets_2_time_emb_proj_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_time_emb_proj_bias                               p_up_blocks_3_resnets_2_time_emb_proj_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_norm2_weight                                     p_up_blocks_3_resnets_2_norm2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_norm2_bias                                       p_up_blocks_3_resnets_2_norm2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv2_weight                                     p_up_blocks_3_resnets_2_conv2_weight                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv2_bias                                       p_up_blocks_3_resnets_2_conv2_bias                                       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv_shortcut_weight                             p_up_blocks_3_resnets_2_conv_shortcut_weight                             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_resnets_2_conv_shortcut_bias                               p_up_blocks_3_resnets_2_conv_shortcut_bias                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_norm_weight                                   p_up_blocks_3_attentions_2_norm_weight                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_norm_bias                                     p_up_blocks_3_attentions_2_norm_bias                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_proj_in_weight                                p_up_blocks_3_attentions_2_proj_in_weight                                ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_proj_in_bias                                  p_up_blocks_3_attentions_2_proj_in_bias                                  ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight             p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias               p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight             p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias               p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight        p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight        ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight    ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias      ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight             p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight             ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias               p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight     ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias       ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight          p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight          ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias            p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias            ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_proj_out_weight                               p_up_blocks_3_attentions_2_proj_out_weight                               ()                                                                                                                                                             {}\n",
      "placeholder    p_up_blocks_3_attentions_2_proj_out_bias                                 p_up_blocks_3_attentions_2_proj_out_bias                                 ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_norm_out_weight                                                   p_conv_norm_out_weight                                                   ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_norm_out_bias                                                     p_conv_norm_out_bias                                                     ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_out_weight                                                        p_conv_out_weight                                                        ()                                                                                                                                                             {}\n",
      "placeholder    p_conv_out_bias                                                          p_conv_out_bias                                                          ()                                                                                                                                                             {}\n",
      "placeholder    sample                                                                   sample                                                                   ()                                                                                                                                                             {}\n",
      "placeholder    timestep                                                                 timestep                                                                 ()                                                                                                                                                             {}\n",
      "placeholder    encoder_hidden_states                                                    encoder_hidden_states                                                    ()                                                                                                                                                             {}\n",
      "call_function  expand                                                                   aten.expand.default                                                      (timestep, [1])                                                                                                                                                {}\n",
      "call_function  arange                                                                   aten.arange.start                                                        (0, 160)                                                                                                                                                       {'dtype': torch.float32, 'device': device(type='cpu'), 'pin_memory': False}\n",
      "call_function  mul                                                                      aten.mul.Tensor                                                          (arange, -9.210340371976184)                                                                                                                                   {}\n",
      "call_function  div                                                                      aten.div.Tensor                                                          (mul, 160)                                                                                                                                                     {}\n",
      "call_function  exp                                                                      aten.exp.default                                                         (div,)                                                                                                                                                         {}\n",
      "call_function  slice_1                                                                  aten.slice.Tensor                                                        (expand, 0, 0, 9223372036854775807)                                                                                                                            {}\n",
      "call_function  unsqueeze                                                                aten.unsqueeze.default                                                   (slice_1, 1)                                                                                                                                                   {}\n",
      "call_function  _to_copy                                                                 aten._to_copy.default                                                    (unsqueeze,)                                                                                                                                                   {'dtype': torch.float32}\n",
      "call_function  unsqueeze_1                                                              aten.unsqueeze.default                                                   (exp, 0)                                                                                                                                                       {}\n",
      "call_function  slice_2                                                                  aten.slice.Tensor                                                        (unsqueeze_1, 1, 0, 9223372036854775807)                                                                                                                       {}\n",
      "call_function  mul_1                                                                    aten.mul.Tensor                                                          (_to_copy, slice_2)                                                                                                                                            {}\n",
      "call_function  mul_2                                                                    aten.mul.Tensor                                                          (mul_1, 1)                                                                                                                                                     {}\n",
      "call_function  sin                                                                      aten.sin.default                                                         (mul_2,)                                                                                                                                                       {}\n",
      "call_function  cos                                                                      aten.cos.default                                                         (mul_2,)                                                                                                                                                       {}\n",
      "call_function  cat                                                                      aten.cat.default                                                         ([sin, cos], -1)                                                                                                                                               {}\n",
      "call_function  slice_3                                                                  aten.slice.Tensor                                                        (cat, 0, 0, 9223372036854775807)                                                                                                                               {}\n",
      "call_function  slice_4                                                                  aten.slice.Tensor                                                        (slice_3, 1, 160, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_5                                                                  aten.slice.Tensor                                                        (cat, 0, 0, 9223372036854775807)                                                                                                                               {}\n",
      "call_function  slice_6                                                                  aten.slice.Tensor                                                        (slice_5, 1, 0, 160)                                                                                                                                           {}\n",
      "call_function  cat_1                                                                    aten.cat.default                                                         ([slice_4, slice_6], -1)                                                                                                                                       {}\n",
      "call_function  _to_copy_1                                                               aten._to_copy.default                                                    (cat_1,)                                                                                                                                                       {'dtype': torch.float32}\n",
      "call_function  linear                                                                   aten.linear.default                                                      (_to_copy_1, p_time_embedding_linear_1_weight, p_time_embedding_linear_1_bias)                                                                                 {}\n",
      "call_function  silu                                                                     aten.silu.default                                                        (linear,)                                                                                                                                                      {}\n",
      "call_function  linear_1                                                                 aten.linear.default                                                      (silu, p_time_embedding_linear_2_weight, p_time_embedding_linear_2_bias)                                                                                       {}\n",
      "call_function  conv2d                                                                   aten.conv2d.default                                                      (sample, p_conv_in_weight, p_conv_in_bias, [1, 1], [1, 1])                                                                                                     {}\n",
      "call_function  group_norm                                                               aten.group_norm.default                                                  (conv2d, 32, p_down_blocks_0_resnets_0_norm1_weight, p_down_blocks_0_resnets_0_norm1_bias)                                                                     {}\n",
      "call_function  silu_1                                                                   aten.silu.default                                                        (group_norm,)                                                                                                                                                  {}\n",
      "call_function  conv2d_1                                                                 aten.conv2d.default                                                      (silu_1, p_down_blocks_0_resnets_0_conv1_weight, p_down_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  silu_2                                                                   aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_2                                                                 aten.linear.default                                                      (silu_2, p_down_blocks_0_resnets_0_time_emb_proj_weight, p_down_blocks_0_resnets_0_time_emb_proj_bias)                                                         {}\n",
      "call_function  slice_7                                                                  aten.slice.Tensor                                                        (linear_2, 0, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  slice_8                                                                  aten.slice.Tensor                                                        (slice_7, 1, 0, 9223372036854775807)                                                                                                                           {}\n",
      "call_function  unsqueeze_2                                                              aten.unsqueeze.default                                                   (slice_8, 2)                                                                                                                                                   {}\n",
      "call_function  unsqueeze_3                                                              aten.unsqueeze.default                                                   (unsqueeze_2, 3)                                                                                                                                               {}\n",
      "call_function  add                                                                      aten.add.Tensor                                                          (conv2d_1, unsqueeze_3)                                                                                                                                        {}\n",
      "call_function  group_norm_1                                                             aten.group_norm.default                                                  (add, 32, p_down_blocks_0_resnets_0_norm2_weight, p_down_blocks_0_resnets_0_norm2_bias)                                                                        {}\n",
      "call_function  silu_3                                                                   aten.silu.default                                                        (group_norm_1,)                                                                                                                                                {}\n",
      "call_function  dropout                                                                  aten.dropout.default                                                     (silu_3, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_2                                                                 aten.conv2d.default                                                      (dropout, p_down_blocks_0_resnets_0_conv2_weight, p_down_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  add_1                                                                    aten.add.Tensor                                                          (conv2d, conv2d_2)                                                                                                                                             {}\n",
      "call_function  div_1                                                                    aten.div.Tensor                                                          (add_1, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_2                                                             aten.group_norm.default                                                  (div_1, 32, p_down_blocks_0_attentions_0_norm_weight, p_down_blocks_0_attentions_0_norm_bias, 1e-06)                                                           {}\n",
      "call_function  permute                                                                  aten.permute.default                                                     (group_norm_2, [0, 2, 3, 1])                                                                                                                                   {}\n",
      "call_function  view                                                                     aten.view.default                                                        (permute, [1, 4096, 320])                                                                                                                                      {}\n",
      "call_function  linear_3                                                                 aten.linear.default                                                      (view, p_down_blocks_0_attentions_0_proj_in_weight, p_down_blocks_0_attentions_0_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm                                                               aten.layer_norm.default                                                  (linear_3, [320], p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_norm1_bias)                {}\n",
      "call_function  linear_4                                                                 aten.linear.default                                                      (layer_norm, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                              {}\n",
      "call_function  linear_5                                                                 aten.linear.default                                                      (layer_norm, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                              {}\n",
      "call_function  linear_6                                                                 aten.linear.default                                                      (layer_norm, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                              {}\n",
      "call_function  view_1                                                                   aten.view.default                                                        (linear_4, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose                                                                aten.transpose.int                                                       (view_1, 1, 2)                                                                                                                                                 {}\n",
      "call_function  view_2                                                                   aten.view.default                                                        (linear_5, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose_1                                                              aten.transpose.int                                                       (view_2, 1, 2)                                                                                                                                                 {}\n",
      "call_function  view_3                                                                   aten.view.default                                                        (linear_6, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose_2                                                              aten.transpose.int                                                       (view_3, 1, 2)                                                                                                                                                 {}\n",
      "call_function  scaled_dot_product_attention                                             aten.scaled_dot_product_attention.default                                (transpose, transpose_1, transpose_2)                                                                                                                          {}\n",
      "call_function  transpose_3                                                              aten.transpose.int                                                       (scaled_dot_product_attention, 1, 2)                                                                                                                           {}\n",
      "call_function  view_4                                                                   aten.view.default                                                        (transpose_3, [1, -1, 320])                                                                                                                                    {}\n",
      "call_function  _to_copy_2                                                               aten._to_copy.default                                                    (view_4,)                                                                                                                                                      {'dtype': torch.float32}\n",
      "call_function  linear_7                                                                 aten.linear.default                                                      (_to_copy_2, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)   {}\n",
      "call_function  dropout_1                                                                aten.dropout.default                                                     (linear_7, 0.0, False)                                                                                                                                         {}\n",
      "call_function  div_2                                                                    aten.div.Tensor                                                          (dropout_1, 1.0)                                                                                                                                               {}\n",
      "call_function  add_2                                                                    aten.add.Tensor                                                          (div_2, linear_3)                                                                                                                                              {}\n",
      "call_function  layer_norm_1                                                             aten.layer_norm.default                                                  (add_2, [320], p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_norm2_bias)                   {}\n",
      "call_function  linear_8                                                                 aten.linear.default                                                      (layer_norm_1, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                            {}\n",
      "call_function  linear_9                                                                 aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_10                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_5                                                                   aten.view.default                                                        (linear_8, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose_4                                                              aten.transpose.int                                                       (view_5, 1, 2)                                                                                                                                                 {}\n",
      "call_function  view_6                                                                   aten.view.default                                                        (linear_9, [1, -1, 5, 64])                                                                                                                                     {}\n",
      "call_function  transpose_5                                                              aten.transpose.int                                                       (view_6, 1, 2)                                                                                                                                                 {}\n",
      "call_function  view_7                                                                   aten.view.default                                                        (linear_10, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_6                                                              aten.transpose.int                                                       (view_7, 1, 2)                                                                                                                                                 {}\n",
      "call_function  scaled_dot_product_attention_1                                           aten.scaled_dot_product_attention.default                                (transpose_4, transpose_5, transpose_6)                                                                                                                        {}\n",
      "call_function  transpose_7                                                              aten.transpose.int                                                       (scaled_dot_product_attention_1, 1, 2)                                                                                                                         {}\n",
      "call_function  view_8                                                                   aten.view.default                                                        (transpose_7, [1, -1, 320])                                                                                                                                    {}\n",
      "call_function  _to_copy_3                                                               aten._to_copy.default                                                    (view_8,)                                                                                                                                                      {'dtype': torch.float32}\n",
      "call_function  linear_11                                                                aten.linear.default                                                      (_to_copy_3, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)   {}\n",
      "call_function  dropout_2                                                                aten.dropout.default                                                     (linear_11, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_3                                                                    aten.div.Tensor                                                          (dropout_2, 1.0)                                                                                                                                               {}\n",
      "call_function  add_3                                                                    aten.add.Tensor                                                          (div_3, add_2)                                                                                                                                                 {}\n",
      "call_function  layer_norm_2                                                             aten.layer_norm.default                                                  (add_3, [320], p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_norm3_bias)                   {}\n",
      "call_function  linear_12                                                                aten.linear.default                                                      (layer_norm_2, p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)   {}\n",
      "call_function  split                                                                    aten.split.Tensor                                                        (linear_12, 1280, -1)                                                                                                                                          {}\n",
      "call_function  getitem                                                                  <built-in function getitem>                                              (split, 0)                                                                                                                                                     {}\n",
      "call_function  getitem_1                                                                <built-in function getitem>                                              (split, 1)                                                                                                                                                     {}\n",
      "call_function  gelu                                                                     aten.gelu.default                                                        (getitem_1,)                                                                                                                                                   {}\n",
      "call_function  mul_3                                                                    aten.mul.Tensor                                                          (getitem, gelu)                                                                                                                                                {}\n",
      "call_function  dropout_3                                                                aten.dropout.default                                                     (mul_3, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_13                                                                aten.linear.default                                                      (dropout_3, p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_weight, p_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2_bias)                {}\n",
      "call_function  add_4                                                                    aten.add.Tensor                                                          (linear_13, add_3)                                                                                                                                             {}\n",
      "call_function  linear_14                                                                aten.linear.default                                                      (add_4, p_down_blocks_0_attentions_0_proj_out_weight, p_down_blocks_0_attentions_0_proj_out_bias)                                                              {}\n",
      "call_function  view_9                                                                   aten.view.default                                                        (linear_14, [1, 64, 64, 320])                                                                                                                                  {}\n",
      "call_function  permute_1                                                                aten.permute.default                                                     (view_9, [0, 3, 1, 2])                                                                                                                                         {}\n",
      "call_function  clone                                                                    aten.clone.default                                                       (permute_1,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_5                                                                    aten.add.Tensor                                                          (clone, div_1)                                                                                                                                                 {}\n",
      "call_function  group_norm_3                                                             aten.group_norm.default                                                  (add_5, 32, p_down_blocks_0_resnets_1_norm1_weight, p_down_blocks_0_resnets_1_norm1_bias)                                                                      {}\n",
      "call_function  silu_4                                                                   aten.silu.default                                                        (group_norm_3,)                                                                                                                                                {}\n",
      "call_function  conv2d_3                                                                 aten.conv2d.default                                                      (silu_4, p_down_blocks_0_resnets_1_conv1_weight, p_down_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  silu_5                                                                   aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_15                                                                aten.linear.default                                                      (silu_5, p_down_blocks_0_resnets_1_time_emb_proj_weight, p_down_blocks_0_resnets_1_time_emb_proj_bias)                                                         {}\n",
      "call_function  slice_9                                                                  aten.slice.Tensor                                                        (linear_15, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_10                                                                 aten.slice.Tensor                                                        (slice_9, 1, 0, 9223372036854775807)                                                                                                                           {}\n",
      "call_function  unsqueeze_4                                                              aten.unsqueeze.default                                                   (slice_10, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_5                                                              aten.unsqueeze.default                                                   (unsqueeze_4, 3)                                                                                                                                               {}\n",
      "call_function  add_6                                                                    aten.add.Tensor                                                          (conv2d_3, unsqueeze_5)                                                                                                                                        {}\n",
      "call_function  group_norm_4                                                             aten.group_norm.default                                                  (add_6, 32, p_down_blocks_0_resnets_1_norm2_weight, p_down_blocks_0_resnets_1_norm2_bias)                                                                      {}\n",
      "call_function  silu_6                                                                   aten.silu.default                                                        (group_norm_4,)                                                                                                                                                {}\n",
      "call_function  dropout_4                                                                aten.dropout.default                                                     (silu_6, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_4                                                                 aten.conv2d.default                                                      (dropout_4, p_down_blocks_0_resnets_1_conv2_weight, p_down_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1])                                                      {}\n",
      "call_function  add_7                                                                    aten.add.Tensor                                                          (add_5, conv2d_4)                                                                                                                                              {}\n",
      "call_function  div_4                                                                    aten.div.Tensor                                                          (add_7, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_5                                                             aten.group_norm.default                                                  (div_4, 32, p_down_blocks_0_attentions_1_norm_weight, p_down_blocks_0_attentions_1_norm_bias, 1e-06)                                                           {}\n",
      "call_function  permute_2                                                                aten.permute.default                                                     (group_norm_5, [0, 2, 3, 1])                                                                                                                                   {}\n",
      "call_function  view_10                                                                  aten.view.default                                                        (permute_2, [1, 4096, 320])                                                                                                                                    {}\n",
      "call_function  linear_16                                                                aten.linear.default                                                      (view_10, p_down_blocks_0_attentions_1_proj_in_weight, p_down_blocks_0_attentions_1_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_3                                                             aten.layer_norm.default                                                  (linear_16, [320], p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_norm1_bias)               {}\n",
      "call_function  linear_17                                                                aten.linear.default                                                      (layer_norm_3, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                            {}\n",
      "call_function  linear_18                                                                aten.linear.default                                                      (layer_norm_3, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                            {}\n",
      "call_function  linear_19                                                                aten.linear.default                                                      (layer_norm_3, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                            {}\n",
      "call_function  view_11                                                                  aten.view.default                                                        (linear_17, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_8                                                              aten.transpose.int                                                       (view_11, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_12                                                                  aten.view.default                                                        (linear_18, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_9                                                              aten.transpose.int                                                       (view_12, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_13                                                                  aten.view.default                                                        (linear_19, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_10                                                             aten.transpose.int                                                       (view_13, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_2                                           aten.scaled_dot_product_attention.default                                (transpose_8, transpose_9, transpose_10)                                                                                                                       {}\n",
      "call_function  transpose_11                                                             aten.transpose.int                                                       (scaled_dot_product_attention_2, 1, 2)                                                                                                                         {}\n",
      "call_function  view_14                                                                  aten.view.default                                                        (transpose_11, [1, -1, 320])                                                                                                                                   {}\n",
      "call_function  _to_copy_4                                                               aten._to_copy.default                                                    (view_14,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_20                                                                aten.linear.default                                                      (_to_copy_4, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)   {}\n",
      "call_function  dropout_5                                                                aten.dropout.default                                                     (linear_20, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_5                                                                    aten.div.Tensor                                                          (dropout_5, 1.0)                                                                                                                                               {}\n",
      "call_function  add_8                                                                    aten.add.Tensor                                                          (div_5, linear_16)                                                                                                                                             {}\n",
      "call_function  layer_norm_4                                                             aten.layer_norm.default                                                  (add_8, [320], p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_norm2_bias)                   {}\n",
      "call_function  linear_21                                                                aten.linear.default                                                      (layer_norm_4, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                            {}\n",
      "call_function  linear_22                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_23                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_15                                                                  aten.view.default                                                        (linear_21, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_12                                                             aten.transpose.int                                                       (view_15, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_16                                                                  aten.view.default                                                        (linear_22, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_13                                                             aten.transpose.int                                                       (view_16, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_17                                                                  aten.view.default                                                        (linear_23, [1, -1, 5, 64])                                                                                                                                    {}\n",
      "call_function  transpose_14                                                             aten.transpose.int                                                       (view_17, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_3                                           aten.scaled_dot_product_attention.default                                (transpose_12, transpose_13, transpose_14)                                                                                                                     {}\n",
      "call_function  transpose_15                                                             aten.transpose.int                                                       (scaled_dot_product_attention_3, 1, 2)                                                                                                                         {}\n",
      "call_function  view_18                                                                  aten.view.default                                                        (transpose_15, [1, -1, 320])                                                                                                                                   {}\n",
      "call_function  _to_copy_5                                                               aten._to_copy.default                                                    (view_18,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_24                                                                aten.linear.default                                                      (_to_copy_5, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)   {}\n",
      "call_function  dropout_6                                                                aten.dropout.default                                                     (linear_24, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_6                                                                    aten.div.Tensor                                                          (dropout_6, 1.0)                                                                                                                                               {}\n",
      "call_function  add_9                                                                    aten.add.Tensor                                                          (div_6, add_8)                                                                                                                                                 {}\n",
      "call_function  layer_norm_5                                                             aten.layer_norm.default                                                  (add_9, [320], p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_norm3_bias)                   {}\n",
      "call_function  linear_25                                                                aten.linear.default                                                      (layer_norm_5, p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)   {}\n",
      "call_function  split_1                                                                  aten.split.Tensor                                                        (linear_25, 1280, -1)                                                                                                                                          {}\n",
      "call_function  getitem_2                                                                <built-in function getitem>                                              (split_1, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_3                                                                <built-in function getitem>                                              (split_1, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_1                                                                   aten.gelu.default                                                        (getitem_3,)                                                                                                                                                   {}\n",
      "call_function  mul_4                                                                    aten.mul.Tensor                                                          (getitem_2, gelu_1)                                                                                                                                            {}\n",
      "call_function  dropout_7                                                                aten.dropout.default                                                     (mul_4, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_26                                                                aten.linear.default                                                      (dropout_7, p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_weight, p_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2_bias)                {}\n",
      "call_function  add_10                                                                   aten.add.Tensor                                                          (linear_26, add_9)                                                                                                                                             {}\n",
      "call_function  linear_27                                                                aten.linear.default                                                      (add_10, p_down_blocks_0_attentions_1_proj_out_weight, p_down_blocks_0_attentions_1_proj_out_bias)                                                             {}\n",
      "call_function  view_19                                                                  aten.view.default                                                        (linear_27, [1, 64, 64, 320])                                                                                                                                  {}\n",
      "call_function  permute_3                                                                aten.permute.default                                                     (view_19, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_1                                                                  aten.clone.default                                                       (permute_3,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_11                                                                   aten.add.Tensor                                                          (clone_1, div_4)                                                                                                                                               {}\n",
      "call_function  conv2d_5                                                                 aten.conv2d.default                                                      (add_11, p_down_blocks_0_downsamplers_0_conv_weight, p_down_blocks_0_downsamplers_0_conv_bias, [2, 2], [1, 1])                                                 {}\n",
      "call_function  group_norm_6                                                             aten.group_norm.default                                                  (conv2d_5, 32, p_down_blocks_1_resnets_0_norm1_weight, p_down_blocks_1_resnets_0_norm1_bias)                                                                   {}\n",
      "call_function  silu_7                                                                   aten.silu.default                                                        (group_norm_6,)                                                                                                                                                {}\n",
      "call_function  conv2d_6                                                                 aten.conv2d.default                                                      (silu_7, p_down_blocks_1_resnets_0_conv1_weight, p_down_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  silu_8                                                                   aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_28                                                                aten.linear.default                                                      (silu_8, p_down_blocks_1_resnets_0_time_emb_proj_weight, p_down_blocks_1_resnets_0_time_emb_proj_bias)                                                         {}\n",
      "call_function  slice_11                                                                 aten.slice.Tensor                                                        (linear_28, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_12                                                                 aten.slice.Tensor                                                        (slice_11, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_6                                                              aten.unsqueeze.default                                                   (slice_12, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_7                                                              aten.unsqueeze.default                                                   (unsqueeze_6, 3)                                                                                                                                               {}\n",
      "call_function  add_12                                                                   aten.add.Tensor                                                          (conv2d_6, unsqueeze_7)                                                                                                                                        {}\n",
      "call_function  group_norm_7                                                             aten.group_norm.default                                                  (add_12, 32, p_down_blocks_1_resnets_0_norm2_weight, p_down_blocks_1_resnets_0_norm2_bias)                                                                     {}\n",
      "call_function  silu_9                                                                   aten.silu.default                                                        (group_norm_7,)                                                                                                                                                {}\n",
      "call_function  dropout_8                                                                aten.dropout.default                                                     (silu_9, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_7                                                                 aten.conv2d.default                                                      (dropout_8, p_down_blocks_1_resnets_0_conv2_weight, p_down_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1])                                                      {}\n",
      "call_function  conv2d_8                                                                 aten.conv2d.default                                                      (conv2d_5, p_down_blocks_1_resnets_0_conv_shortcut_weight, p_down_blocks_1_resnets_0_conv_shortcut_bias)                                                       {}\n",
      "call_function  add_13                                                                   aten.add.Tensor                                                          (conv2d_8, conv2d_7)                                                                                                                                           {}\n",
      "call_function  div_7                                                                    aten.div.Tensor                                                          (add_13, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_8                                                             aten.group_norm.default                                                  (div_7, 32, p_down_blocks_1_attentions_0_norm_weight, p_down_blocks_1_attentions_0_norm_bias, 1e-06)                                                           {}\n",
      "call_function  permute_4                                                                aten.permute.default                                                     (group_norm_8, [0, 2, 3, 1])                                                                                                                                   {}\n",
      "call_function  view_20                                                                  aten.view.default                                                        (permute_4, [1, 1024, 640])                                                                                                                                    {}\n",
      "call_function  linear_29                                                                aten.linear.default                                                      (view_20, p_down_blocks_1_attentions_0_proj_in_weight, p_down_blocks_1_attentions_0_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_6                                                             aten.layer_norm.default                                                  (linear_29, [640], p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_norm1_bias)               {}\n",
      "call_function  linear_30                                                                aten.linear.default                                                      (layer_norm_6, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                            {}\n",
      "call_function  linear_31                                                                aten.linear.default                                                      (layer_norm_6, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                            {}\n",
      "call_function  linear_32                                                                aten.linear.default                                                      (layer_norm_6, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                            {}\n",
      "call_function  view_21                                                                  aten.view.default                                                        (linear_30, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_16                                                             aten.transpose.int                                                       (view_21, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_22                                                                  aten.view.default                                                        (linear_31, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_17                                                             aten.transpose.int                                                       (view_22, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_23                                                                  aten.view.default                                                        (linear_32, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_18                                                             aten.transpose.int                                                       (view_23, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_4                                           aten.scaled_dot_product_attention.default                                (transpose_16, transpose_17, transpose_18)                                                                                                                     {}\n",
      "call_function  transpose_19                                                             aten.transpose.int                                                       (scaled_dot_product_attention_4, 1, 2)                                                                                                                         {}\n",
      "call_function  view_24                                                                  aten.view.default                                                        (transpose_19, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_6                                                               aten._to_copy.default                                                    (view_24,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_33                                                                aten.linear.default                                                      (_to_copy_6, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)   {}\n",
      "call_function  dropout_9                                                                aten.dropout.default                                                     (linear_33, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_8                                                                    aten.div.Tensor                                                          (dropout_9, 1.0)                                                                                                                                               {}\n",
      "call_function  add_14                                                                   aten.add.Tensor                                                          (div_8, linear_29)                                                                                                                                             {}\n",
      "call_function  layer_norm_7                                                             aten.layer_norm.default                                                  (add_14, [640], p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_norm2_bias)                  {}\n",
      "call_function  linear_34                                                                aten.linear.default                                                      (layer_norm_7, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                            {}\n",
      "call_function  linear_35                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_36                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_25                                                                  aten.view.default                                                        (linear_34, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_20                                                             aten.transpose.int                                                       (view_25, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_26                                                                  aten.view.default                                                        (linear_35, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_21                                                             aten.transpose.int                                                       (view_26, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_27                                                                  aten.view.default                                                        (linear_36, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_22                                                             aten.transpose.int                                                       (view_27, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_5                                           aten.scaled_dot_product_attention.default                                (transpose_20, transpose_21, transpose_22)                                                                                                                     {}\n",
      "call_function  transpose_23                                                             aten.transpose.int                                                       (scaled_dot_product_attention_5, 1, 2)                                                                                                                         {}\n",
      "call_function  view_28                                                                  aten.view.default                                                        (transpose_23, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_7                                                               aten._to_copy.default                                                    (view_28,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_37                                                                aten.linear.default                                                      (_to_copy_7, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)   {}\n",
      "call_function  dropout_10                                                               aten.dropout.default                                                     (linear_37, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_9                                                                    aten.div.Tensor                                                          (dropout_10, 1.0)                                                                                                                                              {}\n",
      "call_function  add_15                                                                   aten.add.Tensor                                                          (div_9, add_14)                                                                                                                                                {}\n",
      "call_function  layer_norm_8                                                             aten.layer_norm.default                                                  (add_15, [640], p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_norm3_bias)                  {}\n",
      "call_function  linear_38                                                                aten.linear.default                                                      (layer_norm_8, p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)   {}\n",
      "call_function  split_2                                                                  aten.split.Tensor                                                        (linear_38, 2560, -1)                                                                                                                                          {}\n",
      "call_function  getitem_4                                                                <built-in function getitem>                                              (split_2, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_5                                                                <built-in function getitem>                                              (split_2, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_2                                                                   aten.gelu.default                                                        (getitem_5,)                                                                                                                                                   {}\n",
      "call_function  mul_5                                                                    aten.mul.Tensor                                                          (getitem_4, gelu_2)                                                                                                                                            {}\n",
      "call_function  dropout_11                                                               aten.dropout.default                                                     (mul_5, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_39                                                                aten.linear.default                                                      (dropout_11, p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight, p_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias)               {}\n",
      "call_function  add_16                                                                   aten.add.Tensor                                                          (linear_39, add_15)                                                                                                                                            {}\n",
      "call_function  linear_40                                                                aten.linear.default                                                      (add_16, p_down_blocks_1_attentions_0_proj_out_weight, p_down_blocks_1_attentions_0_proj_out_bias)                                                             {}\n",
      "call_function  view_29                                                                  aten.view.default                                                        (linear_40, [1, 32, 32, 640])                                                                                                                                  {}\n",
      "call_function  permute_5                                                                aten.permute.default                                                     (view_29, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_2                                                                  aten.clone.default                                                       (permute_5,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_17                                                                   aten.add.Tensor                                                          (clone_2, div_7)                                                                                                                                               {}\n",
      "call_function  group_norm_9                                                             aten.group_norm.default                                                  (add_17, 32, p_down_blocks_1_resnets_1_norm1_weight, p_down_blocks_1_resnets_1_norm1_bias)                                                                     {}\n",
      "call_function  silu_10                                                                  aten.silu.default                                                        (group_norm_9,)                                                                                                                                                {}\n",
      "call_function  conv2d_9                                                                 aten.conv2d.default                                                      (silu_10, p_down_blocks_1_resnets_1_conv1_weight, p_down_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_11                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_41                                                                aten.linear.default                                                      (silu_11, p_down_blocks_1_resnets_1_time_emb_proj_weight, p_down_blocks_1_resnets_1_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_13                                                                 aten.slice.Tensor                                                        (linear_41, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_14                                                                 aten.slice.Tensor                                                        (slice_13, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_8                                                              aten.unsqueeze.default                                                   (slice_14, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_9                                                              aten.unsqueeze.default                                                   (unsqueeze_8, 3)                                                                                                                                               {}\n",
      "call_function  add_18                                                                   aten.add.Tensor                                                          (conv2d_9, unsqueeze_9)                                                                                                                                        {}\n",
      "call_function  group_norm_10                                                            aten.group_norm.default                                                  (add_18, 32, p_down_blocks_1_resnets_1_norm2_weight, p_down_blocks_1_resnets_1_norm2_bias)                                                                     {}\n",
      "call_function  silu_12                                                                  aten.silu.default                                                        (group_norm_10,)                                                                                                                                               {}\n",
      "call_function  dropout_12                                                               aten.dropout.default                                                     (silu_12, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_10                                                                aten.conv2d.default                                                      (dropout_12, p_down_blocks_1_resnets_1_conv2_weight, p_down_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  add_19                                                                   aten.add.Tensor                                                          (add_17, conv2d_10)                                                                                                                                            {}\n",
      "call_function  div_10                                                                   aten.div.Tensor                                                          (add_19, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_11                                                            aten.group_norm.default                                                  (div_10, 32, p_down_blocks_1_attentions_1_norm_weight, p_down_blocks_1_attentions_1_norm_bias, 1e-06)                                                          {}\n",
      "call_function  permute_6                                                                aten.permute.default                                                     (group_norm_11, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_30                                                                  aten.view.default                                                        (permute_6, [1, 1024, 640])                                                                                                                                    {}\n",
      "call_function  linear_42                                                                aten.linear.default                                                      (view_30, p_down_blocks_1_attentions_1_proj_in_weight, p_down_blocks_1_attentions_1_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_9                                                             aten.layer_norm.default                                                  (linear_42, [640], p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_norm1_bias)               {}\n",
      "call_function  linear_43                                                                aten.linear.default                                                      (layer_norm_9, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                            {}\n",
      "call_function  linear_44                                                                aten.linear.default                                                      (layer_norm_9, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                            {}\n",
      "call_function  linear_45                                                                aten.linear.default                                                      (layer_norm_9, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                            {}\n",
      "call_function  view_31                                                                  aten.view.default                                                        (linear_43, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_24                                                             aten.transpose.int                                                       (view_31, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_32                                                                  aten.view.default                                                        (linear_44, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_25                                                             aten.transpose.int                                                       (view_32, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_33                                                                  aten.view.default                                                        (linear_45, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_26                                                             aten.transpose.int                                                       (view_33, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_6                                           aten.scaled_dot_product_attention.default                                (transpose_24, transpose_25, transpose_26)                                                                                                                     {}\n",
      "call_function  transpose_27                                                             aten.transpose.int                                                       (scaled_dot_product_attention_6, 1, 2)                                                                                                                         {}\n",
      "call_function  view_34                                                                  aten.view.default                                                        (transpose_27, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_8                                                               aten._to_copy.default                                                    (view_34,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_46                                                                aten.linear.default                                                      (_to_copy_8, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)   {}\n",
      "call_function  dropout_13                                                               aten.dropout.default                                                     (linear_46, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_11                                                                   aten.div.Tensor                                                          (dropout_13, 1.0)                                                                                                                                              {}\n",
      "call_function  add_20                                                                   aten.add.Tensor                                                          (div_11, linear_42)                                                                                                                                            {}\n",
      "call_function  layer_norm_10                                                            aten.layer_norm.default                                                  (add_20, [640], p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_norm2_bias)                  {}\n",
      "call_function  linear_47                                                                aten.linear.default                                                      (layer_norm_10, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                           {}\n",
      "call_function  linear_48                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_49                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_35                                                                  aten.view.default                                                        (linear_47, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_28                                                             aten.transpose.int                                                       (view_35, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_36                                                                  aten.view.default                                                        (linear_48, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_29                                                             aten.transpose.int                                                       (view_36, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_37                                                                  aten.view.default                                                        (linear_49, [1, -1, 10, 64])                                                                                                                                   {}\n",
      "call_function  transpose_30                                                             aten.transpose.int                                                       (view_37, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_7                                           aten.scaled_dot_product_attention.default                                (transpose_28, transpose_29, transpose_30)                                                                                                                     {}\n",
      "call_function  transpose_31                                                             aten.transpose.int                                                       (scaled_dot_product_attention_7, 1, 2)                                                                                                                         {}\n",
      "call_function  view_38                                                                  aten.view.default                                                        (transpose_31, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_9                                                               aten._to_copy.default                                                    (view_38,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_50                                                                aten.linear.default                                                      (_to_copy_9, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)   {}\n",
      "call_function  dropout_14                                                               aten.dropout.default                                                     (linear_50, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_12                                                                   aten.div.Tensor                                                          (dropout_14, 1.0)                                                                                                                                              {}\n",
      "call_function  add_21                                                                   aten.add.Tensor                                                          (div_12, add_20)                                                                                                                                               {}\n",
      "call_function  layer_norm_11                                                            aten.layer_norm.default                                                  (add_21, [640], p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_norm3_bias)                  {}\n",
      "call_function  linear_51                                                                aten.linear.default                                                      (layer_norm_11, p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)  {}\n",
      "call_function  split_3                                                                  aten.split.Tensor                                                        (linear_51, 2560, -1)                                                                                                                                          {}\n",
      "call_function  getitem_6                                                                <built-in function getitem>                                              (split_3, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_7                                                                <built-in function getitem>                                              (split_3, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_3                                                                   aten.gelu.default                                                        (getitem_7,)                                                                                                                                                   {}\n",
      "call_function  mul_6                                                                    aten.mul.Tensor                                                          (getitem_6, gelu_3)                                                                                                                                            {}\n",
      "call_function  dropout_15                                                               aten.dropout.default                                                     (mul_6, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_52                                                                aten.linear.default                                                      (dropout_15, p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight, p_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias)               {}\n",
      "call_function  add_22                                                                   aten.add.Tensor                                                          (linear_52, add_21)                                                                                                                                            {}\n",
      "call_function  linear_53                                                                aten.linear.default                                                      (add_22, p_down_blocks_1_attentions_1_proj_out_weight, p_down_blocks_1_attentions_1_proj_out_bias)                                                             {}\n",
      "call_function  view_39                                                                  aten.view.default                                                        (linear_53, [1, 32, 32, 640])                                                                                                                                  {}\n",
      "call_function  permute_7                                                                aten.permute.default                                                     (view_39, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_3                                                                  aten.clone.default                                                       (permute_7,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_23                                                                   aten.add.Tensor                                                          (clone_3, div_10)                                                                                                                                              {}\n",
      "call_function  conv2d_11                                                                aten.conv2d.default                                                      (add_23, p_down_blocks_1_downsamplers_0_conv_weight, p_down_blocks_1_downsamplers_0_conv_bias, [2, 2], [1, 1])                                                 {}\n",
      "call_function  group_norm_12                                                            aten.group_norm.default                                                  (conv2d_11, 32, p_down_blocks_2_resnets_0_norm1_weight, p_down_blocks_2_resnets_0_norm1_bias)                                                                  {}\n",
      "call_function  silu_13                                                                  aten.silu.default                                                        (group_norm_12,)                                                                                                                                               {}\n",
      "call_function  conv2d_12                                                                aten.conv2d.default                                                      (silu_13, p_down_blocks_2_resnets_0_conv1_weight, p_down_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_14                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_54                                                                aten.linear.default                                                      (silu_14, p_down_blocks_2_resnets_0_time_emb_proj_weight, p_down_blocks_2_resnets_0_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_15                                                                 aten.slice.Tensor                                                        (linear_54, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_16                                                                 aten.slice.Tensor                                                        (slice_15, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_10                                                             aten.unsqueeze.default                                                   (slice_16, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_11                                                             aten.unsqueeze.default                                                   (unsqueeze_10, 3)                                                                                                                                              {}\n",
      "call_function  add_24                                                                   aten.add.Tensor                                                          (conv2d_12, unsqueeze_11)                                                                                                                                      {}\n",
      "call_function  group_norm_13                                                            aten.group_norm.default                                                  (add_24, 32, p_down_blocks_2_resnets_0_norm2_weight, p_down_blocks_2_resnets_0_norm2_bias)                                                                     {}\n",
      "call_function  silu_15                                                                  aten.silu.default                                                        (group_norm_13,)                                                                                                                                               {}\n",
      "call_function  dropout_16                                                               aten.dropout.default                                                     (silu_15, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_13                                                                aten.conv2d.default                                                      (dropout_16, p_down_blocks_2_resnets_0_conv2_weight, p_down_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  conv2d_14                                                                aten.conv2d.default                                                      (conv2d_11, p_down_blocks_2_resnets_0_conv_shortcut_weight, p_down_blocks_2_resnets_0_conv_shortcut_bias)                                                      {}\n",
      "call_function  add_25                                                                   aten.add.Tensor                                                          (conv2d_14, conv2d_13)                                                                                                                                         {}\n",
      "call_function  div_13                                                                   aten.div.Tensor                                                          (add_25, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_14                                                            aten.group_norm.default                                                  (div_13, 32, p_down_blocks_2_attentions_0_norm_weight, p_down_blocks_2_attentions_0_norm_bias, 1e-06)                                                          {}\n",
      "call_function  permute_8                                                                aten.permute.default                                                     (group_norm_14, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_40                                                                  aten.view.default                                                        (permute_8, [1, 256, 1280])                                                                                                                                    {}\n",
      "call_function  linear_55                                                                aten.linear.default                                                      (view_40, p_down_blocks_2_attentions_0_proj_in_weight, p_down_blocks_2_attentions_0_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_12                                                            aten.layer_norm.default                                                  (linear_55, [1280], p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_norm1_bias)              {}\n",
      "call_function  linear_56                                                                aten.linear.default                                                      (layer_norm_12, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                           {}\n",
      "call_function  linear_57                                                                aten.linear.default                                                      (layer_norm_12, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                           {}\n",
      "call_function  linear_58                                                                aten.linear.default                                                      (layer_norm_12, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                           {}\n",
      "call_function  view_41                                                                  aten.view.default                                                        (linear_56, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_32                                                             aten.transpose.int                                                       (view_41, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_42                                                                  aten.view.default                                                        (linear_57, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_33                                                             aten.transpose.int                                                       (view_42, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_43                                                                  aten.view.default                                                        (linear_58, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_34                                                             aten.transpose.int                                                       (view_43, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_8                                           aten.scaled_dot_product_attention.default                                (transpose_32, transpose_33, transpose_34)                                                                                                                     {}\n",
      "call_function  transpose_35                                                             aten.transpose.int                                                       (scaled_dot_product_attention_8, 1, 2)                                                                                                                         {}\n",
      "call_function  view_44                                                                  aten.view.default                                                        (transpose_35, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_10                                                              aten._to_copy.default                                                    (view_44,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_59                                                                aten.linear.default                                                      (_to_copy_10, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)  {}\n",
      "call_function  dropout_17                                                               aten.dropout.default                                                     (linear_59, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_14                                                                   aten.div.Tensor                                                          (dropout_17, 1.0)                                                                                                                                              {}\n",
      "call_function  add_26                                                                   aten.add.Tensor                                                          (div_14, linear_55)                                                                                                                                            {}\n",
      "call_function  layer_norm_13                                                            aten.layer_norm.default                                                  (add_26, [1280], p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_norm2_bias)                 {}\n",
      "call_function  linear_60                                                                aten.linear.default                                                      (layer_norm_13, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                           {}\n",
      "call_function  linear_61                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_62                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_45                                                                  aten.view.default                                                        (linear_60, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_36                                                             aten.transpose.int                                                       (view_45, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_46                                                                  aten.view.default                                                        (linear_61, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_37                                                             aten.transpose.int                                                       (view_46, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_47                                                                  aten.view.default                                                        (linear_62, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_38                                                             aten.transpose.int                                                       (view_47, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_9                                           aten.scaled_dot_product_attention.default                                (transpose_36, transpose_37, transpose_38)                                                                                                                     {}\n",
      "call_function  transpose_39                                                             aten.transpose.int                                                       (scaled_dot_product_attention_9, 1, 2)                                                                                                                         {}\n",
      "call_function  view_48                                                                  aten.view.default                                                        (transpose_39, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_11                                                              aten._to_copy.default                                                    (view_48,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_63                                                                aten.linear.default                                                      (_to_copy_11, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)  {}\n",
      "call_function  dropout_18                                                               aten.dropout.default                                                     (linear_63, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_15                                                                   aten.div.Tensor                                                          (dropout_18, 1.0)                                                                                                                                              {}\n",
      "call_function  add_27                                                                   aten.add.Tensor                                                          (div_15, add_26)                                                                                                                                               {}\n",
      "call_function  layer_norm_14                                                            aten.layer_norm.default                                                  (add_27, [1280], p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_norm3_bias)                 {}\n",
      "call_function  linear_64                                                                aten.linear.default                                                      (layer_norm_14, p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)  {}\n",
      "call_function  split_4                                                                  aten.split.Tensor                                                        (linear_64, 5120, -1)                                                                                                                                          {}\n",
      "call_function  getitem_8                                                                <built-in function getitem>                                              (split_4, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_9                                                                <built-in function getitem>                                              (split_4, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_4                                                                   aten.gelu.default                                                        (getitem_9,)                                                                                                                                                   {}\n",
      "call_function  mul_7                                                                    aten.mul.Tensor                                                          (getitem_8, gelu_4)                                                                                                                                            {}\n",
      "call_function  dropout_19                                                               aten.dropout.default                                                     (mul_7, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_65                                                                aten.linear.default                                                      (dropout_19, p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight, p_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias)               {}\n",
      "call_function  add_28                                                                   aten.add.Tensor                                                          (linear_65, add_27)                                                                                                                                            {}\n",
      "call_function  linear_66                                                                aten.linear.default                                                      (add_28, p_down_blocks_2_attentions_0_proj_out_weight, p_down_blocks_2_attentions_0_proj_out_bias)                                                             {}\n",
      "call_function  view_49                                                                  aten.view.default                                                        (linear_66, [1, 16, 16, 1280])                                                                                                                                 {}\n",
      "call_function  permute_9                                                                aten.permute.default                                                     (view_49, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_4                                                                  aten.clone.default                                                       (permute_9,)                                                                                                                                                   {'memory_format': torch.contiguous_format}\n",
      "call_function  add_29                                                                   aten.add.Tensor                                                          (clone_4, div_13)                                                                                                                                              {}\n",
      "call_function  group_norm_15                                                            aten.group_norm.default                                                  (add_29, 32, p_down_blocks_2_resnets_1_norm1_weight, p_down_blocks_2_resnets_1_norm1_bias)                                                                     {}\n",
      "call_function  silu_16                                                                  aten.silu.default                                                        (group_norm_15,)                                                                                                                                               {}\n",
      "call_function  conv2d_15                                                                aten.conv2d.default                                                      (silu_16, p_down_blocks_2_resnets_1_conv1_weight, p_down_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_17                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_67                                                                aten.linear.default                                                      (silu_17, p_down_blocks_2_resnets_1_time_emb_proj_weight, p_down_blocks_2_resnets_1_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_17                                                                 aten.slice.Tensor                                                        (linear_67, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_18                                                                 aten.slice.Tensor                                                        (slice_17, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_12                                                             aten.unsqueeze.default                                                   (slice_18, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_13                                                             aten.unsqueeze.default                                                   (unsqueeze_12, 3)                                                                                                                                              {}\n",
      "call_function  add_30                                                                   aten.add.Tensor                                                          (conv2d_15, unsqueeze_13)                                                                                                                                      {}\n",
      "call_function  group_norm_16                                                            aten.group_norm.default                                                  (add_30, 32, p_down_blocks_2_resnets_1_norm2_weight, p_down_blocks_2_resnets_1_norm2_bias)                                                                     {}\n",
      "call_function  silu_18                                                                  aten.silu.default                                                        (group_norm_16,)                                                                                                                                               {}\n",
      "call_function  dropout_20                                                               aten.dropout.default                                                     (silu_18, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_16                                                                aten.conv2d.default                                                      (dropout_20, p_down_blocks_2_resnets_1_conv2_weight, p_down_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  add_31                                                                   aten.add.Tensor                                                          (add_29, conv2d_16)                                                                                                                                            {}\n",
      "call_function  div_16                                                                   aten.div.Tensor                                                          (add_31, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_17                                                            aten.group_norm.default                                                  (div_16, 32, p_down_blocks_2_attentions_1_norm_weight, p_down_blocks_2_attentions_1_norm_bias, 1e-06)                                                          {}\n",
      "call_function  permute_10                                                               aten.permute.default                                                     (group_norm_17, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_50                                                                  aten.view.default                                                        (permute_10, [1, 256, 1280])                                                                                                                                   {}\n",
      "call_function  linear_68                                                                aten.linear.default                                                      (view_50, p_down_blocks_2_attentions_1_proj_in_weight, p_down_blocks_2_attentions_1_proj_in_bias)                                                              {}\n",
      "call_function  layer_norm_15                                                            aten.layer_norm.default                                                  (linear_68, [1280], p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_norm1_bias)              {}\n",
      "call_function  linear_69                                                                aten.linear.default                                                      (layer_norm_15, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                           {}\n",
      "call_function  linear_70                                                                aten.linear.default                                                      (layer_norm_15, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                           {}\n",
      "call_function  linear_71                                                                aten.linear.default                                                      (layer_norm_15, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                           {}\n",
      "call_function  view_51                                                                  aten.view.default                                                        (linear_69, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_40                                                             aten.transpose.int                                                       (view_51, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_52                                                                  aten.view.default                                                        (linear_70, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_41                                                             aten.transpose.int                                                       (view_52, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_53                                                                  aten.view.default                                                        (linear_71, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_42                                                             aten.transpose.int                                                       (view_53, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_10                                          aten.scaled_dot_product_attention.default                                (transpose_40, transpose_41, transpose_42)                                                                                                                     {}\n",
      "call_function  transpose_43                                                             aten.transpose.int                                                       (scaled_dot_product_attention_10, 1, 2)                                                                                                                        {}\n",
      "call_function  view_54                                                                  aten.view.default                                                        (transpose_43, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_12                                                              aten._to_copy.default                                                    (view_54,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_72                                                                aten.linear.default                                                      (_to_copy_12, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)  {}\n",
      "call_function  dropout_21                                                               aten.dropout.default                                                     (linear_72, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_17                                                                   aten.div.Tensor                                                          (dropout_21, 1.0)                                                                                                                                              {}\n",
      "call_function  add_32                                                                   aten.add.Tensor                                                          (div_17, linear_68)                                                                                                                                            {}\n",
      "call_function  layer_norm_16                                                            aten.layer_norm.default                                                  (add_32, [1280], p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_norm2_bias)                 {}\n",
      "call_function  linear_73                                                                aten.linear.default                                                      (layer_norm_16, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                           {}\n",
      "call_function  linear_74                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                   {}\n",
      "call_function  linear_75                                                                aten.linear.default                                                      (encoder_hidden_states, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                   {}\n",
      "call_function  view_55                                                                  aten.view.default                                                        (linear_73, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_44                                                             aten.transpose.int                                                       (view_55, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_56                                                                  aten.view.default                                                        (linear_74, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_45                                                             aten.transpose.int                                                       (view_56, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_57                                                                  aten.view.default                                                        (linear_75, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_46                                                             aten.transpose.int                                                       (view_57, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_11                                          aten.scaled_dot_product_attention.default                                (transpose_44, transpose_45, transpose_46)                                                                                                                     {}\n",
      "call_function  transpose_47                                                             aten.transpose.int                                                       (scaled_dot_product_attention_11, 1, 2)                                                                                                                        {}\n",
      "call_function  view_58                                                                  aten.view.default                                                        (transpose_47, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_13                                                              aten._to_copy.default                                                    (view_58,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_76                                                                aten.linear.default                                                      (_to_copy_13, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)  {}\n",
      "call_function  dropout_22                                                               aten.dropout.default                                                     (linear_76, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_18                                                                   aten.div.Tensor                                                          (dropout_22, 1.0)                                                                                                                                              {}\n",
      "call_function  add_33                                                                   aten.add.Tensor                                                          (div_18, add_32)                                                                                                                                               {}\n",
      "call_function  layer_norm_17                                                            aten.layer_norm.default                                                  (add_33, [1280], p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_norm3_bias)                 {}\n",
      "call_function  linear_77                                                                aten.linear.default                                                      (layer_norm_17, p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)  {}\n",
      "call_function  split_5                                                                  aten.split.Tensor                                                        (linear_77, 5120, -1)                                                                                                                                          {}\n",
      "call_function  getitem_10                                                               <built-in function getitem>                                              (split_5, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_11                                                               <built-in function getitem>                                              (split_5, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_5                                                                   aten.gelu.default                                                        (getitem_11,)                                                                                                                                                  {}\n",
      "call_function  mul_8                                                                    aten.mul.Tensor                                                          (getitem_10, gelu_5)                                                                                                                                           {}\n",
      "call_function  dropout_23                                                               aten.dropout.default                                                     (mul_8, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_78                                                                aten.linear.default                                                      (dropout_23, p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight, p_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias)               {}\n",
      "call_function  add_34                                                                   aten.add.Tensor                                                          (linear_78, add_33)                                                                                                                                            {}\n",
      "call_function  linear_79                                                                aten.linear.default                                                      (add_34, p_down_blocks_2_attentions_1_proj_out_weight, p_down_blocks_2_attentions_1_proj_out_bias)                                                             {}\n",
      "call_function  view_59                                                                  aten.view.default                                                        (linear_79, [1, 16, 16, 1280])                                                                                                                                 {}\n",
      "call_function  permute_11                                                               aten.permute.default                                                     (view_59, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_5                                                                  aten.clone.default                                                       (permute_11,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_35                                                                   aten.add.Tensor                                                          (clone_5, div_16)                                                                                                                                              {}\n",
      "call_function  conv2d_17                                                                aten.conv2d.default                                                      (add_35, p_down_blocks_2_downsamplers_0_conv_weight, p_down_blocks_2_downsamplers_0_conv_bias, [2, 2], [1, 1])                                                 {}\n",
      "call_function  group_norm_18                                                            aten.group_norm.default                                                  (conv2d_17, 32, p_down_blocks_3_resnets_0_norm1_weight, p_down_blocks_3_resnets_0_norm1_bias)                                                                  {}\n",
      "call_function  silu_19                                                                  aten.silu.default                                                        (group_norm_18,)                                                                                                                                               {}\n",
      "call_function  conv2d_18                                                                aten.conv2d.default                                                      (silu_19, p_down_blocks_3_resnets_0_conv1_weight, p_down_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_20                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_80                                                                aten.linear.default                                                      (silu_20, p_down_blocks_3_resnets_0_time_emb_proj_weight, p_down_blocks_3_resnets_0_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_19                                                                 aten.slice.Tensor                                                        (linear_80, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_20                                                                 aten.slice.Tensor                                                        (slice_19, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_14                                                             aten.unsqueeze.default                                                   (slice_20, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_15                                                             aten.unsqueeze.default                                                   (unsqueeze_14, 3)                                                                                                                                              {}\n",
      "call_function  add_36                                                                   aten.add.Tensor                                                          (conv2d_18, unsqueeze_15)                                                                                                                                      {}\n",
      "call_function  group_norm_19                                                            aten.group_norm.default                                                  (add_36, 32, p_down_blocks_3_resnets_0_norm2_weight, p_down_blocks_3_resnets_0_norm2_bias)                                                                     {}\n",
      "call_function  silu_21                                                                  aten.silu.default                                                        (group_norm_19,)                                                                                                                                               {}\n",
      "call_function  dropout_24                                                               aten.dropout.default                                                     (silu_21, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_19                                                                aten.conv2d.default                                                      (dropout_24, p_down_blocks_3_resnets_0_conv2_weight, p_down_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  add_37                                                                   aten.add.Tensor                                                          (conv2d_17, conv2d_19)                                                                                                                                         {}\n",
      "call_function  div_19                                                                   aten.div.Tensor                                                          (add_37, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_20                                                            aten.group_norm.default                                                  (div_19, 32, p_down_blocks_3_resnets_1_norm1_weight, p_down_blocks_3_resnets_1_norm1_bias)                                                                     {}\n",
      "call_function  silu_22                                                                  aten.silu.default                                                        (group_norm_20,)                                                                                                                                               {}\n",
      "call_function  conv2d_20                                                                aten.conv2d.default                                                      (silu_22, p_down_blocks_3_resnets_1_conv1_weight, p_down_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1])                                                        {}\n",
      "call_function  silu_23                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_81                                                                aten.linear.default                                                      (silu_23, p_down_blocks_3_resnets_1_time_emb_proj_weight, p_down_blocks_3_resnets_1_time_emb_proj_bias)                                                        {}\n",
      "call_function  slice_21                                                                 aten.slice.Tensor                                                        (linear_81, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_22                                                                 aten.slice.Tensor                                                        (slice_21, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_16                                                             aten.unsqueeze.default                                                   (slice_22, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_17                                                             aten.unsqueeze.default                                                   (unsqueeze_16, 3)                                                                                                                                              {}\n",
      "call_function  add_38                                                                   aten.add.Tensor                                                          (conv2d_20, unsqueeze_17)                                                                                                                                      {}\n",
      "call_function  group_norm_21                                                            aten.group_norm.default                                                  (add_38, 32, p_down_blocks_3_resnets_1_norm2_weight, p_down_blocks_3_resnets_1_norm2_bias)                                                                     {}\n",
      "call_function  silu_24                                                                  aten.silu.default                                                        (group_norm_21,)                                                                                                                                               {}\n",
      "call_function  dropout_25                                                               aten.dropout.default                                                     (silu_24, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_21                                                                aten.conv2d.default                                                      (dropout_25, p_down_blocks_3_resnets_1_conv2_weight, p_down_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1])                                                     {}\n",
      "call_function  add_39                                                                   aten.add.Tensor                                                          (div_19, conv2d_21)                                                                                                                                            {}\n",
      "call_function  div_20                                                                   aten.div.Tensor                                                          (add_39, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_22                                                            aten.group_norm.default                                                  (div_20, 32, p_mid_block_resnets_0_norm1_weight, p_mid_block_resnets_0_norm1_bias)                                                                             {}\n",
      "call_function  silu_25                                                                  aten.silu.default                                                        (group_norm_22,)                                                                                                                                               {}\n",
      "call_function  conv2d_22                                                                aten.conv2d.default                                                      (silu_25, p_mid_block_resnets_0_conv1_weight, p_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1])                                                                {}\n",
      "call_function  silu_26                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_82                                                                aten.linear.default                                                      (silu_26, p_mid_block_resnets_0_time_emb_proj_weight, p_mid_block_resnets_0_time_emb_proj_bias)                                                                {}\n",
      "call_function  slice_23                                                                 aten.slice.Tensor                                                        (linear_82, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_24                                                                 aten.slice.Tensor                                                        (slice_23, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_18                                                             aten.unsqueeze.default                                                   (slice_24, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_19                                                             aten.unsqueeze.default                                                   (unsqueeze_18, 3)                                                                                                                                              {}\n",
      "call_function  add_40                                                                   aten.add.Tensor                                                          (conv2d_22, unsqueeze_19)                                                                                                                                      {}\n",
      "call_function  group_norm_23                                                            aten.group_norm.default                                                  (add_40, 32, p_mid_block_resnets_0_norm2_weight, p_mid_block_resnets_0_norm2_bias)                                                                             {}\n",
      "call_function  silu_27                                                                  aten.silu.default                                                        (group_norm_23,)                                                                                                                                               {}\n",
      "call_function  dropout_26                                                               aten.dropout.default                                                     (silu_27, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_23                                                                aten.conv2d.default                                                      (dropout_26, p_mid_block_resnets_0_conv2_weight, p_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1])                                                             {}\n",
      "call_function  add_41                                                                   aten.add.Tensor                                                          (div_20, conv2d_23)                                                                                                                                            {}\n",
      "call_function  div_21                                                                   aten.div.Tensor                                                          (add_41, 1)                                                                                                                                                    {}\n",
      "call_function  group_norm_24                                                            aten.group_norm.default                                                  (div_21, 32, p_mid_block_attentions_0_norm_weight, p_mid_block_attentions_0_norm_bias, 1e-06)                                                                  {}\n",
      "call_function  permute_12                                                               aten.permute.default                                                     (group_norm_24, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_60                                                                  aten.view.default                                                        (permute_12, [1, 64, 1280])                                                                                                                                    {}\n",
      "call_function  linear_83                                                                aten.linear.default                                                      (view_60, p_mid_block_attentions_0_proj_in_weight, p_mid_block_attentions_0_proj_in_bias)                                                                      {}\n",
      "call_function  layer_norm_18                                                            aten.layer_norm.default                                                  (linear_83, [1280], p_mid_block_attentions_0_transformer_blocks_0_norm1_weight, p_mid_block_attentions_0_transformer_blocks_0_norm1_bias)                      {}\n",
      "call_function  linear_84                                                                aten.linear.default                                                      (layer_norm_18, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                               {}\n",
      "call_function  linear_85                                                                aten.linear.default                                                      (layer_norm_18, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                               {}\n",
      "call_function  linear_86                                                                aten.linear.default                                                      (layer_norm_18, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                               {}\n",
      "call_function  view_61                                                                  aten.view.default                                                        (linear_84, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_48                                                             aten.transpose.int                                                       (view_61, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_62                                                                  aten.view.default                                                        (linear_85, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_49                                                             aten.transpose.int                                                       (view_62, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_63                                                                  aten.view.default                                                        (linear_86, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_50                                                             aten.transpose.int                                                       (view_63, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_12                                          aten.scaled_dot_product_attention.default                                (transpose_48, transpose_49, transpose_50)                                                                                                                     {}\n",
      "call_function  transpose_51                                                             aten.transpose.int                                                       (scaled_dot_product_attention_12, 1, 2)                                                                                                                        {}\n",
      "call_function  view_64                                                                  aten.view.default                                                        (transpose_51, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_14                                                              aten._to_copy.default                                                    (view_64,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_87                                                                aten.linear.default                                                      (_to_copy_14, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)          {}\n",
      "call_function  dropout_27                                                               aten.dropout.default                                                     (linear_87, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_22                                                                   aten.div.Tensor                                                          (dropout_27, 1.0)                                                                                                                                              {}\n",
      "call_function  add_42                                                                   aten.add.Tensor                                                          (div_22, linear_83)                                                                                                                                            {}\n",
      "call_function  layer_norm_19                                                            aten.layer_norm.default                                                  (add_42, [1280], p_mid_block_attentions_0_transformer_blocks_0_norm2_weight, p_mid_block_attentions_0_transformer_blocks_0_norm2_bias)                         {}\n",
      "call_function  linear_88                                                                aten.linear.default                                                      (layer_norm_19, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                               {}\n",
      "call_function  linear_89                                                                aten.linear.default                                                      (encoder_hidden_states, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                       {}\n",
      "call_function  linear_90                                                                aten.linear.default                                                      (encoder_hidden_states, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                       {}\n",
      "call_function  view_65                                                                  aten.view.default                                                        (linear_88, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_52                                                             aten.transpose.int                                                       (view_65, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_66                                                                  aten.view.default                                                        (linear_89, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_53                                                             aten.transpose.int                                                       (view_66, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_67                                                                  aten.view.default                                                        (linear_90, [1, -1, 20, 64])                                                                                                                                   {}\n",
      "call_function  transpose_54                                                             aten.transpose.int                                                       (view_67, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_13                                          aten.scaled_dot_product_attention.default                                (transpose_52, transpose_53, transpose_54)                                                                                                                     {}\n",
      "call_function  transpose_55                                                             aten.transpose.int                                                       (scaled_dot_product_attention_13, 1, 2)                                                                                                                        {}\n",
      "call_function  view_68                                                                  aten.view.default                                                        (transpose_55, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_15                                                              aten._to_copy.default                                                    (view_68,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_91                                                                aten.linear.default                                                      (_to_copy_15, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)          {}\n",
      "call_function  dropout_28                                                               aten.dropout.default                                                     (linear_91, 0.0, False)                                                                                                                                        {}\n",
      "call_function  div_23                                                                   aten.div.Tensor                                                          (dropout_28, 1.0)                                                                                                                                              {}\n",
      "call_function  add_43                                                                   aten.add.Tensor                                                          (div_23, add_42)                                                                                                                                               {}\n",
      "call_function  layer_norm_20                                                            aten.layer_norm.default                                                  (add_43, [1280], p_mid_block_attentions_0_transformer_blocks_0_norm3_weight, p_mid_block_attentions_0_transformer_blocks_0_norm3_bias)                         {}\n",
      "call_function  linear_92                                                                aten.linear.default                                                      (layer_norm_20, p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)          {}\n",
      "call_function  split_6                                                                  aten.split.Tensor                                                        (linear_92, 5120, -1)                                                                                                                                          {}\n",
      "call_function  getitem_12                                                               <built-in function getitem>                                              (split_6, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_13                                                               <built-in function getitem>                                              (split_6, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_6                                                                   aten.gelu.default                                                        (getitem_13,)                                                                                                                                                  {}\n",
      "call_function  mul_9                                                                    aten.mul.Tensor                                                          (getitem_12, gelu_6)                                                                                                                                           {}\n",
      "call_function  dropout_29                                                               aten.dropout.default                                                     (mul_9, 0.0, False)                                                                                                                                            {}\n",
      "call_function  linear_93                                                                aten.linear.default                                                      (dropout_29, p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_weight, p_mid_block_attentions_0_transformer_blocks_0_ff_net_2_bias)                       {}\n",
      "call_function  add_44                                                                   aten.add.Tensor                                                          (linear_93, add_43)                                                                                                                                            {}\n",
      "call_function  linear_94                                                                aten.linear.default                                                      (add_44, p_mid_block_attentions_0_proj_out_weight, p_mid_block_attentions_0_proj_out_bias)                                                                     {}\n",
      "call_function  view_69                                                                  aten.view.default                                                        (linear_94, [1, 8, 8, 1280])                                                                                                                                   {}\n",
      "call_function  permute_13                                                               aten.permute.default                                                     (view_69, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_6                                                                  aten.clone.default                                                       (permute_13,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_45                                                                   aten.add.Tensor                                                          (clone_6, div_21)                                                                                                                                              {}\n",
      "call_function  group_norm_25                                                            aten.group_norm.default                                                  (add_45, 32, p_mid_block_resnets_slice_1__none__none___0_norm1_weight, p_mid_block_resnets_slice_1__none__none___0_norm1_bias)                                 {}\n",
      "call_function  silu_28                                                                  aten.silu.default                                                        (group_norm_25,)                                                                                                                                               {}\n",
      "call_function  conv2d_24                                                                aten.conv2d.default                                                      (silu_28, p_mid_block_resnets_slice_1__none__none___0_conv1_weight, p_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1])                    {}\n",
      "call_function  silu_29                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_95                                                                aten.linear.default                                                      (silu_29, p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_weight, p_mid_block_resnets_slice_1__none__none___0_time_emb_proj_bias)                    {}\n",
      "call_function  slice_25                                                                 aten.slice.Tensor                                                        (linear_95, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_26                                                                 aten.slice.Tensor                                                        (slice_25, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_20                                                             aten.unsqueeze.default                                                   (slice_26, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_21                                                             aten.unsqueeze.default                                                   (unsqueeze_20, 3)                                                                                                                                              {}\n",
      "call_function  add_46                                                                   aten.add.Tensor                                                          (conv2d_24, unsqueeze_21)                                                                                                                                      {}\n",
      "call_function  group_norm_26                                                            aten.group_norm.default                                                  (add_46, 32, p_mid_block_resnets_slice_1__none__none___0_norm2_weight, p_mid_block_resnets_slice_1__none__none___0_norm2_bias)                                 {}\n",
      "call_function  silu_30                                                                  aten.silu.default                                                        (group_norm_26,)                                                                                                                                               {}\n",
      "call_function  dropout_30                                                               aten.dropout.default                                                     (silu_30, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_25                                                                aten.conv2d.default                                                      (dropout_30, p_mid_block_resnets_slice_1__none__none___0_conv2_weight, p_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1])                 {}\n",
      "call_function  add_47                                                                   aten.add.Tensor                                                          (add_45, conv2d_25)                                                                                                                                            {}\n",
      "call_function  div_24                                                                   aten.div.Tensor                                                          (add_47, 1)                                                                                                                                                    {}\n",
      "call_function  cat_2                                                                    aten.cat.default                                                         ([div_24, div_20], 1)                                                                                                                                          {}\n",
      "call_function  group_norm_27                                                            aten.group_norm.default                                                  (cat_2, 32, p_up_blocks_0_resnets_0_norm1_weight, p_up_blocks_0_resnets_0_norm1_bias)                                                                          {}\n",
      "call_function  silu_31                                                                  aten.silu.default                                                        (group_norm_27,)                                                                                                                                               {}\n",
      "call_function  conv2d_26                                                                aten.conv2d.default                                                      (silu_31, p_up_blocks_0_resnets_0_conv1_weight, p_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_32                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_96                                                                aten.linear.default                                                      (silu_32, p_up_blocks_0_resnets_0_time_emb_proj_weight, p_up_blocks_0_resnets_0_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_27                                                                 aten.slice.Tensor                                                        (linear_96, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_28                                                                 aten.slice.Tensor                                                        (slice_27, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_22                                                             aten.unsqueeze.default                                                   (slice_28, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_23                                                             aten.unsqueeze.default                                                   (unsqueeze_22, 3)                                                                                                                                              {}\n",
      "call_function  add_48                                                                   aten.add.Tensor                                                          (conv2d_26, unsqueeze_23)                                                                                                                                      {}\n",
      "call_function  group_norm_28                                                            aten.group_norm.default                                                  (add_48, 32, p_up_blocks_0_resnets_0_norm2_weight, p_up_blocks_0_resnets_0_norm2_bias)                                                                         {}\n",
      "call_function  silu_33                                                                  aten.silu.default                                                        (group_norm_28,)                                                                                                                                               {}\n",
      "call_function  dropout_31                                                               aten.dropout.default                                                     (silu_33, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_27                                                                aten.conv2d.default                                                      (dropout_31, p_up_blocks_0_resnets_0_conv2_weight, p_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_28                                                                aten.conv2d.default                                                      (cat_2, p_up_blocks_0_resnets_0_conv_shortcut_weight, p_up_blocks_0_resnets_0_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_49                                                                   aten.add.Tensor                                                          (conv2d_28, conv2d_27)                                                                                                                                         {}\n",
      "call_function  div_25                                                                   aten.div.Tensor                                                          (add_49, 1.0)                                                                                                                                                  {}\n",
      "call_function  cat_3                                                                    aten.cat.default                                                         ([div_25, div_19], 1)                                                                                                                                          {}\n",
      "call_function  group_norm_29                                                            aten.group_norm.default                                                  (cat_3, 32, p_up_blocks_0_resnets_1_norm1_weight, p_up_blocks_0_resnets_1_norm1_bias)                                                                          {}\n",
      "call_function  silu_34                                                                  aten.silu.default                                                        (group_norm_29,)                                                                                                                                               {}\n",
      "call_function  conv2d_29                                                                aten.conv2d.default                                                      (silu_34, p_up_blocks_0_resnets_1_conv1_weight, p_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_35                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_97                                                                aten.linear.default                                                      (silu_35, p_up_blocks_0_resnets_1_time_emb_proj_weight, p_up_blocks_0_resnets_1_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_29                                                                 aten.slice.Tensor                                                        (linear_97, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_30                                                                 aten.slice.Tensor                                                        (slice_29, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_24                                                             aten.unsqueeze.default                                                   (slice_30, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_25                                                             aten.unsqueeze.default                                                   (unsqueeze_24, 3)                                                                                                                                              {}\n",
      "call_function  add_50                                                                   aten.add.Tensor                                                          (conv2d_29, unsqueeze_25)                                                                                                                                      {}\n",
      "call_function  group_norm_30                                                            aten.group_norm.default                                                  (add_50, 32, p_up_blocks_0_resnets_1_norm2_weight, p_up_blocks_0_resnets_1_norm2_bias)                                                                         {}\n",
      "call_function  silu_36                                                                  aten.silu.default                                                        (group_norm_30,)                                                                                                                                               {}\n",
      "call_function  dropout_32                                                               aten.dropout.default                                                     (silu_36, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_30                                                                aten.conv2d.default                                                      (dropout_32, p_up_blocks_0_resnets_1_conv2_weight, p_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_31                                                                aten.conv2d.default                                                      (cat_3, p_up_blocks_0_resnets_1_conv_shortcut_weight, p_up_blocks_0_resnets_1_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_51                                                                   aten.add.Tensor                                                          (conv2d_31, conv2d_30)                                                                                                                                         {}\n",
      "call_function  div_26                                                                   aten.div.Tensor                                                          (add_51, 1.0)                                                                                                                                                  {}\n",
      "call_function  cat_4                                                                    aten.cat.default                                                         ([div_26, conv2d_17], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_31                                                            aten.group_norm.default                                                  (cat_4, 32, p_up_blocks_0_resnets_2_norm1_weight, p_up_blocks_0_resnets_2_norm1_bias)                                                                          {}\n",
      "call_function  silu_37                                                                  aten.silu.default                                                        (group_norm_31,)                                                                                                                                               {}\n",
      "call_function  conv2d_32                                                                aten.conv2d.default                                                      (silu_37, p_up_blocks_0_resnets_2_conv1_weight, p_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_38                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_98                                                                aten.linear.default                                                      (silu_38, p_up_blocks_0_resnets_2_time_emb_proj_weight, p_up_blocks_0_resnets_2_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_31                                                                 aten.slice.Tensor                                                        (linear_98, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_32                                                                 aten.slice.Tensor                                                        (slice_31, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_26                                                             aten.unsqueeze.default                                                   (slice_32, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_27                                                             aten.unsqueeze.default                                                   (unsqueeze_26, 3)                                                                                                                                              {}\n",
      "call_function  add_52                                                                   aten.add.Tensor                                                          (conv2d_32, unsqueeze_27)                                                                                                                                      {}\n",
      "call_function  group_norm_32                                                            aten.group_norm.default                                                  (add_52, 32, p_up_blocks_0_resnets_2_norm2_weight, p_up_blocks_0_resnets_2_norm2_bias)                                                                         {}\n",
      "call_function  silu_39                                                                  aten.silu.default                                                        (group_norm_32,)                                                                                                                                               {}\n",
      "call_function  dropout_33                                                               aten.dropout.default                                                     (silu_39, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_33                                                                aten.conv2d.default                                                      (dropout_33, p_up_blocks_0_resnets_2_conv2_weight, p_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_34                                                                aten.conv2d.default                                                      (cat_4, p_up_blocks_0_resnets_2_conv_shortcut_weight, p_up_blocks_0_resnets_2_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_53                                                                   aten.add.Tensor                                                          (conv2d_34, conv2d_33)                                                                                                                                         {}\n",
      "call_function  div_27                                                                   aten.div.Tensor                                                          (add_53, 1.0)                                                                                                                                                  {}\n",
      "call_function  upsample_nearest2d                                                       aten.upsample_nearest2d.vec                                              (div_27, None, [2.0, 2.0])                                                                                                                                     {}\n",
      "call_function  conv2d_35                                                                aten.conv2d.default                                                      (upsample_nearest2d, p_up_blocks_0_upsamplers_0_conv_weight, p_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  cat_5                                                                    aten.cat.default                                                         ([conv2d_35, add_35], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_33                                                            aten.group_norm.default                                                  (cat_5, 32, p_up_blocks_1_resnets_0_norm1_weight, p_up_blocks_1_resnets_0_norm1_bias)                                                                          {}\n",
      "call_function  silu_40                                                                  aten.silu.default                                                        (group_norm_33,)                                                                                                                                               {}\n",
      "call_function  conv2d_36                                                                aten.conv2d.default                                                      (silu_40, p_up_blocks_1_resnets_0_conv1_weight, p_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_41                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_99                                                                aten.linear.default                                                      (silu_41, p_up_blocks_1_resnets_0_time_emb_proj_weight, p_up_blocks_1_resnets_0_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_33                                                                 aten.slice.Tensor                                                        (linear_99, 0, 0, 9223372036854775807)                                                                                                                         {}\n",
      "call_function  slice_34                                                                 aten.slice.Tensor                                                        (slice_33, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_28                                                             aten.unsqueeze.default                                                   (slice_34, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_29                                                             aten.unsqueeze.default                                                   (unsqueeze_28, 3)                                                                                                                                              {}\n",
      "call_function  add_54                                                                   aten.add.Tensor                                                          (conv2d_36, unsqueeze_29)                                                                                                                                      {}\n",
      "call_function  group_norm_34                                                            aten.group_norm.default                                                  (add_54, 32, p_up_blocks_1_resnets_0_norm2_weight, p_up_blocks_1_resnets_0_norm2_bias)                                                                         {}\n",
      "call_function  silu_42                                                                  aten.silu.default                                                        (group_norm_34,)                                                                                                                                               {}\n",
      "call_function  dropout_34                                                               aten.dropout.default                                                     (silu_42, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_37                                                                aten.conv2d.default                                                      (dropout_34, p_up_blocks_1_resnets_0_conv2_weight, p_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_38                                                                aten.conv2d.default                                                      (cat_5, p_up_blocks_1_resnets_0_conv_shortcut_weight, p_up_blocks_1_resnets_0_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_55                                                                   aten.add.Tensor                                                          (conv2d_38, conv2d_37)                                                                                                                                         {}\n",
      "call_function  div_28                                                                   aten.div.Tensor                                                          (add_55, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_35                                                            aten.group_norm.default                                                  (div_28, 32, p_up_blocks_1_attentions_0_norm_weight, p_up_blocks_1_attentions_0_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_14                                                               aten.permute.default                                                     (group_norm_35, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_70                                                                  aten.view.default                                                        (permute_14, [1, 256, 1280])                                                                                                                                   {}\n",
      "call_function  linear_100                                                               aten.linear.default                                                      (view_70, p_up_blocks_1_attentions_0_proj_in_weight, p_up_blocks_1_attentions_0_proj_in_bias)                                                                  {}\n",
      "call_function  layer_norm_21                                                            aten.layer_norm.default                                                  (linear_100, [1280], p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_norm1_bias)                 {}\n",
      "call_function  linear_101                                                               aten.linear.default                                                      (layer_norm_21, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_102                                                               aten.linear.default                                                      (layer_norm_21, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_103                                                               aten.linear.default                                                      (layer_norm_21, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_71                                                                  aten.view.default                                                        (linear_101, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_56                                                             aten.transpose.int                                                       (view_71, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_72                                                                  aten.view.default                                                        (linear_102, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_57                                                             aten.transpose.int                                                       (view_72, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_73                                                                  aten.view.default                                                        (linear_103, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_58                                                             aten.transpose.int                                                       (view_73, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_14                                          aten.scaled_dot_product_attention.default                                (transpose_56, transpose_57, transpose_58)                                                                                                                     {}\n",
      "call_function  transpose_59                                                             aten.transpose.int                                                       (scaled_dot_product_attention_14, 1, 2)                                                                                                                        {}\n",
      "call_function  view_74                                                                  aten.view.default                                                        (transpose_59, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_16                                                              aten._to_copy.default                                                    (view_74,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_104                                                               aten.linear.default                                                      (_to_copy_16, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_35                                                               aten.dropout.default                                                     (linear_104, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_29                                                                   aten.div.Tensor                                                          (dropout_35, 1.0)                                                                                                                                              {}\n",
      "call_function  add_56                                                                   aten.add.Tensor                                                          (div_29, linear_100)                                                                                                                                           {}\n",
      "call_function  layer_norm_22                                                            aten.layer_norm.default                                                  (add_56, [1280], p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_norm2_bias)                     {}\n",
      "call_function  linear_105                                                               aten.linear.default                                                      (layer_norm_22, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_106                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_107                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_75                                                                  aten.view.default                                                        (linear_105, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_60                                                             aten.transpose.int                                                       (view_75, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_76                                                                  aten.view.default                                                        (linear_106, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_61                                                             aten.transpose.int                                                       (view_76, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_77                                                                  aten.view.default                                                        (linear_107, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_62                                                             aten.transpose.int                                                       (view_77, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_15                                          aten.scaled_dot_product_attention.default                                (transpose_60, transpose_61, transpose_62)                                                                                                                     {}\n",
      "call_function  transpose_63                                                             aten.transpose.int                                                       (scaled_dot_product_attention_15, 1, 2)                                                                                                                        {}\n",
      "call_function  view_78                                                                  aten.view.default                                                        (transpose_63, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_17                                                              aten._to_copy.default                                                    (view_78,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_108                                                               aten.linear.default                                                      (_to_copy_17, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_36                                                               aten.dropout.default                                                     (linear_108, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_30                                                                   aten.div.Tensor                                                          (dropout_36, 1.0)                                                                                                                                              {}\n",
      "call_function  add_57                                                                   aten.add.Tensor                                                          (div_30, add_56)                                                                                                                                               {}\n",
      "call_function  layer_norm_23                                                            aten.layer_norm.default                                                  (add_57, [1280], p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_norm3_bias)                     {}\n",
      "call_function  linear_109                                                               aten.linear.default                                                      (layer_norm_23, p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_7                                                                  aten.split.Tensor                                                        (linear_109, 5120, -1)                                                                                                                                         {}\n",
      "call_function  getitem_14                                                               <built-in function getitem>                                              (split_7, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_15                                                               <built-in function getitem>                                              (split_7, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_7                                                                   aten.gelu.default                                                        (getitem_15,)                                                                                                                                                  {}\n",
      "call_function  mul_10                                                                   aten.mul.Tensor                                                          (getitem_14, gelu_7)                                                                                                                                           {}\n",
      "call_function  dropout_37                                                               aten.dropout.default                                                     (mul_10, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_110                                                               aten.linear.default                                                      (dropout_37, p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_weight, p_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_58                                                                   aten.add.Tensor                                                          (linear_110, add_57)                                                                                                                                           {}\n",
      "call_function  linear_111                                                               aten.linear.default                                                      (add_58, p_up_blocks_1_attentions_0_proj_out_weight, p_up_blocks_1_attentions_0_proj_out_bias)                                                                 {}\n",
      "call_function  view_79                                                                  aten.view.default                                                        (linear_111, [1, 16, 16, 1280])                                                                                                                                {}\n",
      "call_function  permute_15                                                               aten.permute.default                                                     (view_79, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_7                                                                  aten.clone.default                                                       (permute_15,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_59                                                                   aten.add.Tensor                                                          (clone_7, div_28)                                                                                                                                              {}\n",
      "call_function  cat_6                                                                    aten.cat.default                                                         ([add_59, add_29], 1)                                                                                                                                          {}\n",
      "call_function  group_norm_36                                                            aten.group_norm.default                                                  (cat_6, 32, p_up_blocks_1_resnets_1_norm1_weight, p_up_blocks_1_resnets_1_norm1_bias)                                                                          {}\n",
      "call_function  silu_43                                                                  aten.silu.default                                                        (group_norm_36,)                                                                                                                                               {}\n",
      "call_function  conv2d_39                                                                aten.conv2d.default                                                      (silu_43, p_up_blocks_1_resnets_1_conv1_weight, p_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_44                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_112                                                               aten.linear.default                                                      (silu_44, p_up_blocks_1_resnets_1_time_emb_proj_weight, p_up_blocks_1_resnets_1_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_35                                                                 aten.slice.Tensor                                                        (linear_112, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_36                                                                 aten.slice.Tensor                                                        (slice_35, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_30                                                             aten.unsqueeze.default                                                   (slice_36, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_31                                                             aten.unsqueeze.default                                                   (unsqueeze_30, 3)                                                                                                                                              {}\n",
      "call_function  add_60                                                                   aten.add.Tensor                                                          (conv2d_39, unsqueeze_31)                                                                                                                                      {}\n",
      "call_function  group_norm_37                                                            aten.group_norm.default                                                  (add_60, 32, p_up_blocks_1_resnets_1_norm2_weight, p_up_blocks_1_resnets_1_norm2_bias)                                                                         {}\n",
      "call_function  silu_45                                                                  aten.silu.default                                                        (group_norm_37,)                                                                                                                                               {}\n",
      "call_function  dropout_38                                                               aten.dropout.default                                                     (silu_45, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_40                                                                aten.conv2d.default                                                      (dropout_38, p_up_blocks_1_resnets_1_conv2_weight, p_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_41                                                                aten.conv2d.default                                                      (cat_6, p_up_blocks_1_resnets_1_conv_shortcut_weight, p_up_blocks_1_resnets_1_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_61                                                                   aten.add.Tensor                                                          (conv2d_41, conv2d_40)                                                                                                                                         {}\n",
      "call_function  div_31                                                                   aten.div.Tensor                                                          (add_61, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_38                                                            aten.group_norm.default                                                  (div_31, 32, p_up_blocks_1_attentions_1_norm_weight, p_up_blocks_1_attentions_1_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_16                                                               aten.permute.default                                                     (group_norm_38, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_80                                                                  aten.view.default                                                        (permute_16, [1, 256, 1280])                                                                                                                                   {}\n",
      "call_function  linear_113                                                               aten.linear.default                                                      (view_80, p_up_blocks_1_attentions_1_proj_in_weight, p_up_blocks_1_attentions_1_proj_in_bias)                                                                  {}\n",
      "call_function  layer_norm_24                                                            aten.layer_norm.default                                                  (linear_113, [1280], p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_norm1_bias)                 {}\n",
      "call_function  linear_114                                                               aten.linear.default                                                      (layer_norm_24, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_115                                                               aten.linear.default                                                      (layer_norm_24, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_116                                                               aten.linear.default                                                      (layer_norm_24, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_81                                                                  aten.view.default                                                        (linear_114, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_64                                                             aten.transpose.int                                                       (view_81, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_82                                                                  aten.view.default                                                        (linear_115, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_65                                                             aten.transpose.int                                                       (view_82, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_83                                                                  aten.view.default                                                        (linear_116, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_66                                                             aten.transpose.int                                                       (view_83, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_16                                          aten.scaled_dot_product_attention.default                                (transpose_64, transpose_65, transpose_66)                                                                                                                     {}\n",
      "call_function  transpose_67                                                             aten.transpose.int                                                       (scaled_dot_product_attention_16, 1, 2)                                                                                                                        {}\n",
      "call_function  view_84                                                                  aten.view.default                                                        (transpose_67, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_18                                                              aten._to_copy.default                                                    (view_84,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_117                                                               aten.linear.default                                                      (_to_copy_18, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_39                                                               aten.dropout.default                                                     (linear_117, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_32                                                                   aten.div.Tensor                                                          (dropout_39, 1.0)                                                                                                                                              {}\n",
      "call_function  add_62                                                                   aten.add.Tensor                                                          (div_32, linear_113)                                                                                                                                           {}\n",
      "call_function  layer_norm_25                                                            aten.layer_norm.default                                                  (add_62, [1280], p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_norm2_bias)                     {}\n",
      "call_function  linear_118                                                               aten.linear.default                                                      (layer_norm_25, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_119                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_120                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_85                                                                  aten.view.default                                                        (linear_118, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_68                                                             aten.transpose.int                                                       (view_85, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_86                                                                  aten.view.default                                                        (linear_119, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_69                                                             aten.transpose.int                                                       (view_86, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_87                                                                  aten.view.default                                                        (linear_120, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_70                                                             aten.transpose.int                                                       (view_87, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_17                                          aten.scaled_dot_product_attention.default                                (transpose_68, transpose_69, transpose_70)                                                                                                                     {}\n",
      "call_function  transpose_71                                                             aten.transpose.int                                                       (scaled_dot_product_attention_17, 1, 2)                                                                                                                        {}\n",
      "call_function  view_88                                                                  aten.view.default                                                        (transpose_71, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_19                                                              aten._to_copy.default                                                    (view_88,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_121                                                               aten.linear.default                                                      (_to_copy_19, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_40                                                               aten.dropout.default                                                     (linear_121, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_33                                                                   aten.div.Tensor                                                          (dropout_40, 1.0)                                                                                                                                              {}\n",
      "call_function  add_63                                                                   aten.add.Tensor                                                          (div_33, add_62)                                                                                                                                               {}\n",
      "call_function  layer_norm_26                                                            aten.layer_norm.default                                                  (add_63, [1280], p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_norm3_bias)                     {}\n",
      "call_function  linear_122                                                               aten.linear.default                                                      (layer_norm_26, p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_8                                                                  aten.split.Tensor                                                        (linear_122, 5120, -1)                                                                                                                                         {}\n",
      "call_function  getitem_16                                                               <built-in function getitem>                                              (split_8, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_17                                                               <built-in function getitem>                                              (split_8, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_8                                                                   aten.gelu.default                                                        (getitem_17,)                                                                                                                                                  {}\n",
      "call_function  mul_11                                                                   aten.mul.Tensor                                                          (getitem_16, gelu_8)                                                                                                                                           {}\n",
      "call_function  dropout_41                                                               aten.dropout.default                                                     (mul_11, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_123                                                               aten.linear.default                                                      (dropout_41, p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_weight, p_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_64                                                                   aten.add.Tensor                                                          (linear_123, add_63)                                                                                                                                           {}\n",
      "call_function  linear_124                                                               aten.linear.default                                                      (add_64, p_up_blocks_1_attentions_1_proj_out_weight, p_up_blocks_1_attentions_1_proj_out_bias)                                                                 {}\n",
      "call_function  view_89                                                                  aten.view.default                                                        (linear_124, [1, 16, 16, 1280])                                                                                                                                {}\n",
      "call_function  permute_17                                                               aten.permute.default                                                     (view_89, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_8                                                                  aten.clone.default                                                       (permute_17,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_65                                                                   aten.add.Tensor                                                          (clone_8, div_31)                                                                                                                                              {}\n",
      "call_function  cat_7                                                                    aten.cat.default                                                         ([add_65, conv2d_11], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_39                                                            aten.group_norm.default                                                  (cat_7, 32, p_up_blocks_1_resnets_2_norm1_weight, p_up_blocks_1_resnets_2_norm1_bias)                                                                          {}\n",
      "call_function  silu_46                                                                  aten.silu.default                                                        (group_norm_39,)                                                                                                                                               {}\n",
      "call_function  conv2d_42                                                                aten.conv2d.default                                                      (silu_46, p_up_blocks_1_resnets_2_conv1_weight, p_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_47                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_125                                                               aten.linear.default                                                      (silu_47, p_up_blocks_1_resnets_2_time_emb_proj_weight, p_up_blocks_1_resnets_2_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_37                                                                 aten.slice.Tensor                                                        (linear_125, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_38                                                                 aten.slice.Tensor                                                        (slice_37, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_32                                                             aten.unsqueeze.default                                                   (slice_38, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_33                                                             aten.unsqueeze.default                                                   (unsqueeze_32, 3)                                                                                                                                              {}\n",
      "call_function  add_66                                                                   aten.add.Tensor                                                          (conv2d_42, unsqueeze_33)                                                                                                                                      {}\n",
      "call_function  group_norm_40                                                            aten.group_norm.default                                                  (add_66, 32, p_up_blocks_1_resnets_2_norm2_weight, p_up_blocks_1_resnets_2_norm2_bias)                                                                         {}\n",
      "call_function  silu_48                                                                  aten.silu.default                                                        (group_norm_40,)                                                                                                                                               {}\n",
      "call_function  dropout_42                                                               aten.dropout.default                                                     (silu_48, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_43                                                                aten.conv2d.default                                                      (dropout_42, p_up_blocks_1_resnets_2_conv2_weight, p_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_44                                                                aten.conv2d.default                                                      (cat_7, p_up_blocks_1_resnets_2_conv_shortcut_weight, p_up_blocks_1_resnets_2_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_67                                                                   aten.add.Tensor                                                          (conv2d_44, conv2d_43)                                                                                                                                         {}\n",
      "call_function  div_34                                                                   aten.div.Tensor                                                          (add_67, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_41                                                            aten.group_norm.default                                                  (div_34, 32, p_up_blocks_1_attentions_2_norm_weight, p_up_blocks_1_attentions_2_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_18                                                               aten.permute.default                                                     (group_norm_41, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_90                                                                  aten.view.default                                                        (permute_18, [1, 256, 1280])                                                                                                                                   {}\n",
      "call_function  linear_126                                                               aten.linear.default                                                      (view_90, p_up_blocks_1_attentions_2_proj_in_weight, p_up_blocks_1_attentions_2_proj_in_bias)                                                                  {}\n",
      "call_function  layer_norm_27                                                            aten.layer_norm.default                                                  (linear_126, [1280], p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_norm1_bias)                 {}\n",
      "call_function  linear_127                                                               aten.linear.default                                                      (layer_norm_27, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_128                                                               aten.linear.default                                                      (layer_norm_27, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_129                                                               aten.linear.default                                                      (layer_norm_27, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_91                                                                  aten.view.default                                                        (linear_127, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_72                                                             aten.transpose.int                                                       (view_91, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_92                                                                  aten.view.default                                                        (linear_128, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_73                                                             aten.transpose.int                                                       (view_92, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_93                                                                  aten.view.default                                                        (linear_129, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_74                                                             aten.transpose.int                                                       (view_93, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_18                                          aten.scaled_dot_product_attention.default                                (transpose_72, transpose_73, transpose_74)                                                                                                                     {}\n",
      "call_function  transpose_75                                                             aten.transpose.int                                                       (scaled_dot_product_attention_18, 1, 2)                                                                                                                        {}\n",
      "call_function  view_94                                                                  aten.view.default                                                        (transpose_75, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_20                                                              aten._to_copy.default                                                    (view_94,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_130                                                               aten.linear.default                                                      (_to_copy_20, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_43                                                               aten.dropout.default                                                     (linear_130, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_35                                                                   aten.div.Tensor                                                          (dropout_43, 1.0)                                                                                                                                              {}\n",
      "call_function  add_68                                                                   aten.add.Tensor                                                          (div_35, linear_126)                                                                                                                                           {}\n",
      "call_function  layer_norm_28                                                            aten.layer_norm.default                                                  (add_68, [1280], p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_norm2_bias)                     {}\n",
      "call_function  linear_131                                                               aten.linear.default                                                      (layer_norm_28, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_132                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_133                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_95                                                                  aten.view.default                                                        (linear_131, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_76                                                             aten.transpose.int                                                       (view_95, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_96                                                                  aten.view.default                                                        (linear_132, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_77                                                             aten.transpose.int                                                       (view_96, 1, 2)                                                                                                                                                {}\n",
      "call_function  view_97                                                                  aten.view.default                                                        (linear_133, [1, -1, 20, 64])                                                                                                                                  {}\n",
      "call_function  transpose_78                                                             aten.transpose.int                                                       (view_97, 1, 2)                                                                                                                                                {}\n",
      "call_function  scaled_dot_product_attention_19                                          aten.scaled_dot_product_attention.default                                (transpose_76, transpose_77, transpose_78)                                                                                                                     {}\n",
      "call_function  transpose_79                                                             aten.transpose.int                                                       (scaled_dot_product_attention_19, 1, 2)                                                                                                                        {}\n",
      "call_function  view_98                                                                  aten.view.default                                                        (transpose_79, [1, -1, 1280])                                                                                                                                  {}\n",
      "call_function  _to_copy_21                                                              aten._to_copy.default                                                    (view_98,)                                                                                                                                                     {'dtype': torch.float32}\n",
      "call_function  linear_134                                                               aten.linear.default                                                      (_to_copy_21, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_44                                                               aten.dropout.default                                                     (linear_134, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_36                                                                   aten.div.Tensor                                                          (dropout_44, 1.0)                                                                                                                                              {}\n",
      "call_function  add_69                                                                   aten.add.Tensor                                                          (div_36, add_68)                                                                                                                                               {}\n",
      "call_function  layer_norm_29                                                            aten.layer_norm.default                                                  (add_69, [1280], p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_norm3_bias)                     {}\n",
      "call_function  linear_135                                                               aten.linear.default                                                      (layer_norm_29, p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_9                                                                  aten.split.Tensor                                                        (linear_135, 5120, -1)                                                                                                                                         {}\n",
      "call_function  getitem_18                                                               <built-in function getitem>                                              (split_9, 0)                                                                                                                                                   {}\n",
      "call_function  getitem_19                                                               <built-in function getitem>                                              (split_9, 1)                                                                                                                                                   {}\n",
      "call_function  gelu_9                                                                   aten.gelu.default                                                        (getitem_19,)                                                                                                                                                  {}\n",
      "call_function  mul_12                                                                   aten.mul.Tensor                                                          (getitem_18, gelu_9)                                                                                                                                           {}\n",
      "call_function  dropout_45                                                               aten.dropout.default                                                     (mul_12, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_136                                                               aten.linear.default                                                      (dropout_45, p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_weight, p_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_70                                                                   aten.add.Tensor                                                          (linear_136, add_69)                                                                                                                                           {}\n",
      "call_function  linear_137                                                               aten.linear.default                                                      (add_70, p_up_blocks_1_attentions_2_proj_out_weight, p_up_blocks_1_attentions_2_proj_out_bias)                                                                 {}\n",
      "call_function  view_99                                                                  aten.view.default                                                        (linear_137, [1, 16, 16, 1280])                                                                                                                                {}\n",
      "call_function  permute_19                                                               aten.permute.default                                                     (view_99, [0, 3, 1, 2])                                                                                                                                        {}\n",
      "call_function  clone_9                                                                  aten.clone.default                                                       (permute_19,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_71                                                                   aten.add.Tensor                                                          (clone_9, div_34)                                                                                                                                              {}\n",
      "call_function  upsample_nearest2d_1                                                     aten.upsample_nearest2d.vec                                              (add_71, None, [2.0, 2.0])                                                                                                                                     {}\n",
      "call_function  conv2d_45                                                                aten.conv2d.default                                                      (upsample_nearest2d_1, p_up_blocks_1_upsamplers_0_conv_weight, p_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1])                                           {}\n",
      "call_function  cat_8                                                                    aten.cat.default                                                         ([conv2d_45, add_23], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_42                                                            aten.group_norm.default                                                  (cat_8, 32, p_up_blocks_2_resnets_0_norm1_weight, p_up_blocks_2_resnets_0_norm1_bias)                                                                          {}\n",
      "call_function  silu_49                                                                  aten.silu.default                                                        (group_norm_42,)                                                                                                                                               {}\n",
      "call_function  conv2d_46                                                                aten.conv2d.default                                                      (silu_49, p_up_blocks_2_resnets_0_conv1_weight, p_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_50                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_138                                                               aten.linear.default                                                      (silu_50, p_up_blocks_2_resnets_0_time_emb_proj_weight, p_up_blocks_2_resnets_0_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_39                                                                 aten.slice.Tensor                                                        (linear_138, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_40                                                                 aten.slice.Tensor                                                        (slice_39, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_34                                                             aten.unsqueeze.default                                                   (slice_40, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_35                                                             aten.unsqueeze.default                                                   (unsqueeze_34, 3)                                                                                                                                              {}\n",
      "call_function  add_72                                                                   aten.add.Tensor                                                          (conv2d_46, unsqueeze_35)                                                                                                                                      {}\n",
      "call_function  group_norm_43                                                            aten.group_norm.default                                                  (add_72, 32, p_up_blocks_2_resnets_0_norm2_weight, p_up_blocks_2_resnets_0_norm2_bias)                                                                         {}\n",
      "call_function  silu_51                                                                  aten.silu.default                                                        (group_norm_43,)                                                                                                                                               {}\n",
      "call_function  dropout_46                                                               aten.dropout.default                                                     (silu_51, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_47                                                                aten.conv2d.default                                                      (dropout_46, p_up_blocks_2_resnets_0_conv2_weight, p_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_48                                                                aten.conv2d.default                                                      (cat_8, p_up_blocks_2_resnets_0_conv_shortcut_weight, p_up_blocks_2_resnets_0_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_73                                                                   aten.add.Tensor                                                          (conv2d_48, conv2d_47)                                                                                                                                         {}\n",
      "call_function  div_37                                                                   aten.div.Tensor                                                          (add_73, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_44                                                            aten.group_norm.default                                                  (div_37, 32, p_up_blocks_2_attentions_0_norm_weight, p_up_blocks_2_attentions_0_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_20                                                               aten.permute.default                                                     (group_norm_44, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_100                                                                 aten.view.default                                                        (permute_20, [1, 1024, 640])                                                                                                                                   {}\n",
      "call_function  linear_139                                                               aten.linear.default                                                      (view_100, p_up_blocks_2_attentions_0_proj_in_weight, p_up_blocks_2_attentions_0_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_30                                                            aten.layer_norm.default                                                  (linear_139, [640], p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_140                                                               aten.linear.default                                                      (layer_norm_30, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_141                                                               aten.linear.default                                                      (layer_norm_30, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_142                                                               aten.linear.default                                                      (layer_norm_30, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_101                                                                 aten.view.default                                                        (linear_140, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_80                                                             aten.transpose.int                                                       (view_101, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_102                                                                 aten.view.default                                                        (linear_141, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_81                                                             aten.transpose.int                                                       (view_102, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_103                                                                 aten.view.default                                                        (linear_142, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_82                                                             aten.transpose.int                                                       (view_103, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_20                                          aten.scaled_dot_product_attention.default                                (transpose_80, transpose_81, transpose_82)                                                                                                                     {}\n",
      "call_function  transpose_83                                                             aten.transpose.int                                                       (scaled_dot_product_attention_20, 1, 2)                                                                                                                        {}\n",
      "call_function  view_104                                                                 aten.view.default                                                        (transpose_83, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_22                                                              aten._to_copy.default                                                    (view_104,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_143                                                               aten.linear.default                                                      (_to_copy_22, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_47                                                               aten.dropout.default                                                     (linear_143, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_38                                                                   aten.div.Tensor                                                          (dropout_47, 1.0)                                                                                                                                              {}\n",
      "call_function  add_74                                                                   aten.add.Tensor                                                          (div_38, linear_139)                                                                                                                                           {}\n",
      "call_function  layer_norm_31                                                            aten.layer_norm.default                                                  (add_74, [640], p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_144                                                               aten.linear.default                                                      (layer_norm_31, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_145                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_146                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_105                                                                 aten.view.default                                                        (linear_144, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_84                                                             aten.transpose.int                                                       (view_105, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_106                                                                 aten.view.default                                                        (linear_145, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_85                                                             aten.transpose.int                                                       (view_106, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_107                                                                 aten.view.default                                                        (linear_146, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_86                                                             aten.transpose.int                                                       (view_107, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_21                                          aten.scaled_dot_product_attention.default                                (transpose_84, transpose_85, transpose_86)                                                                                                                     {}\n",
      "call_function  transpose_87                                                             aten.transpose.int                                                       (scaled_dot_product_attention_21, 1, 2)                                                                                                                        {}\n",
      "call_function  view_108                                                                 aten.view.default                                                        (transpose_87, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_23                                                              aten._to_copy.default                                                    (view_108,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_147                                                               aten.linear.default                                                      (_to_copy_23, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_48                                                               aten.dropout.default                                                     (linear_147, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_39                                                                   aten.div.Tensor                                                          (dropout_48, 1.0)                                                                                                                                              {}\n",
      "call_function  add_75                                                                   aten.add.Tensor                                                          (div_39, add_74)                                                                                                                                               {}\n",
      "call_function  layer_norm_32                                                            aten.layer_norm.default                                                  (add_75, [640], p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_148                                                               aten.linear.default                                                      (layer_norm_32, p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_10                                                                 aten.split.Tensor                                                        (linear_148, 2560, -1)                                                                                                                                         {}\n",
      "call_function  getitem_20                                                               <built-in function getitem>                                              (split_10, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_21                                                               <built-in function getitem>                                              (split_10, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_10                                                                  aten.gelu.default                                                        (getitem_21,)                                                                                                                                                  {}\n",
      "call_function  mul_13                                                                   aten.mul.Tensor                                                          (getitem_20, gelu_10)                                                                                                                                          {}\n",
      "call_function  dropout_49                                                               aten.dropout.default                                                     (mul_13, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_149                                                               aten.linear.default                                                      (dropout_49, p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_weight, p_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_76                                                                   aten.add.Tensor                                                          (linear_149, add_75)                                                                                                                                           {}\n",
      "call_function  linear_150                                                               aten.linear.default                                                      (add_76, p_up_blocks_2_attentions_0_proj_out_weight, p_up_blocks_2_attentions_0_proj_out_bias)                                                                 {}\n",
      "call_function  view_109                                                                 aten.view.default                                                        (linear_150, [1, 32, 32, 640])                                                                                                                                 {}\n",
      "call_function  permute_21                                                               aten.permute.default                                                     (view_109, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_10                                                                 aten.clone.default                                                       (permute_21,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_77                                                                   aten.add.Tensor                                                          (clone_10, div_37)                                                                                                                                             {}\n",
      "call_function  cat_9                                                                    aten.cat.default                                                         ([add_77, add_17], 1)                                                                                                                                          {}\n",
      "call_function  group_norm_45                                                            aten.group_norm.default                                                  (cat_9, 32, p_up_blocks_2_resnets_1_norm1_weight, p_up_blocks_2_resnets_1_norm1_bias)                                                                          {}\n",
      "call_function  silu_52                                                                  aten.silu.default                                                        (group_norm_45,)                                                                                                                                               {}\n",
      "call_function  conv2d_49                                                                aten.conv2d.default                                                      (silu_52, p_up_blocks_2_resnets_1_conv1_weight, p_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_53                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_151                                                               aten.linear.default                                                      (silu_53, p_up_blocks_2_resnets_1_time_emb_proj_weight, p_up_blocks_2_resnets_1_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_41                                                                 aten.slice.Tensor                                                        (linear_151, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_42                                                                 aten.slice.Tensor                                                        (slice_41, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_36                                                             aten.unsqueeze.default                                                   (slice_42, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_37                                                             aten.unsqueeze.default                                                   (unsqueeze_36, 3)                                                                                                                                              {}\n",
      "call_function  add_78                                                                   aten.add.Tensor                                                          (conv2d_49, unsqueeze_37)                                                                                                                                      {}\n",
      "call_function  group_norm_46                                                            aten.group_norm.default                                                  (add_78, 32, p_up_blocks_2_resnets_1_norm2_weight, p_up_blocks_2_resnets_1_norm2_bias)                                                                         {}\n",
      "call_function  silu_54                                                                  aten.silu.default                                                        (group_norm_46,)                                                                                                                                               {}\n",
      "call_function  dropout_50                                                               aten.dropout.default                                                     (silu_54, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_50                                                                aten.conv2d.default                                                      (dropout_50, p_up_blocks_2_resnets_1_conv2_weight, p_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_51                                                                aten.conv2d.default                                                      (cat_9, p_up_blocks_2_resnets_1_conv_shortcut_weight, p_up_blocks_2_resnets_1_conv_shortcut_bias)                                                              {}\n",
      "call_function  add_79                                                                   aten.add.Tensor                                                          (conv2d_51, conv2d_50)                                                                                                                                         {}\n",
      "call_function  div_40                                                                   aten.div.Tensor                                                          (add_79, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_47                                                            aten.group_norm.default                                                  (div_40, 32, p_up_blocks_2_attentions_1_norm_weight, p_up_blocks_2_attentions_1_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_22                                                               aten.permute.default                                                     (group_norm_47, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_110                                                                 aten.view.default                                                        (permute_22, [1, 1024, 640])                                                                                                                                   {}\n",
      "call_function  linear_152                                                               aten.linear.default                                                      (view_110, p_up_blocks_2_attentions_1_proj_in_weight, p_up_blocks_2_attentions_1_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_33                                                            aten.layer_norm.default                                                  (linear_152, [640], p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_153                                                               aten.linear.default                                                      (layer_norm_33, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_154                                                               aten.linear.default                                                      (layer_norm_33, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_155                                                               aten.linear.default                                                      (layer_norm_33, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_111                                                                 aten.view.default                                                        (linear_153, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_88                                                             aten.transpose.int                                                       (view_111, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_112                                                                 aten.view.default                                                        (linear_154, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_89                                                             aten.transpose.int                                                       (view_112, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_113                                                                 aten.view.default                                                        (linear_155, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_90                                                             aten.transpose.int                                                       (view_113, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_22                                          aten.scaled_dot_product_attention.default                                (transpose_88, transpose_89, transpose_90)                                                                                                                     {}\n",
      "call_function  transpose_91                                                             aten.transpose.int                                                       (scaled_dot_product_attention_22, 1, 2)                                                                                                                        {}\n",
      "call_function  view_114                                                                 aten.view.default                                                        (transpose_91, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_24                                                              aten._to_copy.default                                                    (view_114,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_156                                                               aten.linear.default                                                      (_to_copy_24, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_51                                                               aten.dropout.default                                                     (linear_156, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_41                                                                   aten.div.Tensor                                                          (dropout_51, 1.0)                                                                                                                                              {}\n",
      "call_function  add_80                                                                   aten.add.Tensor                                                          (div_41, linear_152)                                                                                                                                           {}\n",
      "call_function  layer_norm_34                                                            aten.layer_norm.default                                                  (add_80, [640], p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_157                                                               aten.linear.default                                                      (layer_norm_34, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_158                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_159                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_115                                                                 aten.view.default                                                        (linear_157, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_92                                                             aten.transpose.int                                                       (view_115, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_116                                                                 aten.view.default                                                        (linear_158, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_93                                                             aten.transpose.int                                                       (view_116, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_117                                                                 aten.view.default                                                        (linear_159, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_94                                                             aten.transpose.int                                                       (view_117, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_23                                          aten.scaled_dot_product_attention.default                                (transpose_92, transpose_93, transpose_94)                                                                                                                     {}\n",
      "call_function  transpose_95                                                             aten.transpose.int                                                       (scaled_dot_product_attention_23, 1, 2)                                                                                                                        {}\n",
      "call_function  view_118                                                                 aten.view.default                                                        (transpose_95, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_25                                                              aten._to_copy.default                                                    (view_118,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_160                                                               aten.linear.default                                                      (_to_copy_25, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_52                                                               aten.dropout.default                                                     (linear_160, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_42                                                                   aten.div.Tensor                                                          (dropout_52, 1.0)                                                                                                                                              {}\n",
      "call_function  add_81                                                                   aten.add.Tensor                                                          (div_42, add_80)                                                                                                                                               {}\n",
      "call_function  layer_norm_35                                                            aten.layer_norm.default                                                  (add_81, [640], p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_161                                                               aten.linear.default                                                      (layer_norm_35, p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_11                                                                 aten.split.Tensor                                                        (linear_161, 2560, -1)                                                                                                                                         {}\n",
      "call_function  getitem_22                                                               <built-in function getitem>                                              (split_11, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_23                                                               <built-in function getitem>                                              (split_11, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_11                                                                  aten.gelu.default                                                        (getitem_23,)                                                                                                                                                  {}\n",
      "call_function  mul_14                                                                   aten.mul.Tensor                                                          (getitem_22, gelu_11)                                                                                                                                          {}\n",
      "call_function  dropout_53                                                               aten.dropout.default                                                     (mul_14, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_162                                                               aten.linear.default                                                      (dropout_53, p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_weight, p_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_82                                                                   aten.add.Tensor                                                          (linear_162, add_81)                                                                                                                                           {}\n",
      "call_function  linear_163                                                               aten.linear.default                                                      (add_82, p_up_blocks_2_attentions_1_proj_out_weight, p_up_blocks_2_attentions_1_proj_out_bias)                                                                 {}\n",
      "call_function  view_119                                                                 aten.view.default                                                        (linear_163, [1, 32, 32, 640])                                                                                                                                 {}\n",
      "call_function  permute_23                                                               aten.permute.default                                                     (view_119, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_11                                                                 aten.clone.default                                                       (permute_23,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_83                                                                   aten.add.Tensor                                                          (clone_11, div_40)                                                                                                                                             {}\n",
      "call_function  cat_10                                                                   aten.cat.default                                                         ([add_83, conv2d_5], 1)                                                                                                                                        {}\n",
      "call_function  group_norm_48                                                            aten.group_norm.default                                                  (cat_10, 32, p_up_blocks_2_resnets_2_norm1_weight, p_up_blocks_2_resnets_2_norm1_bias)                                                                         {}\n",
      "call_function  silu_55                                                                  aten.silu.default                                                        (group_norm_48,)                                                                                                                                               {}\n",
      "call_function  conv2d_52                                                                aten.conv2d.default                                                      (silu_55, p_up_blocks_2_resnets_2_conv1_weight, p_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_56                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_164                                                               aten.linear.default                                                      (silu_56, p_up_blocks_2_resnets_2_time_emb_proj_weight, p_up_blocks_2_resnets_2_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_43                                                                 aten.slice.Tensor                                                        (linear_164, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_44                                                                 aten.slice.Tensor                                                        (slice_43, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_38                                                             aten.unsqueeze.default                                                   (slice_44, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_39                                                             aten.unsqueeze.default                                                   (unsqueeze_38, 3)                                                                                                                                              {}\n",
      "call_function  add_84                                                                   aten.add.Tensor                                                          (conv2d_52, unsqueeze_39)                                                                                                                                      {}\n",
      "call_function  group_norm_49                                                            aten.group_norm.default                                                  (add_84, 32, p_up_blocks_2_resnets_2_norm2_weight, p_up_blocks_2_resnets_2_norm2_bias)                                                                         {}\n",
      "call_function  silu_57                                                                  aten.silu.default                                                        (group_norm_49,)                                                                                                                                               {}\n",
      "call_function  dropout_54                                                               aten.dropout.default                                                     (silu_57, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_53                                                                aten.conv2d.default                                                      (dropout_54, p_up_blocks_2_resnets_2_conv2_weight, p_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_54                                                                aten.conv2d.default                                                      (cat_10, p_up_blocks_2_resnets_2_conv_shortcut_weight, p_up_blocks_2_resnets_2_conv_shortcut_bias)                                                             {}\n",
      "call_function  add_85                                                                   aten.add.Tensor                                                          (conv2d_54, conv2d_53)                                                                                                                                         {}\n",
      "call_function  div_43                                                                   aten.div.Tensor                                                          (add_85, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_50                                                            aten.group_norm.default                                                  (div_43, 32, p_up_blocks_2_attentions_2_norm_weight, p_up_blocks_2_attentions_2_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_24                                                               aten.permute.default                                                     (group_norm_50, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_120                                                                 aten.view.default                                                        (permute_24, [1, 1024, 640])                                                                                                                                   {}\n",
      "call_function  linear_165                                                               aten.linear.default                                                      (view_120, p_up_blocks_2_attentions_2_proj_in_weight, p_up_blocks_2_attentions_2_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_36                                                            aten.layer_norm.default                                                  (linear_165, [640], p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_166                                                               aten.linear.default                                                      (layer_norm_36, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_167                                                               aten.linear.default                                                      (layer_norm_36, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_168                                                               aten.linear.default                                                      (layer_norm_36, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_121                                                                 aten.view.default                                                        (linear_166, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_96                                                             aten.transpose.int                                                       (view_121, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_122                                                                 aten.view.default                                                        (linear_167, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_97                                                             aten.transpose.int                                                       (view_122, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_123                                                                 aten.view.default                                                        (linear_168, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_98                                                             aten.transpose.int                                                       (view_123, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_24                                          aten.scaled_dot_product_attention.default                                (transpose_96, transpose_97, transpose_98)                                                                                                                     {}\n",
      "call_function  transpose_99                                                             aten.transpose.int                                                       (scaled_dot_product_attention_24, 1, 2)                                                                                                                        {}\n",
      "call_function  view_124                                                                 aten.view.default                                                        (transpose_99, [1, -1, 640])                                                                                                                                   {}\n",
      "call_function  _to_copy_26                                                              aten._to_copy.default                                                    (view_124,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_169                                                               aten.linear.default                                                      (_to_copy_26, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_55                                                               aten.dropout.default                                                     (linear_169, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_44                                                                   aten.div.Tensor                                                          (dropout_55, 1.0)                                                                                                                                              {}\n",
      "call_function  add_86                                                                   aten.add.Tensor                                                          (div_44, linear_165)                                                                                                                                           {}\n",
      "call_function  layer_norm_37                                                            aten.layer_norm.default                                                  (add_86, [640], p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_170                                                               aten.linear.default                                                      (layer_norm_37, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_171                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_172                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_125                                                                 aten.view.default                                                        (linear_170, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_100                                                            aten.transpose.int                                                       (view_125, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_126                                                                 aten.view.default                                                        (linear_171, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_101                                                            aten.transpose.int                                                       (view_126, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_127                                                                 aten.view.default                                                        (linear_172, [1, -1, 10, 64])                                                                                                                                  {}\n",
      "call_function  transpose_102                                                            aten.transpose.int                                                       (view_127, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_25                                          aten.scaled_dot_product_attention.default                                (transpose_100, transpose_101, transpose_102)                                                                                                                  {}\n",
      "call_function  transpose_103                                                            aten.transpose.int                                                       (scaled_dot_product_attention_25, 1, 2)                                                                                                                        {}\n",
      "call_function  view_128                                                                 aten.view.default                                                        (transpose_103, [1, -1, 640])                                                                                                                                  {}\n",
      "call_function  _to_copy_27                                                              aten._to_copy.default                                                    (view_128,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_173                                                               aten.linear.default                                                      (_to_copy_27, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_56                                                               aten.dropout.default                                                     (linear_173, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_45                                                                   aten.div.Tensor                                                          (dropout_56, 1.0)                                                                                                                                              {}\n",
      "call_function  add_87                                                                   aten.add.Tensor                                                          (div_45, add_86)                                                                                                                                               {}\n",
      "call_function  layer_norm_38                                                            aten.layer_norm.default                                                  (add_87, [640], p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_174                                                               aten.linear.default                                                      (layer_norm_38, p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_12                                                                 aten.split.Tensor                                                        (linear_174, 2560, -1)                                                                                                                                         {}\n",
      "call_function  getitem_24                                                               <built-in function getitem>                                              (split_12, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_25                                                               <built-in function getitem>                                              (split_12, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_12                                                                  aten.gelu.default                                                        (getitem_25,)                                                                                                                                                  {}\n",
      "call_function  mul_15                                                                   aten.mul.Tensor                                                          (getitem_24, gelu_12)                                                                                                                                          {}\n",
      "call_function  dropout_57                                                               aten.dropout.default                                                     (mul_15, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_175                                                               aten.linear.default                                                      (dropout_57, p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_weight, p_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_88                                                                   aten.add.Tensor                                                          (linear_175, add_87)                                                                                                                                           {}\n",
      "call_function  linear_176                                                               aten.linear.default                                                      (add_88, p_up_blocks_2_attentions_2_proj_out_weight, p_up_blocks_2_attentions_2_proj_out_bias)                                                                 {}\n",
      "call_function  view_129                                                                 aten.view.default                                                        (linear_176, [1, 32, 32, 640])                                                                                                                                 {}\n",
      "call_function  permute_25                                                               aten.permute.default                                                     (view_129, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_12                                                                 aten.clone.default                                                       (permute_25,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_89                                                                   aten.add.Tensor                                                          (clone_12, div_43)                                                                                                                                             {}\n",
      "call_function  upsample_nearest2d_2                                                     aten.upsample_nearest2d.vec                                              (add_89, None, [2.0, 2.0])                                                                                                                                     {}\n",
      "call_function  conv2d_55                                                                aten.conv2d.default                                                      (upsample_nearest2d_2, p_up_blocks_2_upsamplers_0_conv_weight, p_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1])                                           {}\n",
      "call_function  cat_11                                                                   aten.cat.default                                                         ([conv2d_55, add_11], 1)                                                                                                                                       {}\n",
      "call_function  group_norm_51                                                            aten.group_norm.default                                                  (cat_11, 32, p_up_blocks_3_resnets_0_norm1_weight, p_up_blocks_3_resnets_0_norm1_bias)                                                                         {}\n",
      "call_function  silu_58                                                                  aten.silu.default                                                        (group_norm_51,)                                                                                                                                               {}\n",
      "call_function  conv2d_56                                                                aten.conv2d.default                                                      (silu_58, p_up_blocks_3_resnets_0_conv1_weight, p_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_59                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_177                                                               aten.linear.default                                                      (silu_59, p_up_blocks_3_resnets_0_time_emb_proj_weight, p_up_blocks_3_resnets_0_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_45                                                                 aten.slice.Tensor                                                        (linear_177, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_46                                                                 aten.slice.Tensor                                                        (slice_45, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_40                                                             aten.unsqueeze.default                                                   (slice_46, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_41                                                             aten.unsqueeze.default                                                   (unsqueeze_40, 3)                                                                                                                                              {}\n",
      "call_function  add_90                                                                   aten.add.Tensor                                                          (conv2d_56, unsqueeze_41)                                                                                                                                      {}\n",
      "call_function  group_norm_52                                                            aten.group_norm.default                                                  (add_90, 32, p_up_blocks_3_resnets_0_norm2_weight, p_up_blocks_3_resnets_0_norm2_bias)                                                                         {}\n",
      "call_function  silu_60                                                                  aten.silu.default                                                        (group_norm_52,)                                                                                                                                               {}\n",
      "call_function  dropout_58                                                               aten.dropout.default                                                     (silu_60, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_57                                                                aten.conv2d.default                                                      (dropout_58, p_up_blocks_3_resnets_0_conv2_weight, p_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_58                                                                aten.conv2d.default                                                      (cat_11, p_up_blocks_3_resnets_0_conv_shortcut_weight, p_up_blocks_3_resnets_0_conv_shortcut_bias)                                                             {}\n",
      "call_function  add_91                                                                   aten.add.Tensor                                                          (conv2d_58, conv2d_57)                                                                                                                                         {}\n",
      "call_function  div_46                                                                   aten.div.Tensor                                                          (add_91, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_53                                                            aten.group_norm.default                                                  (div_46, 32, p_up_blocks_3_attentions_0_norm_weight, p_up_blocks_3_attentions_0_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_26                                                               aten.permute.default                                                     (group_norm_53, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_130                                                                 aten.view.default                                                        (permute_26, [1, 4096, 320])                                                                                                                                   {}\n",
      "call_function  linear_178                                                               aten.linear.default                                                      (view_130, p_up_blocks_3_attentions_0_proj_in_weight, p_up_blocks_3_attentions_0_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_39                                                            aten.layer_norm.default                                                  (linear_178, [320], p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_179                                                               aten.linear.default                                                      (layer_norm_39, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_180                                                               aten.linear.default                                                      (layer_norm_39, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_181                                                               aten.linear.default                                                      (layer_norm_39, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_131                                                                 aten.view.default                                                        (linear_179, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_104                                                            aten.transpose.int                                                       (view_131, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_132                                                                 aten.view.default                                                        (linear_180, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_105                                                            aten.transpose.int                                                       (view_132, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_133                                                                 aten.view.default                                                        (linear_181, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_106                                                            aten.transpose.int                                                       (view_133, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_26                                          aten.scaled_dot_product_attention.default                                (transpose_104, transpose_105, transpose_106)                                                                                                                  {}\n",
      "call_function  transpose_107                                                            aten.transpose.int                                                       (scaled_dot_product_attention_26, 1, 2)                                                                                                                        {}\n",
      "call_function  view_134                                                                 aten.view.default                                                        (transpose_107, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_28                                                              aten._to_copy.default                                                    (view_134,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_182                                                               aten.linear.default                                                      (_to_copy_28, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_59                                                               aten.dropout.default                                                     (linear_182, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_47                                                                   aten.div.Tensor                                                          (dropout_59, 1.0)                                                                                                                                              {}\n",
      "call_function  add_92                                                                   aten.add.Tensor                                                          (div_47, linear_178)                                                                                                                                           {}\n",
      "call_function  layer_norm_40                                                            aten.layer_norm.default                                                  (add_92, [320], p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_183                                                               aten.linear.default                                                      (layer_norm_40, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_184                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_185                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_135                                                                 aten.view.default                                                        (linear_183, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_108                                                            aten.transpose.int                                                       (view_135, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_136                                                                 aten.view.default                                                        (linear_184, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_109                                                            aten.transpose.int                                                       (view_136, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_137                                                                 aten.view.default                                                        (linear_185, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_110                                                            aten.transpose.int                                                       (view_137, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_27                                          aten.scaled_dot_product_attention.default                                (transpose_108, transpose_109, transpose_110)                                                                                                                  {}\n",
      "call_function  transpose_111                                                            aten.transpose.int                                                       (scaled_dot_product_attention_27, 1, 2)                                                                                                                        {}\n",
      "call_function  view_138                                                                 aten.view.default                                                        (transpose_111, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_29                                                              aten._to_copy.default                                                    (view_138,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_186                                                               aten.linear.default                                                      (_to_copy_29, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_60                                                               aten.dropout.default                                                     (linear_186, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_48                                                                   aten.div.Tensor                                                          (dropout_60, 1.0)                                                                                                                                              {}\n",
      "call_function  add_93                                                                   aten.add.Tensor                                                          (div_48, add_92)                                                                                                                                               {}\n",
      "call_function  layer_norm_41                                                            aten.layer_norm.default                                                  (add_93, [320], p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_187                                                               aten.linear.default                                                      (layer_norm_41, p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_13                                                                 aten.split.Tensor                                                        (linear_187, 1280, -1)                                                                                                                                         {}\n",
      "call_function  getitem_26                                                               <built-in function getitem>                                              (split_13, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_27                                                               <built-in function getitem>                                              (split_13, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_13                                                                  aten.gelu.default                                                        (getitem_27,)                                                                                                                                                  {}\n",
      "call_function  mul_16                                                                   aten.mul.Tensor                                                          (getitem_26, gelu_13)                                                                                                                                          {}\n",
      "call_function  dropout_61                                                               aten.dropout.default                                                     (mul_16, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_188                                                               aten.linear.default                                                      (dropout_61, p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_weight, p_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_94                                                                   aten.add.Tensor                                                          (linear_188, add_93)                                                                                                                                           {}\n",
      "call_function  linear_189                                                               aten.linear.default                                                      (add_94, p_up_blocks_3_attentions_0_proj_out_weight, p_up_blocks_3_attentions_0_proj_out_bias)                                                                 {}\n",
      "call_function  view_139                                                                 aten.view.default                                                        (linear_189, [1, 64, 64, 320])                                                                                                                                 {}\n",
      "call_function  permute_27                                                               aten.permute.default                                                     (view_139, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_13                                                                 aten.clone.default                                                       (permute_27,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_95                                                                   aten.add.Tensor                                                          (clone_13, div_46)                                                                                                                                             {}\n",
      "call_function  cat_12                                                                   aten.cat.default                                                         ([add_95, add_5], 1)                                                                                                                                           {}\n",
      "call_function  group_norm_54                                                            aten.group_norm.default                                                  (cat_12, 32, p_up_blocks_3_resnets_1_norm1_weight, p_up_blocks_3_resnets_1_norm1_bias)                                                                         {}\n",
      "call_function  silu_61                                                                  aten.silu.default                                                        (group_norm_54,)                                                                                                                                               {}\n",
      "call_function  conv2d_59                                                                aten.conv2d.default                                                      (silu_61, p_up_blocks_3_resnets_1_conv1_weight, p_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_62                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_190                                                               aten.linear.default                                                      (silu_62, p_up_blocks_3_resnets_1_time_emb_proj_weight, p_up_blocks_3_resnets_1_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_47                                                                 aten.slice.Tensor                                                        (linear_190, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_48                                                                 aten.slice.Tensor                                                        (slice_47, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_42                                                             aten.unsqueeze.default                                                   (slice_48, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_43                                                             aten.unsqueeze.default                                                   (unsqueeze_42, 3)                                                                                                                                              {}\n",
      "call_function  add_96                                                                   aten.add.Tensor                                                          (conv2d_59, unsqueeze_43)                                                                                                                                      {}\n",
      "call_function  group_norm_55                                                            aten.group_norm.default                                                  (add_96, 32, p_up_blocks_3_resnets_1_norm2_weight, p_up_blocks_3_resnets_1_norm2_bias)                                                                         {}\n",
      "call_function  silu_63                                                                  aten.silu.default                                                        (group_norm_55,)                                                                                                                                               {}\n",
      "call_function  dropout_62                                                               aten.dropout.default                                                     (silu_63, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_60                                                                aten.conv2d.default                                                      (dropout_62, p_up_blocks_3_resnets_1_conv2_weight, p_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_61                                                                aten.conv2d.default                                                      (cat_12, p_up_blocks_3_resnets_1_conv_shortcut_weight, p_up_blocks_3_resnets_1_conv_shortcut_bias)                                                             {}\n",
      "call_function  add_97                                                                   aten.add.Tensor                                                          (conv2d_61, conv2d_60)                                                                                                                                         {}\n",
      "call_function  div_49                                                                   aten.div.Tensor                                                          (add_97, 1.0)                                                                                                                                                  {}\n",
      "call_function  group_norm_56                                                            aten.group_norm.default                                                  (div_49, 32, p_up_blocks_3_attentions_1_norm_weight, p_up_blocks_3_attentions_1_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_28                                                               aten.permute.default                                                     (group_norm_56, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_140                                                                 aten.view.default                                                        (permute_28, [1, 4096, 320])                                                                                                                                   {}\n",
      "call_function  linear_191                                                               aten.linear.default                                                      (view_140, p_up_blocks_3_attentions_1_proj_in_weight, p_up_blocks_3_attentions_1_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_42                                                            aten.layer_norm.default                                                  (linear_191, [320], p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_192                                                               aten.linear.default                                                      (layer_norm_42, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_193                                                               aten.linear.default                                                      (layer_norm_42, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_194                                                               aten.linear.default                                                      (layer_norm_42, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_141                                                                 aten.view.default                                                        (linear_192, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_112                                                            aten.transpose.int                                                       (view_141, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_142                                                                 aten.view.default                                                        (linear_193, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_113                                                            aten.transpose.int                                                       (view_142, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_143                                                                 aten.view.default                                                        (linear_194, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_114                                                            aten.transpose.int                                                       (view_143, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_28                                          aten.scaled_dot_product_attention.default                                (transpose_112, transpose_113, transpose_114)                                                                                                                  {}\n",
      "call_function  transpose_115                                                            aten.transpose.int                                                       (scaled_dot_product_attention_28, 1, 2)                                                                                                                        {}\n",
      "call_function  view_144                                                                 aten.view.default                                                        (transpose_115, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_30                                                              aten._to_copy.default                                                    (view_144,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_195                                                               aten.linear.default                                                      (_to_copy_30, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_63                                                               aten.dropout.default                                                     (linear_195, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_50                                                                   aten.div.Tensor                                                          (dropout_63, 1.0)                                                                                                                                              {}\n",
      "call_function  add_98                                                                   aten.add.Tensor                                                          (div_50, linear_191)                                                                                                                                           {}\n",
      "call_function  layer_norm_43                                                            aten.layer_norm.default                                                  (add_98, [320], p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_norm2_bias)                      {}\n",
      "call_function  linear_196                                                               aten.linear.default                                                      (layer_norm_43, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_197                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_198                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_145                                                                 aten.view.default                                                        (linear_196, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_116                                                            aten.transpose.int                                                       (view_145, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_146                                                                 aten.view.default                                                        (linear_197, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_117                                                            aten.transpose.int                                                       (view_146, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_147                                                                 aten.view.default                                                        (linear_198, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_118                                                            aten.transpose.int                                                       (view_147, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_29                                          aten.scaled_dot_product_attention.default                                (transpose_116, transpose_117, transpose_118)                                                                                                                  {}\n",
      "call_function  transpose_119                                                            aten.transpose.int                                                       (scaled_dot_product_attention_29, 1, 2)                                                                                                                        {}\n",
      "call_function  view_148                                                                 aten.view.default                                                        (transpose_119, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_31                                                              aten._to_copy.default                                                    (view_148,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_199                                                               aten.linear.default                                                      (_to_copy_31, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_64                                                               aten.dropout.default                                                     (linear_199, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_51                                                                   aten.div.Tensor                                                          (dropout_64, 1.0)                                                                                                                                              {}\n",
      "call_function  add_99                                                                   aten.add.Tensor                                                          (div_51, add_98)                                                                                                                                               {}\n",
      "call_function  layer_norm_44                                                            aten.layer_norm.default                                                  (add_99, [320], p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_norm3_bias)                      {}\n",
      "call_function  linear_200                                                               aten.linear.default                                                      (layer_norm_44, p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_14                                                                 aten.split.Tensor                                                        (linear_200, 1280, -1)                                                                                                                                         {}\n",
      "call_function  getitem_28                                                               <built-in function getitem>                                              (split_14, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_29                                                               <built-in function getitem>                                              (split_14, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_14                                                                  aten.gelu.default                                                        (getitem_29,)                                                                                                                                                  {}\n",
      "call_function  mul_17                                                                   aten.mul.Tensor                                                          (getitem_28, gelu_14)                                                                                                                                          {}\n",
      "call_function  dropout_65                                                               aten.dropout.default                                                     (mul_17, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_201                                                               aten.linear.default                                                      (dropout_65, p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_weight, p_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_100                                                                  aten.add.Tensor                                                          (linear_201, add_99)                                                                                                                                           {}\n",
      "call_function  linear_202                                                               aten.linear.default                                                      (add_100, p_up_blocks_3_attentions_1_proj_out_weight, p_up_blocks_3_attentions_1_proj_out_bias)                                                                {}\n",
      "call_function  view_149                                                                 aten.view.default                                                        (linear_202, [1, 64, 64, 320])                                                                                                                                 {}\n",
      "call_function  permute_29                                                               aten.permute.default                                                     (view_149, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_14                                                                 aten.clone.default                                                       (permute_29,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_101                                                                  aten.add.Tensor                                                          (clone_14, div_49)                                                                                                                                             {}\n",
      "call_function  cat_13                                                                   aten.cat.default                                                         ([add_101, conv2d], 1)                                                                                                                                         {}\n",
      "call_function  group_norm_57                                                            aten.group_norm.default                                                  (cat_13, 32, p_up_blocks_3_resnets_2_norm1_weight, p_up_blocks_3_resnets_2_norm1_bias)                                                                         {}\n",
      "call_function  silu_64                                                                  aten.silu.default                                                        (group_norm_57,)                                                                                                                                               {}\n",
      "call_function  conv2d_62                                                                aten.conv2d.default                                                      (silu_64, p_up_blocks_3_resnets_2_conv1_weight, p_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1])                                                            {}\n",
      "call_function  silu_65                                                                  aten.silu.default                                                        (linear_1,)                                                                                                                                                    {}\n",
      "call_function  linear_203                                                               aten.linear.default                                                      (silu_65, p_up_blocks_3_resnets_2_time_emb_proj_weight, p_up_blocks_3_resnets_2_time_emb_proj_bias)                                                            {}\n",
      "call_function  slice_49                                                                 aten.slice.Tensor                                                        (linear_203, 0, 0, 9223372036854775807)                                                                                                                        {}\n",
      "call_function  slice_50                                                                 aten.slice.Tensor                                                        (slice_49, 1, 0, 9223372036854775807)                                                                                                                          {}\n",
      "call_function  unsqueeze_44                                                             aten.unsqueeze.default                                                   (slice_50, 2)                                                                                                                                                  {}\n",
      "call_function  unsqueeze_45                                                             aten.unsqueeze.default                                                   (unsqueeze_44, 3)                                                                                                                                              {}\n",
      "call_function  add_102                                                                  aten.add.Tensor                                                          (conv2d_62, unsqueeze_45)                                                                                                                                      {}\n",
      "call_function  group_norm_58                                                            aten.group_norm.default                                                  (add_102, 32, p_up_blocks_3_resnets_2_norm2_weight, p_up_blocks_3_resnets_2_norm2_bias)                                                                        {}\n",
      "call_function  silu_66                                                                  aten.silu.default                                                        (group_norm_58,)                                                                                                                                               {}\n",
      "call_function  dropout_66                                                               aten.dropout.default                                                     (silu_66, 0.0, False)                                                                                                                                          {}\n",
      "call_function  conv2d_63                                                                aten.conv2d.default                                                      (dropout_66, p_up_blocks_3_resnets_2_conv2_weight, p_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1])                                                         {}\n",
      "call_function  conv2d_64                                                                aten.conv2d.default                                                      (cat_13, p_up_blocks_3_resnets_2_conv_shortcut_weight, p_up_blocks_3_resnets_2_conv_shortcut_bias)                                                             {}\n",
      "call_function  add_103                                                                  aten.add.Tensor                                                          (conv2d_64, conv2d_63)                                                                                                                                         {}\n",
      "call_function  div_52                                                                   aten.div.Tensor                                                          (add_103, 1.0)                                                                                                                                                 {}\n",
      "call_function  group_norm_59                                                            aten.group_norm.default                                                  (div_52, 32, p_up_blocks_3_attentions_2_norm_weight, p_up_blocks_3_attentions_2_norm_bias, 1e-06)                                                              {}\n",
      "call_function  permute_30                                                               aten.permute.default                                                     (group_norm_59, [0, 2, 3, 1])                                                                                                                                  {}\n",
      "call_function  view_150                                                                 aten.view.default                                                        (permute_30, [1, 4096, 320])                                                                                                                                   {}\n",
      "call_function  linear_204                                                               aten.linear.default                                                      (view_150, p_up_blocks_3_attentions_2_proj_in_weight, p_up_blocks_3_attentions_2_proj_in_bias)                                                                 {}\n",
      "call_function  layer_norm_45                                                            aten.layer_norm.default                                                  (linear_204, [320], p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_norm1_bias)                  {}\n",
      "call_function  linear_205                                                               aten.linear.default                                                      (layer_norm_45, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q_weight)                                                                             {}\n",
      "call_function  linear_206                                                               aten.linear.default                                                      (layer_norm_45, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k_weight)                                                                             {}\n",
      "call_function  linear_207                                                               aten.linear.default                                                      (layer_norm_45, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v_weight)                                                                             {}\n",
      "call_function  view_151                                                                 aten.view.default                                                        (linear_205, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_120                                                            aten.transpose.int                                                       (view_151, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_152                                                                 aten.view.default                                                        (linear_206, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_121                                                            aten.transpose.int                                                       (view_152, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_153                                                                 aten.view.default                                                        (linear_207, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_122                                                            aten.transpose.int                                                       (view_153, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_30                                          aten.scaled_dot_product_attention.default                                (transpose_120, transpose_121, transpose_122)                                                                                                                  {}\n",
      "call_function  transpose_123                                                            aten.transpose.int                                                       (scaled_dot_product_attention_30, 1, 2)                                                                                                                        {}\n",
      "call_function  view_154                                                                 aten.view.default                                                        (transpose_123, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_32                                                              aten._to_copy.default                                                    (view_154,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_208                                                               aten.linear.default                                                      (_to_copy_32, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0_bias)      {}\n",
      "call_function  dropout_67                                                               aten.dropout.default                                                     (linear_208, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_53                                                                   aten.div.Tensor                                                          (dropout_67, 1.0)                                                                                                                                              {}\n",
      "call_function  add_104                                                                  aten.add.Tensor                                                          (div_53, linear_204)                                                                                                                                           {}\n",
      "call_function  layer_norm_46                                                            aten.layer_norm.default                                                  (add_104, [320], p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_norm2_bias)                     {}\n",
      "call_function  linear_209                                                               aten.linear.default                                                      (layer_norm_46, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q_weight)                                                                             {}\n",
      "call_function  linear_210                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k_weight)                                                                     {}\n",
      "call_function  linear_211                                                               aten.linear.default                                                      (encoder_hidden_states, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v_weight)                                                                     {}\n",
      "call_function  view_155                                                                 aten.view.default                                                        (linear_209, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_124                                                            aten.transpose.int                                                       (view_155, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_156                                                                 aten.view.default                                                        (linear_210, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_125                                                            aten.transpose.int                                                       (view_156, 1, 2)                                                                                                                                               {}\n",
      "call_function  view_157                                                                 aten.view.default                                                        (linear_211, [1, -1, 5, 64])                                                                                                                                   {}\n",
      "call_function  transpose_126                                                            aten.transpose.int                                                       (view_157, 1, 2)                                                                                                                                               {}\n",
      "call_function  scaled_dot_product_attention_31                                          aten.scaled_dot_product_attention.default                                (transpose_124, transpose_125, transpose_126)                                                                                                                  {}\n",
      "call_function  transpose_127                                                            aten.transpose.int                                                       (scaled_dot_product_attention_31, 1, 2)                                                                                                                        {}\n",
      "call_function  view_158                                                                 aten.view.default                                                        (transpose_127, [1, -1, 320])                                                                                                                                  {}\n",
      "call_function  _to_copy_33                                                              aten._to_copy.default                                                    (view_158,)                                                                                                                                                    {'dtype': torch.float32}\n",
      "call_function  linear_212                                                               aten.linear.default                                                      (_to_copy_33, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0_bias)      {}\n",
      "call_function  dropout_68                                                               aten.dropout.default                                                     (linear_212, 0.0, False)                                                                                                                                       {}\n",
      "call_function  div_54                                                                   aten.div.Tensor                                                          (dropout_68, 1.0)                                                                                                                                              {}\n",
      "call_function  add_105                                                                  aten.add.Tensor                                                          (div_54, add_104)                                                                                                                                              {}\n",
      "call_function  layer_norm_47                                                            aten.layer_norm.default                                                  (add_105, [320], p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_norm3_bias)                     {}\n",
      "call_function  linear_213                                                               aten.linear.default                                                      (layer_norm_47, p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj_bias)      {}\n",
      "call_function  split_15                                                                 aten.split.Tensor                                                        (linear_213, 1280, -1)                                                                                                                                         {}\n",
      "call_function  getitem_30                                                               <built-in function getitem>                                              (split_15, 0)                                                                                                                                                  {}\n",
      "call_function  getitem_31                                                               <built-in function getitem>                                              (split_15, 1)                                                                                                                                                  {}\n",
      "call_function  gelu_15                                                                  aten.gelu.default                                                        (getitem_31,)                                                                                                                                                  {}\n",
      "call_function  mul_18                                                                   aten.mul.Tensor                                                          (getitem_30, gelu_15)                                                                                                                                          {}\n",
      "call_function  dropout_69                                                               aten.dropout.default                                                     (mul_18, 0.0, False)                                                                                                                                           {}\n",
      "call_function  linear_214                                                               aten.linear.default                                                      (dropout_69, p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_weight, p_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2_bias)                   {}\n",
      "call_function  add_106                                                                  aten.add.Tensor                                                          (linear_214, add_105)                                                                                                                                          {}\n",
      "call_function  linear_215                                                               aten.linear.default                                                      (add_106, p_up_blocks_3_attentions_2_proj_out_weight, p_up_blocks_3_attentions_2_proj_out_bias)                                                                {}\n",
      "call_function  view_159                                                                 aten.view.default                                                        (linear_215, [1, 64, 64, 320])                                                                                                                                 {}\n",
      "call_function  permute_31                                                               aten.permute.default                                                     (view_159, [0, 3, 1, 2])                                                                                                                                       {}\n",
      "call_function  clone_15                                                                 aten.clone.default                                                       (permute_31,)                                                                                                                                                  {'memory_format': torch.contiguous_format}\n",
      "call_function  add_107                                                                  aten.add.Tensor                                                          (clone_15, div_52)                                                                                                                                             {}\n",
      "call_function  group_norm_60                                                            aten.group_norm.default                                                  (add_107, 32, p_conv_norm_out_weight, p_conv_norm_out_bias)                                                                                                    {}\n",
      "call_function  silu_67                                                                  aten.silu.default                                                        (group_norm_60,)                                                                                                                                               {}\n",
      "call_function  conv2d_65                                                                aten.conv2d.default                                                      (silu_67, p_conv_out_weight, p_conv_out_bias, [1, 1], [1, 1])                                                                                                  {}\n",
      "output         output                                                                   output                                                                   ((conv2d_65,),)                                                                                                                                                {}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import UNet2DConditionModel\n",
    "from torch.export import export, save\n",
    "from torch.export import ExportedProgram\n",
    "import torch.nn.functional as F   \n",
    "\n",
    "#monkey patch\n",
    "#F.gelu = F.relu\n",
    "#F.silu = F.relu \n",
    "\n",
    "CKPT  = \"prs-eth/marigold-depth-v1-1\"\n",
    "unet  = UNet2DConditionModel.from_pretrained(CKPT, subfolder=\"unet\").cpu()\n",
    "unet.disable_xformers_memory_efficient_attention()\n",
    "\n",
    "example = (\n",
    "    torch.randn(1, 8, 64, 64),   # latent\n",
    "    torch.tensor([0]),           # timestep\n",
    "    torch.randn(1, 77, 1024)     # text enc\n",
    ")\n",
    "\n",
    "gm_unet: ExportedProgram = export(unet, example)\n",
    "save(gm_unet, \"unet_fp32.ep\")\n",
    "gm_unet.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0615e66f",
   "metadata": {},
   "source": [
    "### 2. Find unsupported ops like GELU and GroupNorm for swapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e6d483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupported ops: {'aten.permute.default', 'aten.sin.default', 'aten.cos.default', 'aten.exp.default', 'aten.unsqueeze.default', 'aten.add.Tensor', 'aten.expand.default', 'aten.mul.Tensor', 'aten.linear.default', 'aten.transpose.int', '<built-in function getitem>', 'aten.scaled_dot_product_attention.default', 'aten.cat.default', 'aten.group_norm.default', 'aten._to_copy.default', 'aten.relu.default', 'aten.dropout.default', 'aten.conv2d.default', 'aten.slice.Tensor', 'aten.view.default', 'aten.upsample_nearest2d.vec', 'aten.clone.default', 'aten.layer_norm.default', 'aten.arange.start', 'aten.div.Tensor', 'aten.split.Tensor'}\n"
     ]
    }
   ],
   "source": [
    "unsupported_ops = set()\n",
    "for n in gm_unet.graph.nodes:\n",
    "    if n.op in (\"call_function\", \"call_module\") and \"quant\" not in str(n.target):\n",
    "        unsupported_ops.add(str(n.target))\n",
    "print(\"Unsupported ops:\", unsupported_ops) # helps us specific the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d41847",
   "metadata": {},
   "source": [
    "### 3. Patch the model ops and apply to UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c02bd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# def patch_model_for_qat(module: nn.Module):\n",
    "#     for name, child in module.named_children():\n",
    "#         # swap layers first \n",
    "#         if isinstance(child, nn.GELU):\n",
    "#             setattr(module, name, nn.ReLU(inplace=True))\n",
    "#         elif isinstance(child, nn.SiLU):\n",
    "#             setattr(module, name, nn.ReLU(inplace=True))\n",
    "#         elif isinstance(child, nn.GroupNorm):\n",
    "#             setattr(module, name, nn.BatchNorm2d(child.num_channels))\n",
    "#         elif isinstance(child, nn.LayerNorm):\n",
    "#             setattr(module, name, nn.Identity())\n",
    "#         #  then recurse regardless of what it is now\n",
    "#         patch_model_for_qat(getattr(module, name))\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- module-level swaps -------------------------------------------------\n",
    "_BAD2GOOD = {\n",
    "    nn.GELU: lambda _: nn.ReLU(inplace=False),\n",
    "    nn.SiLU: lambda _: nn.Hardswish(),\n",
    "}\n",
    "\n",
    "# --- functional activation swaps ---------------------------------------\n",
    "_BAD_FUNCS = {F.gelu, F.silu}\n",
    "_GOOD_FUNC = F.relu\n",
    "\n",
    "def patch_model_for_qat(module: nn.Module):\n",
    "    \"\"\"Recursively replace unsupported modules and functional activations.\"\"\"\n",
    "    # 1) swap child modules\n",
    "    for name, child in list(module.named_children()):\n",
    "        for bad_cls, make_good in _BAD2GOOD.items():\n",
    "            if isinstance(child, bad_cls):\n",
    "                setattr(module, name, make_good(child))\n",
    "                child = getattr(module, name)          # updated ref\n",
    "                break\n",
    "        patch_model_for_qat(child)                     # recurse\n",
    "\n",
    "    # 2) swap stored functional refs\n",
    "    for attr_name, attr_val in vars(module).items():\n",
    "        if callable(attr_val) and attr_val in _BAD_FUNCS:\n",
    "            setattr(module, attr_name, _GOOD_FUNC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcbccf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_model_for_qat(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b67418a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "At least one forbidden layer slipped through!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#quick sanity check\u001b[39;00m\n\u001b[1;32m      2\u001b[0m bad_types \u001b[38;5;241m=\u001b[39m (nn\u001b[38;5;241m.\u001b[39mGELU, nn\u001b[38;5;241m.\u001b[39mSiLU, nn\u001b[38;5;241m.\u001b[39mGroupNorm, nn\u001b[38;5;241m.\u001b[39mLayerNorm)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(m, bad_types) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m unet\u001b[38;5;241m.\u001b[39mmodules()), \\\n\u001b[1;32m      5\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one forbidden layer slipped through!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer-type sweep is clean.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m gm_patched \u001b[38;5;241m=\u001b[39m export(unet, example)\n",
      "\u001b[0;31mAssertionError\u001b[0m: At least one forbidden layer slipped through!"
     ]
    }
   ],
   "source": [
    "#quick sanity check\n",
    "bad_types = (nn.GELU, nn.SiLU, nn.GroupNorm, nn.LayerNorm)\n",
    "\n",
    "assert not any(isinstance(m, bad_types) for m in unet.modules()), \\\n",
    "       \"At least one forbidden layer slipped through!\"\n",
    "print(\"Layer-type sweep is clean.\")\n",
    "\n",
    "gm_patched = export(unet, example)\n",
    "ops_left = {str(n.target) for n in gm_patched.graph.nodes\n",
    "            if \"gelu\" in str(n.target) \n",
    "            or \"silu\" in str(n.target)\n",
    "            or \"group_norm\" in str(n.target)\n",
    "            or \"layer_norm\" in str(n.target)}\n",
    "print(\"Ops still present:\", ops_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995b4bb",
   "metadata": {},
   "source": [
    "### 4. Add quantization stubs, prepare the model, and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7516a256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/torch/ao/quantization/observer.py:221: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights written  unet_qat_ready.pt\n"
     ]
    }
   ],
   "source": [
    "# for state_dict we must rebuild the quant graph at runtime\n",
    "from torch.ao.quantization import get_default_qat_qconfig, prepare_qat\n",
    "import torch, copy\n",
    "\n",
    "torch.backends.quantized.engine = \"fbgemm\" \n",
    "# Set QAT config and prepare\n",
    "unet.qconfig = get_default_qat_qconfig(\"fbgemm\")\n",
    "qat_model = copy.deepcopy(unet)\n",
    "qat_model.train()   \n",
    "prepare_qat(qat_model, inplace=True)\n",
    "\n",
    "torch.save(qat_model.state_dict(), \"unet_qat_ready.pt\")\n",
    "print(\"weights written  unet_qat_ready.pt\")\n",
    "\n",
    "# Save the fully prepared model (structure + weights)\n",
    "# torch.save(qat_model, \"unet_qat_prepared.pth\")  # full model object with stubs\n",
    "# print(\"Full QAT module saved  unet_qat_prepared.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c3a5602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/torch/ao/quantization/observer.py:221: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full QAT module saved  unet_qat_dev.pth  (via dill)\n"
     ]
    }
   ],
   "source": [
    "# for full model using dill, includes enitre graph\n",
    "from torch.ao.quantization import get_default_qat_qconfig, prepare_qat\n",
    "import torch, copy, dill\n",
    "\n",
    "torch.backends.quantized.engine = \"fbgemm\" \n",
    "# Set QAT config and prepare\n",
    "unet.qconfig = get_default_qat_qconfig(\"fbgemm\")\n",
    "qat_model = copy.deepcopy(unet)\n",
    "qat_model.train()   \n",
    "prepare_qat(qat_model, inplace=True)\n",
    "\n",
    "torch.save(\n",
    "    qat_model,\n",
    "    \"unet_qat_dev.pth\",\n",
    "    pickle_module=dill,\n",
    "    pickle_protocol=dill.HIGHEST_PROTOCOL,\n",
    ")\n",
    "print(\"Full QAT module saved  unet_qat_dev.pth  (via dill)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3121cd3",
   "metadata": {},
   "source": [
    "### 5. Sanity check  Load and run dummy input through QAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c4e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/torch/ao/quantization/observer.py:221: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_66440/1813139022.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"unet_qat_ready.pt\", map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-dict QAT model  output shape: torch.Size([1, 4, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.ao.quantization import get_default_qat_qconfig, prepare_qat\n",
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "# -------- rebuild identical QAT skeleton --------------------------\n",
    "qat_unet = UNet2DConditionModel.from_pretrained(CKPT, subfolder=\"unet\").cpu()\n",
    "patch_model_for_qat(qat_unet)                          # your patcher\n",
    "torch.backends.quantized.engine = \"fbgemm\"\n",
    "qat_unet.qconfig = get_default_qat_qconfig(\"fbgemm\")\n",
    "qat_unet.train()\n",
    "prepare_qat(qat_unet, inplace=True)                    # add observers\n",
    "qat_unet.eval()\n",
    "\n",
    "# -------- load weights-only checkpoint ----------------------------\n",
    "state_dict = torch.load(\"unet_qat_ready.pt\", map_location=\"cpu\")\n",
    "qat_unet.load_state_dict(state_dict)\n",
    "\n",
    "# -------- dummy forward sanity check ------------------------------\n",
    "with torch.no_grad():\n",
    "    latent   = torch.randn(1, 8, 64, 64)\n",
    "    timestep = torch.tensor([0])\n",
    "    cond     = torch.randn(1, 77, 1024)\n",
    "    out      = qat_unet(latent, timestep, cond)\n",
    "\n",
    "print(\"State-dict QAT model  output shape:\", out.sample.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f6bd4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  minimal, safe loader for the dill file \n",
    "# import dill, torch\n",
    "# torch.backends.quantized.engine = \"fbgemm\"   # backend first!\n",
    "\n",
    "# qat = torch.load(\"unet_qat_dev.pth\",\n",
    "#                  pickle_module=dill,\n",
    "#                  map_location=\"cpu\")\n",
    "\n",
    "# # ---------- initialise observer buffers once -----------------------\n",
    "# qat.train()                                   # observers/fake-quants active\n",
    "# with torch.no_grad():\n",
    "#     _ = qat(torch.randn(1, 8, 64, 64),\n",
    "#             torch.tensor([0]),\n",
    "#             torch.randn(1, 77, 1024))         # warm-up pass\n",
    "# qat.eval()                                    # freeze for inference\n",
    "\n",
    "# # ---------- real sanity check --------------------------------------\n",
    "# with torch.no_grad():\n",
    "#     out = qat(torch.randn(1, 8, 64, 64),\n",
    "#               torch.tensor([0]),\n",
    "#               torch.randn(1, 77, 1024))\n",
    "# print(\"dill-pickle QAT model \", out.sample.shape)   # should print [1, 4, 64, 64]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d8b5d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55f3e3",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dfd56f",
   "metadata": {},
   "source": [
    "### 1. Load the model and save as a torch.export graph for op inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a7eef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE exported   vae_fp32.ep\n",
      "opcode         name                                                              target                                                            args                                                                                                                                                            kwargs\n",
      "-------------  ----------------------------------------------------------------  ----------------------------------------------------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------------------------------\n",
      "placeholder    p_encoder_conv_in_weight                                          p_encoder_conv_in_weight                                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_in_bias                                            p_encoder_conv_in_bias                                            ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm1_weight                    p_encoder_down_blocks_0_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm1_bias                      p_encoder_down_blocks_0_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv1_weight                    p_encoder_down_blocks_0_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv1_bias                      p_encoder_down_blocks_0_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm2_weight                    p_encoder_down_blocks_0_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_norm2_bias                      p_encoder_down_blocks_0_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv2_weight                    p_encoder_down_blocks_0_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_0_conv2_bias                      p_encoder_down_blocks_0_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm1_weight                    p_encoder_down_blocks_0_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm1_bias                      p_encoder_down_blocks_0_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv1_weight                    p_encoder_down_blocks_0_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv1_bias                      p_encoder_down_blocks_0_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm2_weight                    p_encoder_down_blocks_0_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_norm2_bias                      p_encoder_down_blocks_0_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv2_weight                    p_encoder_down_blocks_0_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_resnets_1_conv2_bias                      p_encoder_down_blocks_0_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_downsamplers_0_conv_weight                p_encoder_down_blocks_0_downsamplers_0_conv_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_0_downsamplers_0_conv_bias                  p_encoder_down_blocks_0_downsamplers_0_conv_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm1_weight                    p_encoder_down_blocks_1_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm1_bias                      p_encoder_down_blocks_1_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv1_weight                    p_encoder_down_blocks_1_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv1_bias                      p_encoder_down_blocks_1_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm2_weight                    p_encoder_down_blocks_1_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_norm2_bias                      p_encoder_down_blocks_1_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv2_weight                    p_encoder_down_blocks_1_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv2_bias                      p_encoder_down_blocks_1_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight            p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight            ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias              p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias              ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm1_weight                    p_encoder_down_blocks_1_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm1_bias                      p_encoder_down_blocks_1_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv1_weight                    p_encoder_down_blocks_1_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv1_bias                      p_encoder_down_blocks_1_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm2_weight                    p_encoder_down_blocks_1_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_norm2_bias                      p_encoder_down_blocks_1_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv2_weight                    p_encoder_down_blocks_1_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_resnets_1_conv2_bias                      p_encoder_down_blocks_1_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_downsamplers_0_conv_weight                p_encoder_down_blocks_1_downsamplers_0_conv_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_1_downsamplers_0_conv_bias                  p_encoder_down_blocks_1_downsamplers_0_conv_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm1_weight                    p_encoder_down_blocks_2_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm1_bias                      p_encoder_down_blocks_2_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv1_weight                    p_encoder_down_blocks_2_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv1_bias                      p_encoder_down_blocks_2_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm2_weight                    p_encoder_down_blocks_2_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_norm2_bias                      p_encoder_down_blocks_2_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv2_weight                    p_encoder_down_blocks_2_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv2_bias                      p_encoder_down_blocks_2_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight            p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight            ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias              p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias              ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm1_weight                    p_encoder_down_blocks_2_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm1_bias                      p_encoder_down_blocks_2_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv1_weight                    p_encoder_down_blocks_2_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv1_bias                      p_encoder_down_blocks_2_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm2_weight                    p_encoder_down_blocks_2_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_norm2_bias                      p_encoder_down_blocks_2_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv2_weight                    p_encoder_down_blocks_2_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_resnets_1_conv2_bias                      p_encoder_down_blocks_2_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_downsamplers_0_conv_weight                p_encoder_down_blocks_2_downsamplers_0_conv_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_2_downsamplers_0_conv_bias                  p_encoder_down_blocks_2_downsamplers_0_conv_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm1_weight                    p_encoder_down_blocks_3_resnets_0_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm1_bias                      p_encoder_down_blocks_3_resnets_0_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv1_weight                    p_encoder_down_blocks_3_resnets_0_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv1_bias                      p_encoder_down_blocks_3_resnets_0_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm2_weight                    p_encoder_down_blocks_3_resnets_0_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_norm2_bias                      p_encoder_down_blocks_3_resnets_0_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv2_weight                    p_encoder_down_blocks_3_resnets_0_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_0_conv2_bias                      p_encoder_down_blocks_3_resnets_0_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm1_weight                    p_encoder_down_blocks_3_resnets_1_norm1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm1_bias                      p_encoder_down_blocks_3_resnets_1_norm1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv1_weight                    p_encoder_down_blocks_3_resnets_1_conv1_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv1_bias                      p_encoder_down_blocks_3_resnets_1_conv1_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm2_weight                    p_encoder_down_blocks_3_resnets_1_norm2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_norm2_bias                      p_encoder_down_blocks_3_resnets_1_norm2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv2_weight                    p_encoder_down_blocks_3_resnets_1_conv2_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_down_blocks_3_resnets_1_conv2_bias                      p_encoder_down_blocks_3_resnets_1_conv2_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm1_weight                        p_encoder_mid_block_resnets_0_norm1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm1_bias                          p_encoder_mid_block_resnets_0_norm1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv1_weight                        p_encoder_mid_block_resnets_0_conv1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv1_bias                          p_encoder_mid_block_resnets_0_conv1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm2_weight                        p_encoder_mid_block_resnets_0_norm2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_norm2_bias                          p_encoder_mid_block_resnets_0_norm2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv2_weight                        p_encoder_mid_block_resnets_0_conv2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_0_conv2_bias                          p_encoder_mid_block_resnets_0_conv2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_group_norm_weight                p_encoder_mid_block_attentions_0_group_norm_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_group_norm_bias                  p_encoder_mid_block_attentions_0_group_norm_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_q_weight                      p_encoder_mid_block_attentions_0_to_q_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_q_bias                        p_encoder_mid_block_attentions_0_to_q_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_k_weight                      p_encoder_mid_block_attentions_0_to_k_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_k_bias                        p_encoder_mid_block_attentions_0_to_k_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_v_weight                      p_encoder_mid_block_attentions_0_to_v_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_v_bias                        p_encoder_mid_block_attentions_0_to_v_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_out_0_weight                  p_encoder_mid_block_attentions_0_to_out_0_weight                  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_attentions_0_to_out_0_bias                    p_encoder_mid_block_attentions_0_to_out_0_bias                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_norm_out_weight                                    p_encoder_conv_norm_out_weight                                    ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_norm_out_bias                                      p_encoder_conv_norm_out_bias                                      ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_out_weight                                         p_encoder_conv_out_weight                                         ()                                                                                                                                                              {}\n",
      "placeholder    p_encoder_conv_out_bias                                           p_encoder_conv_out_bias                                           ()                                                                                                                                                              {}\n",
      "placeholder    p_quant_conv_weight                                               p_quant_conv_weight                                               ()                                                                                                                                                              {}\n",
      "placeholder    p_quant_conv_bias                                                 p_quant_conv_bias                                                 ()                                                                                                                                                              {}\n",
      "placeholder    p_post_quant_conv_weight                                          p_post_quant_conv_weight                                          ()                                                                                                                                                              {}\n",
      "placeholder    p_post_quant_conv_bias                                            p_post_quant_conv_bias                                            ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_in_weight                                          p_decoder_conv_in_weight                                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_in_bias                                            p_decoder_conv_in_bias                                            ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm1_weight                        p_decoder_mid_block_resnets_0_norm1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm1_bias                          p_decoder_mid_block_resnets_0_norm1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv1_weight                        p_decoder_mid_block_resnets_0_conv1_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv1_bias                          p_decoder_mid_block_resnets_0_conv1_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm2_weight                        p_decoder_mid_block_resnets_0_norm2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_norm2_bias                          p_decoder_mid_block_resnets_0_norm2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv2_weight                        p_decoder_mid_block_resnets_0_conv2_weight                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_0_conv2_bias                          p_decoder_mid_block_resnets_0_conv2_bias                          ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_group_norm_weight                p_decoder_mid_block_attentions_0_group_norm_weight                ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_group_norm_bias                  p_decoder_mid_block_attentions_0_group_norm_bias                  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_q_weight                      p_decoder_mid_block_attentions_0_to_q_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_q_bias                        p_decoder_mid_block_attentions_0_to_q_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_k_weight                      p_decoder_mid_block_attentions_0_to_k_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_k_bias                        p_decoder_mid_block_attentions_0_to_k_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_v_weight                      p_decoder_mid_block_attentions_0_to_v_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_v_bias                        p_decoder_mid_block_attentions_0_to_v_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_out_0_weight                  p_decoder_mid_block_attentions_0_to_out_0_weight                  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_attentions_0_to_out_0_bias                    p_decoder_mid_block_attentions_0_to_out_0_bias                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight  ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm1_weight                      p_decoder_up_blocks_0_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm1_bias                        p_decoder_up_blocks_0_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv1_weight                      p_decoder_up_blocks_0_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv1_bias                        p_decoder_up_blocks_0_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm2_weight                      p_decoder_up_blocks_0_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_norm2_bias                        p_decoder_up_blocks_0_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv2_weight                      p_decoder_up_blocks_0_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_0_conv2_bias                        p_decoder_up_blocks_0_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm1_weight                      p_decoder_up_blocks_0_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm1_bias                        p_decoder_up_blocks_0_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv1_weight                      p_decoder_up_blocks_0_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv1_bias                        p_decoder_up_blocks_0_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm2_weight                      p_decoder_up_blocks_0_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_norm2_bias                        p_decoder_up_blocks_0_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv2_weight                      p_decoder_up_blocks_0_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_1_conv2_bias                        p_decoder_up_blocks_0_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm1_weight                      p_decoder_up_blocks_0_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm1_bias                        p_decoder_up_blocks_0_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv1_weight                      p_decoder_up_blocks_0_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv1_bias                        p_decoder_up_blocks_0_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm2_weight                      p_decoder_up_blocks_0_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_norm2_bias                        p_decoder_up_blocks_0_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv2_weight                      p_decoder_up_blocks_0_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_resnets_2_conv2_bias                        p_decoder_up_blocks_0_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_upsamplers_0_conv_weight                    p_decoder_up_blocks_0_upsamplers_0_conv_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_0_upsamplers_0_conv_bias                      p_decoder_up_blocks_0_upsamplers_0_conv_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm1_weight                      p_decoder_up_blocks_1_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm1_bias                        p_decoder_up_blocks_1_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv1_weight                      p_decoder_up_blocks_1_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv1_bias                        p_decoder_up_blocks_1_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm2_weight                      p_decoder_up_blocks_1_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_norm2_bias                        p_decoder_up_blocks_1_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv2_weight                      p_decoder_up_blocks_1_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_0_conv2_bias                        p_decoder_up_blocks_1_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm1_weight                      p_decoder_up_blocks_1_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm1_bias                        p_decoder_up_blocks_1_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv1_weight                      p_decoder_up_blocks_1_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv1_bias                        p_decoder_up_blocks_1_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm2_weight                      p_decoder_up_blocks_1_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_norm2_bias                        p_decoder_up_blocks_1_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv2_weight                      p_decoder_up_blocks_1_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_1_conv2_bias                        p_decoder_up_blocks_1_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm1_weight                      p_decoder_up_blocks_1_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm1_bias                        p_decoder_up_blocks_1_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv1_weight                      p_decoder_up_blocks_1_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv1_bias                        p_decoder_up_blocks_1_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm2_weight                      p_decoder_up_blocks_1_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_norm2_bias                        p_decoder_up_blocks_1_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv2_weight                      p_decoder_up_blocks_1_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_resnets_2_conv2_bias                        p_decoder_up_blocks_1_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_upsamplers_0_conv_weight                    p_decoder_up_blocks_1_upsamplers_0_conv_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_1_upsamplers_0_conv_bias                      p_decoder_up_blocks_1_upsamplers_0_conv_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm1_weight                      p_decoder_up_blocks_2_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm1_bias                        p_decoder_up_blocks_2_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv1_weight                      p_decoder_up_blocks_2_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv1_bias                        p_decoder_up_blocks_2_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm2_weight                      p_decoder_up_blocks_2_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_norm2_bias                        p_decoder_up_blocks_2_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv2_weight                      p_decoder_up_blocks_2_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv2_bias                        p_decoder_up_blocks_2_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight              p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight              ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias                p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias                ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm1_weight                      p_decoder_up_blocks_2_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm1_bias                        p_decoder_up_blocks_2_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv1_weight                      p_decoder_up_blocks_2_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv1_bias                        p_decoder_up_blocks_2_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm2_weight                      p_decoder_up_blocks_2_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_norm2_bias                        p_decoder_up_blocks_2_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv2_weight                      p_decoder_up_blocks_2_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_1_conv2_bias                        p_decoder_up_blocks_2_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm1_weight                      p_decoder_up_blocks_2_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm1_bias                        p_decoder_up_blocks_2_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv1_weight                      p_decoder_up_blocks_2_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv1_bias                        p_decoder_up_blocks_2_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm2_weight                      p_decoder_up_blocks_2_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_norm2_bias                        p_decoder_up_blocks_2_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv2_weight                      p_decoder_up_blocks_2_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_resnets_2_conv2_bias                        p_decoder_up_blocks_2_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_upsamplers_0_conv_weight                    p_decoder_up_blocks_2_upsamplers_0_conv_weight                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_2_upsamplers_0_conv_bias                      p_decoder_up_blocks_2_upsamplers_0_conv_bias                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm1_weight                      p_decoder_up_blocks_3_resnets_0_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm1_bias                        p_decoder_up_blocks_3_resnets_0_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv1_weight                      p_decoder_up_blocks_3_resnets_0_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv1_bias                        p_decoder_up_blocks_3_resnets_0_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm2_weight                      p_decoder_up_blocks_3_resnets_0_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_norm2_bias                        p_decoder_up_blocks_3_resnets_0_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv2_weight                      p_decoder_up_blocks_3_resnets_0_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv2_bias                        p_decoder_up_blocks_3_resnets_0_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight              p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight              ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias                p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias                ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm1_weight                      p_decoder_up_blocks_3_resnets_1_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm1_bias                        p_decoder_up_blocks_3_resnets_1_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv1_weight                      p_decoder_up_blocks_3_resnets_1_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv1_bias                        p_decoder_up_blocks_3_resnets_1_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm2_weight                      p_decoder_up_blocks_3_resnets_1_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_norm2_bias                        p_decoder_up_blocks_3_resnets_1_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv2_weight                      p_decoder_up_blocks_3_resnets_1_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_1_conv2_bias                        p_decoder_up_blocks_3_resnets_1_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm1_weight                      p_decoder_up_blocks_3_resnets_2_norm1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm1_bias                        p_decoder_up_blocks_3_resnets_2_norm1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv1_weight                      p_decoder_up_blocks_3_resnets_2_conv1_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv1_bias                        p_decoder_up_blocks_3_resnets_2_conv1_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm2_weight                      p_decoder_up_blocks_3_resnets_2_norm2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_norm2_bias                        p_decoder_up_blocks_3_resnets_2_norm2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv2_weight                      p_decoder_up_blocks_3_resnets_2_conv2_weight                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_up_blocks_3_resnets_2_conv2_bias                        p_decoder_up_blocks_3_resnets_2_conv2_bias                        ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_norm_out_weight                                    p_decoder_conv_norm_out_weight                                    ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_norm_out_bias                                      p_decoder_conv_norm_out_bias                                      ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_out_weight                                         p_decoder_conv_out_weight                                         ()                                                                                                                                                              {}\n",
      "placeholder    p_decoder_conv_out_bias                                           p_decoder_conv_out_bias                                           ()                                                                                                                                                              {}\n",
      "placeholder    sample                                                            sample                                                            ()                                                                                                                                                              {}\n",
      "call_function  conv2d                                                            aten.conv2d.default                                               (sample, p_encoder_conv_in_weight, p_encoder_conv_in_bias, [1, 1], [1, 1])                                                                                      {}\n",
      "call_function  group_norm                                                        aten.group_norm.default                                           (conv2d, 32, p_encoder_down_blocks_0_resnets_0_norm1_weight, p_encoder_down_blocks_0_resnets_0_norm1_bias, 1e-06)                                               {}\n",
      "call_function  relu                                                              aten.relu.default                                                 (group_norm,)                                                                                                                                                   {}\n",
      "call_function  conv2d_1                                                          aten.conv2d.default                                               (relu, p_encoder_down_blocks_0_resnets_0_conv1_weight, p_encoder_down_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1])                                            {}\n",
      "call_function  group_norm_1                                                      aten.group_norm.default                                           (conv2d_1, 32, p_encoder_down_blocks_0_resnets_0_norm2_weight, p_encoder_down_blocks_0_resnets_0_norm2_bias, 1e-06)                                             {}\n",
      "call_function  relu_1                                                            aten.relu.default                                                 (group_norm_1,)                                                                                                                                                 {}\n",
      "call_function  dropout                                                           aten.dropout.default                                              (relu_1, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_2                                                          aten.conv2d.default                                               (dropout, p_encoder_down_blocks_0_resnets_0_conv2_weight, p_encoder_down_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  add                                                               aten.add.Tensor                                                   (conv2d, conv2d_2)                                                                                                                                              {}\n",
      "call_function  div                                                               aten.div.Tensor                                                   (add, 1.0)                                                                                                                                                      {}\n",
      "call_function  group_norm_2                                                      aten.group_norm.default                                           (div, 32, p_encoder_down_blocks_0_resnets_1_norm1_weight, p_encoder_down_blocks_0_resnets_1_norm1_bias, 1e-06)                                                  {}\n",
      "call_function  relu_2                                                            aten.relu.default                                                 (group_norm_2,)                                                                                                                                                 {}\n",
      "call_function  conv2d_3                                                          aten.conv2d.default                                               (relu_2, p_encoder_down_blocks_0_resnets_1_conv1_weight, p_encoder_down_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_3                                                      aten.group_norm.default                                           (conv2d_3, 32, p_encoder_down_blocks_0_resnets_1_norm2_weight, p_encoder_down_blocks_0_resnets_1_norm2_bias, 1e-06)                                             {}\n",
      "call_function  relu_3                                                            aten.relu.default                                                 (group_norm_3,)                                                                                                                                                 {}\n",
      "call_function  dropout_1                                                         aten.dropout.default                                              (relu_3, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_4                                                          aten.conv2d.default                                               (dropout_1, p_encoder_down_blocks_0_resnets_1_conv2_weight, p_encoder_down_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_1                                                             aten.add.Tensor                                                   (div, conv2d_4)                                                                                                                                                 {}\n",
      "call_function  div_1                                                             aten.div.Tensor                                                   (add_1, 1.0)                                                                                                                                                    {}\n",
      "call_function  pad                                                               aten.pad.default                                                  (div_1, [0, 1, 0, 1], 'constant', 0.0)                                                                                                                          {}\n",
      "call_function  conv2d_5                                                          aten.conv2d.default                                               (pad, p_encoder_down_blocks_0_downsamplers_0_conv_weight, p_encoder_down_blocks_0_downsamplers_0_conv_bias, [2, 2])                                             {}\n",
      "call_function  group_norm_4                                                      aten.group_norm.default                                           (conv2d_5, 32, p_encoder_down_blocks_1_resnets_0_norm1_weight, p_encoder_down_blocks_1_resnets_0_norm1_bias, 1e-06)                                             {}\n",
      "call_function  relu_4                                                            aten.relu.default                                                 (group_norm_4,)                                                                                                                                                 {}\n",
      "call_function  conv2d_6                                                          aten.conv2d.default                                               (relu_4, p_encoder_down_blocks_1_resnets_0_conv1_weight, p_encoder_down_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_5                                                      aten.group_norm.default                                           (conv2d_6, 32, p_encoder_down_blocks_1_resnets_0_norm2_weight, p_encoder_down_blocks_1_resnets_0_norm2_bias, 1e-06)                                             {}\n",
      "call_function  relu_5                                                            aten.relu.default                                                 (group_norm_5,)                                                                                                                                                 {}\n",
      "call_function  dropout_2                                                         aten.dropout.default                                              (relu_5, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_7                                                          aten.conv2d.default                                               (dropout_2, p_encoder_down_blocks_1_resnets_0_conv2_weight, p_encoder_down_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  conv2d_8                                                          aten.conv2d.default                                               (conv2d_5, p_encoder_down_blocks_1_resnets_0_conv_shortcut_weight, p_encoder_down_blocks_1_resnets_0_conv_shortcut_bias)                                        {}\n",
      "call_function  add_2                                                             aten.add.Tensor                                                   (conv2d_8, conv2d_7)                                                                                                                                            {}\n",
      "call_function  div_2                                                             aten.div.Tensor                                                   (add_2, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_6                                                      aten.group_norm.default                                           (div_2, 32, p_encoder_down_blocks_1_resnets_1_norm1_weight, p_encoder_down_blocks_1_resnets_1_norm1_bias, 1e-06)                                                {}\n",
      "call_function  relu_6                                                            aten.relu.default                                                 (group_norm_6,)                                                                                                                                                 {}\n",
      "call_function  conv2d_9                                                          aten.conv2d.default                                               (relu_6, p_encoder_down_blocks_1_resnets_1_conv1_weight, p_encoder_down_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_7                                                      aten.group_norm.default                                           (conv2d_9, 32, p_encoder_down_blocks_1_resnets_1_norm2_weight, p_encoder_down_blocks_1_resnets_1_norm2_bias, 1e-06)                                             {}\n",
      "call_function  relu_7                                                            aten.relu.default                                                 (group_norm_7,)                                                                                                                                                 {}\n",
      "call_function  dropout_3                                                         aten.dropout.default                                              (relu_7, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_10                                                         aten.conv2d.default                                               (dropout_3, p_encoder_down_blocks_1_resnets_1_conv2_weight, p_encoder_down_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_3                                                             aten.add.Tensor                                                   (div_2, conv2d_10)                                                                                                                                              {}\n",
      "call_function  div_3                                                             aten.div.Tensor                                                   (add_3, 1.0)                                                                                                                                                    {}\n",
      "call_function  pad_1                                                             aten.pad.default                                                  (div_3, [0, 1, 0, 1], 'constant', 0.0)                                                                                                                          {}\n",
      "call_function  conv2d_11                                                         aten.conv2d.default                                               (pad_1, p_encoder_down_blocks_1_downsamplers_0_conv_weight, p_encoder_down_blocks_1_downsamplers_0_conv_bias, [2, 2])                                           {}\n",
      "call_function  group_norm_8                                                      aten.group_norm.default                                           (conv2d_11, 32, p_encoder_down_blocks_2_resnets_0_norm1_weight, p_encoder_down_blocks_2_resnets_0_norm1_bias, 1e-06)                                            {}\n",
      "call_function  relu_8                                                            aten.relu.default                                                 (group_norm_8,)                                                                                                                                                 {}\n",
      "call_function  conv2d_12                                                         aten.conv2d.default                                               (relu_8, p_encoder_down_blocks_2_resnets_0_conv1_weight, p_encoder_down_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  group_norm_9                                                      aten.group_norm.default                                           (conv2d_12, 32, p_encoder_down_blocks_2_resnets_0_norm2_weight, p_encoder_down_blocks_2_resnets_0_norm2_bias, 1e-06)                                            {}\n",
      "call_function  relu_9                                                            aten.relu.default                                                 (group_norm_9,)                                                                                                                                                 {}\n",
      "call_function  dropout_4                                                         aten.dropout.default                                              (relu_9, 0.0, False)                                                                                                                                            {}\n",
      "call_function  conv2d_13                                                         aten.conv2d.default                                               (dropout_4, p_encoder_down_blocks_2_resnets_0_conv2_weight, p_encoder_down_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  conv2d_14                                                         aten.conv2d.default                                               (conv2d_11, p_encoder_down_blocks_2_resnets_0_conv_shortcut_weight, p_encoder_down_blocks_2_resnets_0_conv_shortcut_bias)                                       {}\n",
      "call_function  add_4                                                             aten.add.Tensor                                                   (conv2d_14, conv2d_13)                                                                                                                                          {}\n",
      "call_function  div_4                                                             aten.div.Tensor                                                   (add_4, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_10                                                     aten.group_norm.default                                           (div_4, 32, p_encoder_down_blocks_2_resnets_1_norm1_weight, p_encoder_down_blocks_2_resnets_1_norm1_bias, 1e-06)                                                {}\n",
      "call_function  relu_10                                                           aten.relu.default                                                 (group_norm_10,)                                                                                                                                                {}\n",
      "call_function  conv2d_15                                                         aten.conv2d.default                                               (relu_10, p_encoder_down_blocks_2_resnets_1_conv1_weight, p_encoder_down_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  group_norm_11                                                     aten.group_norm.default                                           (conv2d_15, 32, p_encoder_down_blocks_2_resnets_1_norm2_weight, p_encoder_down_blocks_2_resnets_1_norm2_bias, 1e-06)                                            {}\n",
      "call_function  relu_11                                                           aten.relu.default                                                 (group_norm_11,)                                                                                                                                                {}\n",
      "call_function  dropout_5                                                         aten.dropout.default                                              (relu_11, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_16                                                         aten.conv2d.default                                               (dropout_5, p_encoder_down_blocks_2_resnets_1_conv2_weight, p_encoder_down_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_5                                                             aten.add.Tensor                                                   (div_4, conv2d_16)                                                                                                                                              {}\n",
      "call_function  div_5                                                             aten.div.Tensor                                                   (add_5, 1.0)                                                                                                                                                    {}\n",
      "call_function  pad_2                                                             aten.pad.default                                                  (div_5, [0, 1, 0, 1], 'constant', 0.0)                                                                                                                          {}\n",
      "call_function  conv2d_17                                                         aten.conv2d.default                                               (pad_2, p_encoder_down_blocks_2_downsamplers_0_conv_weight, p_encoder_down_blocks_2_downsamplers_0_conv_bias, [2, 2])                                           {}\n",
      "call_function  group_norm_12                                                     aten.group_norm.default                                           (conv2d_17, 32, p_encoder_down_blocks_3_resnets_0_norm1_weight, p_encoder_down_blocks_3_resnets_0_norm1_bias, 1e-06)                                            {}\n",
      "call_function  relu_12                                                           aten.relu.default                                                 (group_norm_12,)                                                                                                                                                {}\n",
      "call_function  conv2d_18                                                         aten.conv2d.default                                               (relu_12, p_encoder_down_blocks_3_resnets_0_conv1_weight, p_encoder_down_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  group_norm_13                                                     aten.group_norm.default                                           (conv2d_18, 32, p_encoder_down_blocks_3_resnets_0_norm2_weight, p_encoder_down_blocks_3_resnets_0_norm2_bias, 1e-06)                                            {}\n",
      "call_function  relu_13                                                           aten.relu.default                                                 (group_norm_13,)                                                                                                                                                {}\n",
      "call_function  dropout_6                                                         aten.dropout.default                                              (relu_13, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_19                                                         aten.conv2d.default                                               (dropout_6, p_encoder_down_blocks_3_resnets_0_conv2_weight, p_encoder_down_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_6                                                             aten.add.Tensor                                                   (conv2d_17, conv2d_19)                                                                                                                                          {}\n",
      "call_function  div_6                                                             aten.div.Tensor                                                   (add_6, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_14                                                     aten.group_norm.default                                           (div_6, 32, p_encoder_down_blocks_3_resnets_1_norm1_weight, p_encoder_down_blocks_3_resnets_1_norm1_bias, 1e-06)                                                {}\n",
      "call_function  relu_14                                                           aten.relu.default                                                 (group_norm_14,)                                                                                                                                                {}\n",
      "call_function  conv2d_20                                                         aten.conv2d.default                                               (relu_14, p_encoder_down_blocks_3_resnets_1_conv1_weight, p_encoder_down_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1])                                         {}\n",
      "call_function  group_norm_15                                                     aten.group_norm.default                                           (conv2d_20, 32, p_encoder_down_blocks_3_resnets_1_norm2_weight, p_encoder_down_blocks_3_resnets_1_norm2_bias, 1e-06)                                            {}\n",
      "call_function  relu_15                                                           aten.relu.default                                                 (group_norm_15,)                                                                                                                                                {}\n",
      "call_function  dropout_7                                                         aten.dropout.default                                              (relu_15, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_21                                                         aten.conv2d.default                                               (dropout_7, p_encoder_down_blocks_3_resnets_1_conv2_weight, p_encoder_down_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1])                                       {}\n",
      "call_function  add_7                                                             aten.add.Tensor                                                   (div_6, conv2d_21)                                                                                                                                              {}\n",
      "call_function  div_7                                                             aten.div.Tensor                                                   (add_7, 1.0)                                                                                                                                                    {}\n",
      "call_function  group_norm_16                                                     aten.group_norm.default                                           (div_7, 32, p_encoder_mid_block_resnets_0_norm1_weight, p_encoder_mid_block_resnets_0_norm1_bias, 1e-06)                                                        {}\n",
      "call_function  relu_16                                                           aten.relu.default                                                 (group_norm_16,)                                                                                                                                                {}\n",
      "call_function  conv2d_22                                                         aten.conv2d.default                                               (relu_16, p_encoder_mid_block_resnets_0_conv1_weight, p_encoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1])                                                 {}\n",
      "call_function  group_norm_17                                                     aten.group_norm.default                                           (conv2d_22, 32, p_encoder_mid_block_resnets_0_norm2_weight, p_encoder_mid_block_resnets_0_norm2_bias, 1e-06)                                                    {}\n",
      "call_function  relu_17                                                           aten.relu.default                                                 (group_norm_17,)                                                                                                                                                {}\n",
      "call_function  dropout_8                                                         aten.dropout.default                                              (relu_17, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_23                                                         aten.conv2d.default                                               (dropout_8, p_encoder_mid_block_resnets_0_conv2_weight, p_encoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1])                                               {}\n",
      "call_function  add_8                                                             aten.add.Tensor                                                   (div_7, conv2d_23)                                                                                                                                              {}\n",
      "call_function  div_8                                                             aten.div.Tensor                                                   (add_8, 1)                                                                                                                                                      {}\n",
      "call_function  view                                                              aten.view.default                                                 (div_8, [1, 512, 4096])                                                                                                                                         {}\n",
      "call_function  transpose                                                         aten.transpose.int                                                (view, 1, 2)                                                                                                                                                    {}\n",
      "call_function  transpose_1                                                       aten.transpose.int                                                (transpose, 1, 2)                                                                                                                                               {}\n",
      "call_function  group_norm_18                                                     aten.group_norm.default                                           (transpose_1, 32, p_encoder_mid_block_attentions_0_group_norm_weight, p_encoder_mid_block_attentions_0_group_norm_bias, 1e-06)                                  {}\n",
      "call_function  transpose_2                                                       aten.transpose.int                                                (group_norm_18, 1, 2)                                                                                                                                           {}\n",
      "call_function  linear                                                            aten.linear.default                                               (transpose_2, p_encoder_mid_block_attentions_0_to_q_weight, p_encoder_mid_block_attentions_0_to_q_bias)                                                         {}\n",
      "call_function  linear_1                                                          aten.linear.default                                               (transpose_2, p_encoder_mid_block_attentions_0_to_k_weight, p_encoder_mid_block_attentions_0_to_k_bias)                                                         {}\n",
      "call_function  linear_2                                                          aten.linear.default                                               (transpose_2, p_encoder_mid_block_attentions_0_to_v_weight, p_encoder_mid_block_attentions_0_to_v_bias)                                                         {}\n",
      "call_function  view_1                                                            aten.view.default                                                 (linear, [1, -1, 1, 512])                                                                                                                                       {}\n",
      "call_function  transpose_3                                                       aten.transpose.int                                                (view_1, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_2                                                            aten.view.default                                                 (linear_1, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_4                                                       aten.transpose.int                                                (view_2, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_3                                                            aten.view.default                                                 (linear_2, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_5                                                       aten.transpose.int                                                (view_3, 1, 2)                                                                                                                                                  {}\n",
      "call_function  scaled_dot_product_attention                                      aten.scaled_dot_product_attention.default                         (transpose_3, transpose_4, transpose_5)                                                                                                                         {}\n",
      "call_function  transpose_6                                                       aten.transpose.int                                                (scaled_dot_product_attention, 1, 2)                                                                                                                            {}\n",
      "call_function  view_4                                                            aten.view.default                                                 (transpose_6, [1, -1, 512])                                                                                                                                     {}\n",
      "call_function  _to_copy                                                          aten._to_copy.default                                             (view_4,)                                                                                                                                                       {'dtype': torch.float32}\n",
      "call_function  linear_3                                                          aten.linear.default                                               (_to_copy, p_encoder_mid_block_attentions_0_to_out_0_weight, p_encoder_mid_block_attentions_0_to_out_0_bias)                                                    {}\n",
      "call_function  dropout_9                                                         aten.dropout.default                                              (linear_3, 0.0, False)                                                                                                                                          {}\n",
      "call_function  transpose_7                                                       aten.transpose.int                                                (dropout_9, -1, -2)                                                                                                                                             {}\n",
      "call_function  view_5                                                            aten.view.default                                                 (transpose_7, [1, 512, 64, 64])                                                                                                                                 {}\n",
      "call_function  add_9                                                             aten.add.Tensor                                                   (view_5, div_8)                                                                                                                                                 {}\n",
      "call_function  div_9                                                             aten.div.Tensor                                                   (add_9, 1)                                                                                                                                                      {}\n",
      "call_function  group_norm_19                                                     aten.group_norm.default                                           (div_9, 32, p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_norm1_bias, 1e-06)            {}\n",
      "call_function  relu_18                                                           aten.relu.default                                                 (group_norm_19,)                                                                                                                                                {}\n",
      "call_function  conv2d_24                                                         aten.conv2d.default                                               (relu_18, p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1])     {}\n",
      "call_function  group_norm_20                                                     aten.group_norm.default                                           (conv2d_24, 32, p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_norm2_bias, 1e-06)        {}\n",
      "call_function  relu_19                                                           aten.relu.default                                                 (group_norm_20,)                                                                                                                                                {}\n",
      "call_function  dropout_10                                                        aten.dropout.default                                              (relu_19, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_25                                                         aten.conv2d.default                                               (dropout_10, p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_weight, p_encoder_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1])  {}\n",
      "call_function  add_10                                                            aten.add.Tensor                                                   (div_9, conv2d_25)                                                                                                                                              {}\n",
      "call_function  div_10                                                            aten.div.Tensor                                                   (add_10, 1)                                                                                                                                                     {}\n",
      "call_function  group_norm_21                                                     aten.group_norm.default                                           (div_10, 32, p_encoder_conv_norm_out_weight, p_encoder_conv_norm_out_bias, 1e-06)                                                                               {}\n",
      "call_function  relu_20                                                           aten.relu.default                                                 (group_norm_21,)                                                                                                                                                {}\n",
      "call_function  conv2d_26                                                         aten.conv2d.default                                               (relu_20, p_encoder_conv_out_weight, p_encoder_conv_out_bias, [1, 1], [1, 1])                                                                                   {}\n",
      "call_function  conv2d_27                                                         aten.conv2d.default                                               (conv2d_26, p_quant_conv_weight, p_quant_conv_bias)                                                                                                             {}\n",
      "call_function  split                                                             aten.split.Tensor                                                 (conv2d_27, 4, 1)                                                                                                                                               {}\n",
      "call_function  getitem                                                           <built-in function getitem>                                       (split, 0)                                                                                                                                                      {}\n",
      "call_function  conv2d_28                                                         aten.conv2d.default                                               (getitem, p_post_quant_conv_weight, p_post_quant_conv_bias)                                                                                                     {}\n",
      "call_function  conv2d_29                                                         aten.conv2d.default                                               (conv2d_28, p_decoder_conv_in_weight, p_decoder_conv_in_bias, [1, 1], [1, 1])                                                                                   {}\n",
      "call_function  group_norm_22                                                     aten.group_norm.default                                           (conv2d_29, 32, p_decoder_mid_block_resnets_0_norm1_weight, p_decoder_mid_block_resnets_0_norm1_bias, 1e-06)                                                    {}\n",
      "call_function  relu_21                                                           aten.relu.default                                                 (group_norm_22,)                                                                                                                                                {}\n",
      "call_function  conv2d_30                                                         aten.conv2d.default                                               (relu_21, p_decoder_mid_block_resnets_0_conv1_weight, p_decoder_mid_block_resnets_0_conv1_bias, [1, 1], [1, 1])                                                 {}\n",
      "call_function  group_norm_23                                                     aten.group_norm.default                                           (conv2d_30, 32, p_decoder_mid_block_resnets_0_norm2_weight, p_decoder_mid_block_resnets_0_norm2_bias, 1e-06)                                                    {}\n",
      "call_function  relu_22                                                           aten.relu.default                                                 (group_norm_23,)                                                                                                                                                {}\n",
      "call_function  dropout_11                                                        aten.dropout.default                                              (relu_22, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_31                                                         aten.conv2d.default                                               (dropout_11, p_decoder_mid_block_resnets_0_conv2_weight, p_decoder_mid_block_resnets_0_conv2_bias, [1, 1], [1, 1])                                              {}\n",
      "call_function  add_11                                                            aten.add.Tensor                                                   (conv2d_29, conv2d_31)                                                                                                                                          {}\n",
      "call_function  div_11                                                            aten.div.Tensor                                                   (add_11, 1)                                                                                                                                                     {}\n",
      "call_function  view_6                                                            aten.view.default                                                 (div_11, [1, 512, 4096])                                                                                                                                        {}\n",
      "call_function  transpose_8                                                       aten.transpose.int                                                (view_6, 1, 2)                                                                                                                                                  {}\n",
      "call_function  transpose_9                                                       aten.transpose.int                                                (transpose_8, 1, 2)                                                                                                                                             {}\n",
      "call_function  group_norm_24                                                     aten.group_norm.default                                           (transpose_9, 32, p_decoder_mid_block_attentions_0_group_norm_weight, p_decoder_mid_block_attentions_0_group_norm_bias, 1e-06)                                  {}\n",
      "call_function  transpose_10                                                      aten.transpose.int                                                (group_norm_24, 1, 2)                                                                                                                                           {}\n",
      "call_function  linear_4                                                          aten.linear.default                                               (transpose_10, p_decoder_mid_block_attentions_0_to_q_weight, p_decoder_mid_block_attentions_0_to_q_bias)                                                        {}\n",
      "call_function  linear_5                                                          aten.linear.default                                               (transpose_10, p_decoder_mid_block_attentions_0_to_k_weight, p_decoder_mid_block_attentions_0_to_k_bias)                                                        {}\n",
      "call_function  linear_6                                                          aten.linear.default                                               (transpose_10, p_decoder_mid_block_attentions_0_to_v_weight, p_decoder_mid_block_attentions_0_to_v_bias)                                                        {}\n",
      "call_function  view_7                                                            aten.view.default                                                 (linear_4, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_11                                                      aten.transpose.int                                                (view_7, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_8                                                            aten.view.default                                                 (linear_5, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_12                                                      aten.transpose.int                                                (view_8, 1, 2)                                                                                                                                                  {}\n",
      "call_function  view_9                                                            aten.view.default                                                 (linear_6, [1, -1, 1, 512])                                                                                                                                     {}\n",
      "call_function  transpose_13                                                      aten.transpose.int                                                (view_9, 1, 2)                                                                                                                                                  {}\n",
      "call_function  scaled_dot_product_attention_1                                    aten.scaled_dot_product_attention.default                         (transpose_11, transpose_12, transpose_13)                                                                                                                      {}\n",
      "call_function  transpose_14                                                      aten.transpose.int                                                (scaled_dot_product_attention_1, 1, 2)                                                                                                                          {}\n",
      "call_function  view_10                                                           aten.view.default                                                 (transpose_14, [1, -1, 512])                                                                                                                                    {}\n",
      "call_function  _to_copy_1                                                        aten._to_copy.default                                             (view_10,)                                                                                                                                                      {'dtype': torch.float32}\n",
      "call_function  linear_7                                                          aten.linear.default                                               (_to_copy_1, p_decoder_mid_block_attentions_0_to_out_0_weight, p_decoder_mid_block_attentions_0_to_out_0_bias)                                                  {}\n",
      "call_function  dropout_12                                                        aten.dropout.default                                              (linear_7, 0.0, False)                                                                                                                                          {}\n",
      "call_function  transpose_15                                                      aten.transpose.int                                                (dropout_12, -1, -2)                                                                                                                                            {}\n",
      "call_function  view_11                                                           aten.view.default                                                 (transpose_15, [1, 512, 64, 64])                                                                                                                                {}\n",
      "call_function  add_12                                                            aten.add.Tensor                                                   (view_11, div_11)                                                                                                                                               {}\n",
      "call_function  div_12                                                            aten.div.Tensor                                                   (add_12, 1)                                                                                                                                                     {}\n",
      "call_function  group_norm_25                                                     aten.group_norm.default                                           (div_12, 32, p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_norm1_bias, 1e-06)           {}\n",
      "call_function  relu_23                                                           aten.relu.default                                                 (group_norm_25,)                                                                                                                                                {}\n",
      "call_function  conv2d_32                                                         aten.conv2d.default                                               (relu_23, p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_conv1_bias, [1, 1], [1, 1])     {}\n",
      "call_function  group_norm_26                                                     aten.group_norm.default                                           (conv2d_32, 32, p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_norm2_bias, 1e-06)        {}\n",
      "call_function  relu_24                                                           aten.relu.default                                                 (group_norm_26,)                                                                                                                                                {}\n",
      "call_function  dropout_13                                                        aten.dropout.default                                              (relu_24, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_33                                                         aten.conv2d.default                                               (dropout_13, p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_weight, p_decoder_mid_block_resnets_slice_1__none__none___0_conv2_bias, [1, 1], [1, 1])  {}\n",
      "call_function  add_13                                                            aten.add.Tensor                                                   (div_12, conv2d_33)                                                                                                                                             {}\n",
      "call_function  div_13                                                            aten.div.Tensor                                                   (add_13, 1)                                                                                                                                                     {}\n",
      "call_function  _to_copy_2                                                        aten._to_copy.default                                             (div_13,)                                                                                                                                                       {'dtype': torch.float32}\n",
      "call_function  group_norm_27                                                     aten.group_norm.default                                           (_to_copy_2, 32, p_decoder_up_blocks_0_resnets_0_norm1_weight, p_decoder_up_blocks_0_resnets_0_norm1_bias, 1e-06)                                               {}\n",
      "call_function  relu_25                                                           aten.relu.default                                                 (group_norm_27,)                                                                                                                                                {}\n",
      "call_function  conv2d_34                                                         aten.conv2d.default                                               (relu_25, p_decoder_up_blocks_0_resnets_0_conv1_weight, p_decoder_up_blocks_0_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_28                                                     aten.group_norm.default                                           (conv2d_34, 32, p_decoder_up_blocks_0_resnets_0_norm2_weight, p_decoder_up_blocks_0_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  relu_26                                                           aten.relu.default                                                 (group_norm_28,)                                                                                                                                                {}\n",
      "call_function  dropout_14                                                        aten.dropout.default                                              (relu_26, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_35                                                         aten.conv2d.default                                               (dropout_14, p_decoder_up_blocks_0_resnets_0_conv2_weight, p_decoder_up_blocks_0_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_14                                                            aten.add.Tensor                                                   (_to_copy_2, conv2d_35)                                                                                                                                         {}\n",
      "call_function  div_14                                                            aten.div.Tensor                                                   (add_14, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_29                                                     aten.group_norm.default                                           (div_14, 32, p_decoder_up_blocks_0_resnets_1_norm1_weight, p_decoder_up_blocks_0_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  relu_27                                                           aten.relu.default                                                 (group_norm_29,)                                                                                                                                                {}\n",
      "call_function  conv2d_36                                                         aten.conv2d.default                                               (relu_27, p_decoder_up_blocks_0_resnets_1_conv1_weight, p_decoder_up_blocks_0_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_30                                                     aten.group_norm.default                                           (conv2d_36, 32, p_decoder_up_blocks_0_resnets_1_norm2_weight, p_decoder_up_blocks_0_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  relu_28                                                           aten.relu.default                                                 (group_norm_30,)                                                                                                                                                {}\n",
      "call_function  dropout_15                                                        aten.dropout.default                                              (relu_28, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_37                                                         aten.conv2d.default                                               (dropout_15, p_decoder_up_blocks_0_resnets_1_conv2_weight, p_decoder_up_blocks_0_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_15                                                            aten.add.Tensor                                                   (div_14, conv2d_37)                                                                                                                                             {}\n",
      "call_function  div_15                                                            aten.div.Tensor                                                   (add_15, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_31                                                     aten.group_norm.default                                           (div_15, 32, p_decoder_up_blocks_0_resnets_2_norm1_weight, p_decoder_up_blocks_0_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  relu_29                                                           aten.relu.default                                                 (group_norm_31,)                                                                                                                                                {}\n",
      "call_function  conv2d_38                                                         aten.conv2d.default                                               (relu_29, p_decoder_up_blocks_0_resnets_2_conv1_weight, p_decoder_up_blocks_0_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_32                                                     aten.group_norm.default                                           (conv2d_38, 32, p_decoder_up_blocks_0_resnets_2_norm2_weight, p_decoder_up_blocks_0_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  relu_30                                                           aten.relu.default                                                 (group_norm_32,)                                                                                                                                                {}\n",
      "call_function  dropout_16                                                        aten.dropout.default                                              (relu_30, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_39                                                         aten.conv2d.default                                               (dropout_16, p_decoder_up_blocks_0_resnets_2_conv2_weight, p_decoder_up_blocks_0_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_16                                                            aten.add.Tensor                                                   (div_15, conv2d_39)                                                                                                                                             {}\n",
      "call_function  div_16                                                            aten.div.Tensor                                                   (add_16, 1.0)                                                                                                                                                   {}\n",
      "call_function  upsample_nearest2d                                                aten.upsample_nearest2d.vec                                       (div_16, None, [2.0, 2.0])                                                                                                                                      {}\n",
      "call_function  conv2d_40                                                         aten.conv2d.default                                               (upsample_nearest2d, p_decoder_up_blocks_0_upsamplers_0_conv_weight, p_decoder_up_blocks_0_upsamplers_0_conv_bias, [1, 1], [1, 1])                              {}\n",
      "call_function  group_norm_33                                                     aten.group_norm.default                                           (conv2d_40, 32, p_decoder_up_blocks_1_resnets_0_norm1_weight, p_decoder_up_blocks_1_resnets_0_norm1_bias, 1e-06)                                                {}\n",
      "call_function  relu_31                                                           aten.relu.default                                                 (group_norm_33,)                                                                                                                                                {}\n",
      "call_function  conv2d_41                                                         aten.conv2d.default                                               (relu_31, p_decoder_up_blocks_1_resnets_0_conv1_weight, p_decoder_up_blocks_1_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_34                                                     aten.group_norm.default                                           (conv2d_41, 32, p_decoder_up_blocks_1_resnets_0_norm2_weight, p_decoder_up_blocks_1_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  relu_32                                                           aten.relu.default                                                 (group_norm_34,)                                                                                                                                                {}\n",
      "call_function  dropout_17                                                        aten.dropout.default                                              (relu_32, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_42                                                         aten.conv2d.default                                               (dropout_17, p_decoder_up_blocks_1_resnets_0_conv2_weight, p_decoder_up_blocks_1_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_17                                                            aten.add.Tensor                                                   (conv2d_40, conv2d_42)                                                                                                                                          {}\n",
      "call_function  div_17                                                            aten.div.Tensor                                                   (add_17, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_35                                                     aten.group_norm.default                                           (div_17, 32, p_decoder_up_blocks_1_resnets_1_norm1_weight, p_decoder_up_blocks_1_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  relu_33                                                           aten.relu.default                                                 (group_norm_35,)                                                                                                                                                {}\n",
      "call_function  conv2d_43                                                         aten.conv2d.default                                               (relu_33, p_decoder_up_blocks_1_resnets_1_conv1_weight, p_decoder_up_blocks_1_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_36                                                     aten.group_norm.default                                           (conv2d_43, 32, p_decoder_up_blocks_1_resnets_1_norm2_weight, p_decoder_up_blocks_1_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  relu_34                                                           aten.relu.default                                                 (group_norm_36,)                                                                                                                                                {}\n",
      "call_function  dropout_18                                                        aten.dropout.default                                              (relu_34, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_44                                                         aten.conv2d.default                                               (dropout_18, p_decoder_up_blocks_1_resnets_1_conv2_weight, p_decoder_up_blocks_1_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_18                                                            aten.add.Tensor                                                   (div_17, conv2d_44)                                                                                                                                             {}\n",
      "call_function  div_18                                                            aten.div.Tensor                                                   (add_18, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_37                                                     aten.group_norm.default                                           (div_18, 32, p_decoder_up_blocks_1_resnets_2_norm1_weight, p_decoder_up_blocks_1_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  relu_35                                                           aten.relu.default                                                 (group_norm_37,)                                                                                                                                                {}\n",
      "call_function  conv2d_45                                                         aten.conv2d.default                                               (relu_35, p_decoder_up_blocks_1_resnets_2_conv1_weight, p_decoder_up_blocks_1_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_38                                                     aten.group_norm.default                                           (conv2d_45, 32, p_decoder_up_blocks_1_resnets_2_norm2_weight, p_decoder_up_blocks_1_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  relu_36                                                           aten.relu.default                                                 (group_norm_38,)                                                                                                                                                {}\n",
      "call_function  dropout_19                                                        aten.dropout.default                                              (relu_36, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_46                                                         aten.conv2d.default                                               (dropout_19, p_decoder_up_blocks_1_resnets_2_conv2_weight, p_decoder_up_blocks_1_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_19                                                            aten.add.Tensor                                                   (div_18, conv2d_46)                                                                                                                                             {}\n",
      "call_function  div_19                                                            aten.div.Tensor                                                   (add_19, 1.0)                                                                                                                                                   {}\n",
      "call_function  upsample_nearest2d_1                                              aten.upsample_nearest2d.vec                                       (div_19, None, [2.0, 2.0])                                                                                                                                      {}\n",
      "call_function  conv2d_47                                                         aten.conv2d.default                                               (upsample_nearest2d_1, p_decoder_up_blocks_1_upsamplers_0_conv_weight, p_decoder_up_blocks_1_upsamplers_0_conv_bias, [1, 1], [1, 1])                            {}\n",
      "call_function  group_norm_39                                                     aten.group_norm.default                                           (conv2d_47, 32, p_decoder_up_blocks_2_resnets_0_norm1_weight, p_decoder_up_blocks_2_resnets_0_norm1_bias, 1e-06)                                                {}\n",
      "call_function  relu_37                                                           aten.relu.default                                                 (group_norm_39,)                                                                                                                                                {}\n",
      "call_function  conv2d_48                                                         aten.conv2d.default                                               (relu_37, p_decoder_up_blocks_2_resnets_0_conv1_weight, p_decoder_up_blocks_2_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_40                                                     aten.group_norm.default                                           (conv2d_48, 32, p_decoder_up_blocks_2_resnets_0_norm2_weight, p_decoder_up_blocks_2_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  relu_38                                                           aten.relu.default                                                 (group_norm_40,)                                                                                                                                                {}\n",
      "call_function  dropout_20                                                        aten.dropout.default                                              (relu_38, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_49                                                         aten.conv2d.default                                               (dropout_20, p_decoder_up_blocks_2_resnets_0_conv2_weight, p_decoder_up_blocks_2_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  clone                                                             aten.clone.default                                                (conv2d_47,)                                                                                                                                                    {'memory_format': torch.contiguous_format}\n",
      "call_function  conv2d_50                                                         aten.conv2d.default                                               (clone, p_decoder_up_blocks_2_resnets_0_conv_shortcut_weight, p_decoder_up_blocks_2_resnets_0_conv_shortcut_bias)                                               {}\n",
      "call_function  add_20                                                            aten.add.Tensor                                                   (conv2d_50, conv2d_49)                                                                                                                                          {}\n",
      "call_function  div_20                                                            aten.div.Tensor                                                   (add_20, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_41                                                     aten.group_norm.default                                           (div_20, 32, p_decoder_up_blocks_2_resnets_1_norm1_weight, p_decoder_up_blocks_2_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  relu_39                                                           aten.relu.default                                                 (group_norm_41,)                                                                                                                                                {}\n",
      "call_function  conv2d_51                                                         aten.conv2d.default                                               (relu_39, p_decoder_up_blocks_2_resnets_1_conv1_weight, p_decoder_up_blocks_2_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_42                                                     aten.group_norm.default                                           (conv2d_51, 32, p_decoder_up_blocks_2_resnets_1_norm2_weight, p_decoder_up_blocks_2_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  relu_40                                                           aten.relu.default                                                 (group_norm_42,)                                                                                                                                                {}\n",
      "call_function  dropout_21                                                        aten.dropout.default                                              (relu_40, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_52                                                         aten.conv2d.default                                               (dropout_21, p_decoder_up_blocks_2_resnets_1_conv2_weight, p_decoder_up_blocks_2_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_21                                                            aten.add.Tensor                                                   (div_20, conv2d_52)                                                                                                                                             {}\n",
      "call_function  div_21                                                            aten.div.Tensor                                                   (add_21, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_43                                                     aten.group_norm.default                                           (div_21, 32, p_decoder_up_blocks_2_resnets_2_norm1_weight, p_decoder_up_blocks_2_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  relu_41                                                           aten.relu.default                                                 (group_norm_43,)                                                                                                                                                {}\n",
      "call_function  conv2d_53                                                         aten.conv2d.default                                               (relu_41, p_decoder_up_blocks_2_resnets_2_conv1_weight, p_decoder_up_blocks_2_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_44                                                     aten.group_norm.default                                           (conv2d_53, 32, p_decoder_up_blocks_2_resnets_2_norm2_weight, p_decoder_up_blocks_2_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  relu_42                                                           aten.relu.default                                                 (group_norm_44,)                                                                                                                                                {}\n",
      "call_function  dropout_22                                                        aten.dropout.default                                              (relu_42, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_54                                                         aten.conv2d.default                                               (dropout_22, p_decoder_up_blocks_2_resnets_2_conv2_weight, p_decoder_up_blocks_2_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_22                                                            aten.add.Tensor                                                   (div_21, conv2d_54)                                                                                                                                             {}\n",
      "call_function  div_22                                                            aten.div.Tensor                                                   (add_22, 1.0)                                                                                                                                                   {}\n",
      "call_function  upsample_nearest2d_2                                              aten.upsample_nearest2d.vec                                       (div_22, None, [2.0, 2.0])                                                                                                                                      {}\n",
      "call_function  conv2d_55                                                         aten.conv2d.default                                               (upsample_nearest2d_2, p_decoder_up_blocks_2_upsamplers_0_conv_weight, p_decoder_up_blocks_2_upsamplers_0_conv_bias, [1, 1], [1, 1])                            {}\n",
      "call_function  group_norm_45                                                     aten.group_norm.default                                           (conv2d_55, 32, p_decoder_up_blocks_3_resnets_0_norm1_weight, p_decoder_up_blocks_3_resnets_0_norm1_bias, 1e-06)                                                {}\n",
      "call_function  relu_43                                                           aten.relu.default                                                 (group_norm_45,)                                                                                                                                                {}\n",
      "call_function  conv2d_56                                                         aten.conv2d.default                                               (relu_43, p_decoder_up_blocks_3_resnets_0_conv1_weight, p_decoder_up_blocks_3_resnets_0_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_46                                                     aten.group_norm.default                                           (conv2d_56, 32, p_decoder_up_blocks_3_resnets_0_norm2_weight, p_decoder_up_blocks_3_resnets_0_norm2_bias, 1e-06)                                                {}\n",
      "call_function  relu_44                                                           aten.relu.default                                                 (group_norm_46,)                                                                                                                                                {}\n",
      "call_function  dropout_23                                                        aten.dropout.default                                              (relu_44, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_57                                                         aten.conv2d.default                                               (dropout_23, p_decoder_up_blocks_3_resnets_0_conv2_weight, p_decoder_up_blocks_3_resnets_0_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  conv2d_58                                                         aten.conv2d.default                                               (conv2d_55, p_decoder_up_blocks_3_resnets_0_conv_shortcut_weight, p_decoder_up_blocks_3_resnets_0_conv_shortcut_bias)                                           {}\n",
      "call_function  add_23                                                            aten.add.Tensor                                                   (conv2d_58, conv2d_57)                                                                                                                                          {}\n",
      "call_function  div_23                                                            aten.div.Tensor                                                   (add_23, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_47                                                     aten.group_norm.default                                           (div_23, 32, p_decoder_up_blocks_3_resnets_1_norm1_weight, p_decoder_up_blocks_3_resnets_1_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  relu_45                                                           aten.relu.default                                                 (group_norm_47,)                                                                                                                                                {}\n",
      "call_function  conv2d_59                                                         aten.conv2d.default                                               (relu_45, p_decoder_up_blocks_3_resnets_1_conv1_weight, p_decoder_up_blocks_3_resnets_1_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_48                                                     aten.group_norm.default                                           (conv2d_59, 32, p_decoder_up_blocks_3_resnets_1_norm2_weight, p_decoder_up_blocks_3_resnets_1_norm2_bias, 1e-06)                                                {}\n",
      "call_function  relu_46                                                           aten.relu.default                                                 (group_norm_48,)                                                                                                                                                {}\n",
      "call_function  dropout_24                                                        aten.dropout.default                                              (relu_46, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_60                                                         aten.conv2d.default                                               (dropout_24, p_decoder_up_blocks_3_resnets_1_conv2_weight, p_decoder_up_blocks_3_resnets_1_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_24                                                            aten.add.Tensor                                                   (div_23, conv2d_60)                                                                                                                                             {}\n",
      "call_function  div_24                                                            aten.div.Tensor                                                   (add_24, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_49                                                     aten.group_norm.default                                           (div_24, 32, p_decoder_up_blocks_3_resnets_2_norm1_weight, p_decoder_up_blocks_3_resnets_2_norm1_bias, 1e-06)                                                   {}\n",
      "call_function  relu_47                                                           aten.relu.default                                                 (group_norm_49,)                                                                                                                                                {}\n",
      "call_function  conv2d_61                                                         aten.conv2d.default                                               (relu_47, p_decoder_up_blocks_3_resnets_2_conv1_weight, p_decoder_up_blocks_3_resnets_2_conv1_bias, [1, 1], [1, 1])                                             {}\n",
      "call_function  group_norm_50                                                     aten.group_norm.default                                           (conv2d_61, 32, p_decoder_up_blocks_3_resnets_2_norm2_weight, p_decoder_up_blocks_3_resnets_2_norm2_bias, 1e-06)                                                {}\n",
      "call_function  relu_48                                                           aten.relu.default                                                 (group_norm_50,)                                                                                                                                                {}\n",
      "call_function  dropout_25                                                        aten.dropout.default                                              (relu_48, 0.0, False)                                                                                                                                           {}\n",
      "call_function  conv2d_62                                                         aten.conv2d.default                                               (dropout_25, p_decoder_up_blocks_3_resnets_2_conv2_weight, p_decoder_up_blocks_3_resnets_2_conv2_bias, [1, 1], [1, 1])                                          {}\n",
      "call_function  add_25                                                            aten.add.Tensor                                                   (div_24, conv2d_62)                                                                                                                                             {}\n",
      "call_function  div_25                                                            aten.div.Tensor                                                   (add_25, 1.0)                                                                                                                                                   {}\n",
      "call_function  group_norm_51                                                     aten.group_norm.default                                           (div_25, 32, p_decoder_conv_norm_out_weight, p_decoder_conv_norm_out_bias, 1e-06)                                                                               {}\n",
      "call_function  relu_49                                                           aten.relu.default                                                 (group_norm_51,)                                                                                                                                                {}\n",
      "call_function  conv2d_63                                                         aten.conv2d.default                                               (relu_49, p_decoder_conv_out_weight, p_decoder_conv_out_bias, [1, 1], [1, 1])                                                                                   {}\n",
      "output         output                                                            output                                                            ((conv2d_63,),)                                                                                                                                                 {}\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "from diffusers import AutoencoderKL\n",
    "from torch.export import export, save\n",
    "\n",
    "#monkey patch\n",
    "F.gelu = F.relu\n",
    "F.silu = F.relu \n",
    "\n",
    "CKPT = \"prs-eth/marigold-depth-v1-1\"\n",
    "vae  = AutoencoderKL.from_pretrained(CKPT, subfolder=\"vae\").cpu().eval()\n",
    "\n",
    "example_rgb = torch.randn(1, 3, 512, 512)       # typical input\n",
    "\n",
    "gm_vae = export(vae, (example_rgb,))            # sample_posterior = False\n",
    "save(gm_vae, \"vae_fp32.ep\")\n",
    "print(\"VAE exported   vae_fp32.ep\")\n",
    "gm_vae.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b6e208",
   "metadata": {},
   "source": [
    "### 2. Find unsupported ops like GELU and GroupNorm for swapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a750f519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupported ops in VAE: {'aten.upsample_nearest2d.vec', 'aten.clone.default', 'aten.split.Tensor', 'aten.dropout.default', 'aten.relu.default', 'aten.pad.default', 'aten.linear.default', 'aten.scaled_dot_product_attention.default', 'aten._to_copy.default', '<built-in function getitem>', 'aten.view.default', 'aten.add.Tensor', 'aten.div.Tensor', 'aten.group_norm.default', 'aten.transpose.int', 'aten.conv2d.default'}\n"
     ]
    }
   ],
   "source": [
    "unsupported_ops = {\n",
    "    str(n.target)\n",
    "    for n in gm_vae.graph.nodes\n",
    "    if n.op in (\"call_function\", \"call_module\") and \"quant\" not in str(n.target)\n",
    "}\n",
    "print(\"Unsupported ops in VAE:\", unsupported_ops)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2fa9cf",
   "metadata": {},
   "source": [
    "### 3. Patch the model ops and apply to VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1692d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "F.dropout = lambda x, p=0.0, train=False, inplace=False: x        # keep\n",
    "if hasattr(F, \"scaled_dot_product_attention\"):\n",
    "    def _fake_sdpa(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False):\n",
    "        w = (q @ k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n",
    "        return (w.softmax(-1) @ v)\n",
    "    F.scaled_dot_product_attention = _fake_sdpa\n",
    "\n",
    "_BAD2GOOD = {\n",
    "    nn.GELU:      lambda _: nn.ReLU(inplace=False),      #  NOT in-place\n",
    "    nn.SiLU:      lambda _: nn.ReLU(inplace=False),\n",
    "    nn.GroupNorm: lambda _: nn.Identity(),\n",
    "    #nn.GroupNorm: lambda m: nn.InstanceNorm2d(m.num_channels,\n",
    "                                          #eps=1e-3, affine=True),\n",
    "    nn.LayerNorm: lambda _: nn.Identity(),\n",
    "}\n",
    "_BAD_FUNCS = {F.gelu, F.silu}\n",
    "_GOOD_FUNC = lambda x: F.relu(x, inplace=False)         # functional, no mutate\n",
    "\n",
    "def patch_model_for_qat(m: nn.Module):\n",
    "    for name, child in list(m.named_children()):\n",
    "        for bad, make_good in _BAD2GOOD.items():\n",
    "            if isinstance(child, bad):\n",
    "                setattr(m, name, make_good(child))\n",
    "                child = getattr(m, name)\n",
    "                break\n",
    "        patch_model_for_qat(child)\n",
    "    for attr, val in vars(m).items():\n",
    "        if callable(val) and val in _BAD_FUNCS:\n",
    "            setattr(m, attr, _GOOD_FUNC)\n",
    "\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# _BAD2GOOD = {\n",
    "#     nn.GELU:      lambda _: nn.ReLU(inplace=True),\n",
    "#     nn.SiLU:      lambda _: nn.ReLU(inplace=True),\n",
    "#     nn.GroupNorm: lambda _: nn.Identity(),\n",
    "#     nn.LayerNorm: lambda _: nn.Identity(),\n",
    "# }\n",
    "# _BAD_FUNCS = {F.gelu, F.silu}\n",
    "# _GOOD_FUNC = F.relu\n",
    "\n",
    "# def patch_model_for_qat(module: nn.Module):\n",
    "#     for name, child in list(module.named_children()):\n",
    "#         for bad_cls, make_good in _BAD2GOOD.items():\n",
    "#             if isinstance(child, bad_cls):\n",
    "#                 setattr(module, name, make_good(child))\n",
    "#                 child = getattr(module, name)\n",
    "#                 break\n",
    "#         patch_model_for_qat(child)\n",
    "#     for attr, val in vars(module).items():\n",
    "#         if callable(val) and val in _BAD_FUNCS:\n",
    "#             setattr(module, attr, _GOOD_FUNC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2bd13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_model_for_qat(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8b3ddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer-type sweep is clean.\n",
      "Ops still present: set()\n"
     ]
    }
   ],
   "source": [
    "#quick sanity check\n",
    "bad_types = (nn.GELU, nn.SiLU, nn.GroupNorm, nn.LayerNorm)\n",
    "\n",
    "assert not any(isinstance(m, bad_types) for m in vae.modules()), \\\n",
    "        \"At least one forbidden layer slipped through!\"\n",
    "print(\"Layer-type sweep is clean.\")\n",
    "\n",
    "gm_patched = export(vae, (example_rgb,))\n",
    "left = {str(n.target) for n in gm_patched.graph.nodes\n",
    "        if any(k in str(n.target) for k in (\"gelu\",\"silu\",\"group_norm\",\"layer_norm\"))}\n",
    "print(\"Ops still present:\", left)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc27b8",
   "metadata": {},
   "source": [
    "### 4. Add quantization stubs, prepare the model, and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9afcfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradshaw/Marigold/venv/marigold/lib/python3.10/site-packages/torch/ao/quantization/observer.py:221: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights written   vae_qat_ready.pt\n"
     ]
    }
   ],
   "source": [
    "# for state_dict we must rebuild the quant graph at runtime\n",
    "import torch, copy, dill\n",
    "from torch.ao.quantization import get_default_qat_qconfig, prepare_qat\n",
    "\n",
    "torch.backends.quantized.engine = \"fbgemm\"\n",
    "\n",
    "# -- 4.1  weights-only checkpoint -------------------------------------------\n",
    "vae.qconfig = get_default_qat_qconfig(\"fbgemm\")   # attach default QAT cfg\n",
    "qat_vae = copy.deepcopy(vae)                      # deep-copy keeps orig safe\n",
    "qat_vae.train()                                   # observers need train()\n",
    "prepare_qat(qat_vae, inplace=True)                # inserts fake-quant/observers\n",
    "\n",
    "torch.save(qat_vae.state_dict(), \"vae_qat_ready.pt\")\n",
    "print(\"weights written   vae_qat_ready.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fff9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full QAT module saved   vae_qat_dev.pth  (via dill)\n"
     ]
    }
   ],
   "source": [
    "# for full model using dill, includes enitre graph\n",
    "from torch.ao.quantization import get_default_qat_qconfig, prepare_qat\n",
    "import torch, copy, dill\n",
    "\n",
    "torch.backends.quantized.engine = \"fbgemm\" \n",
    "# Set QAT config and prepare\n",
    "vae.qconfig = get_default_qat_qconfig(\"fbgemm\")   # attach default QAT cfg\n",
    "qat_vae = copy.deepcopy(vae)                      # deep-copy keeps orig safe\n",
    "qat_vae.train()                                   # observers need train()\n",
    "prepare_qat(qat_vae, inplace=True)                # inserts fake-quant/observers\n",
    "\n",
    "torch.save(\n",
    "    qat_vae,\n",
    "    \"vae_qat_dev.pth\",\n",
    "    pickle_module=dill,\n",
    "    pickle_protocol=dill.HIGHEST_PROTOCOL,\n",
    ")\n",
    "print(\"Full QAT module saved   vae_qat_dev.pth  (via dill)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01167a95",
   "metadata": {},
   "source": [
    "### 5. Sanity check  Load and run dummy input through QAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd4d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132739/2468874237.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"vae_qat_ready.pt\", map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'diffusers.models.autoencoders.vae.DecoderOutput'>\n",
      "odict_keys(['sample'])\n",
      "reconstruction shape: torch.Size([1, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.ao.quantization import get_default_qat_qconfig, prepare_qat\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "# ---- 5.1  Re-instantiate *vanilla* VAE -------------------------------------\n",
    "qat_vae = AutoencoderKL.from_pretrained(CKPT, subfolder=\"vae\").cpu()\n",
    "\n",
    "patch_model_for_qat(qat_vae)                      # <-- your sanitizer\n",
    "torch.backends.quantized.engine = \"fbgemm\"\n",
    "qat_vae.qconfig = get_default_qat_qconfig(\"fbgemm\")\n",
    "qat_vae.train()\n",
    "prepare_qat(qat_vae, inplace=True)                # add observers\n",
    "qat_vae.eval()                                    # switch to eval for test\n",
    "\n",
    "# ---- 5.2  Load weights-only checkpoint -------------------------------------\n",
    "state_dict = torch.load(\"vae_qat_ready.pt\", map_location=\"cpu\")\n",
    "qat_vae.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_dummy = torch.randn(1, 3, 512, 512)\n",
    "    # full round-trip  returns AutoencoderKLOutput\n",
    "    out = qat_vae(x_dummy)\n",
    "\n",
    "print(type(out))                 # <class 'diffusers.utils.outputs.DecoderOutput'>\n",
    "print(out.keys())                # ('sample',)\n",
    "print(\"reconstruction shape:\", out.sample.shape)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     x_dummy  = torch.randn(1, 3, 512, 512)\n",
    "#     recon    = qat_vae(x_dummy)                   # encodedecode happens\n",
    "# print(\"State-dict QAT VAE  output shape:\", recon.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8981e3",
   "metadata": {},
   "source": [
    "## sandbox\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "947d0aca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at ../aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at ../aten/src/ATen/native/quantized/cpu/qlinear.cpp:1317 [kernel]\nQuantizedCUDA: registered at ../aten/src/ATen/native/quantized/cudnn/Linear.cpp:359 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:297 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:351 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# 7) Dummy inference still works now that weve patched get_time_embed\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 63\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mint8_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m77\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dummy INT8 inference OK  output shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, out\u001b[38;5;241m.\u001b[39msample\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# 8) Save both checkpoints\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m, in \u001b[0;36mQuantWrapper.forward\u001b[0;34m(self, latent, timestep, cond)\u001b[0m\n\u001b[1;32m     30\u001b[0m lq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_lat(latent)\n\u001b[1;32m     31\u001b[0m cq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_cond(cond)\n\u001b[0;32m---> 32\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m out\u001b[38;5;241m.\u001b[39msample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequant(out\u001b[38;5;241m.\u001b[39msample)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1141\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# 1. time\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m t_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_time_embed(sample\u001b[38;5;241m=\u001b[39msample, timestep\u001b[38;5;241m=\u001b[39mtimestep)\n\u001b[0;32m-> 1141\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m class_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_class_embed(sample\u001b[38;5;241m=\u001b[39msample, class_labels\u001b[38;5;241m=\u001b[39mclass_labels)\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_emb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/diffusers/models/embeddings.py:1308\u001b[0m, in \u001b[0;36mTimestepEmbedding.forward\u001b[0;34m(self, sample, condition)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     sample \u001b[38;5;241m=\u001b[39m sample \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_proj(condition)\n\u001b[0;32m-> 1308\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1311\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(sample)\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/linear.py:168\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_point\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Marigold/venv/marigold/lib/python3.10/site-packages/torch/_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[0;32m-> 1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at ../aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at ../aten/src/ATen/native/quantized/cpu/qlinear.cpp:1317 [kernel]\nQuantizedCUDA: registered at ../aten/src/ATen/native/quantized/cudnn/Linear.cpp:359 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:297 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:351 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "# ----------------- PTQ Convert + Dummy Inference -----------------\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.ao.quantization import (\n",
    "    QuantStub, DeQuantStub,\n",
    "    get_default_qat_qconfig, prepare_qat, convert,\n",
    "    disable_observer\n",
    ")\n",
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "# 1) Load & patch FP32 UNet (GELUReLU, GNBN, etc)\n",
    "CKPT = \"prs-eth/marigold-depth-v1-1\"\n",
    "fp32 = UNet2DConditionModel.from_pretrained(CKPT, subfolder=\"unet\").cpu()\n",
    "patch_model_for_qat(fp32)\n",
    "\n",
    "# 1b) Patch out the dtype-cast so quantized tensors dont break\n",
    "def _patched_time_embed(self, sample, timestep):\n",
    "    return self.time_proj(timestep)   # drop `.to(dtype=sample.dtype)`\n",
    "fp32.get_time_embed = _patched_time_embed.__get__(fp32, UNet2DConditionModel)\n",
    "\n",
    "# 2) Wrap with QuantStub/DeQuantStub so we can feed pure float32\n",
    "class QuantWrapper(nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.quant_lat  = QuantStub()\n",
    "        self.quant_cond = QuantStub()\n",
    "        self.unet       = unet\n",
    "        self.dequant    = DeQuantStub()\n",
    "    def forward(self, latent, timestep, cond):\n",
    "        lq = self.quant_lat(latent)\n",
    "        cq = self.quant_cond(cond)\n",
    "        out = self.unet(lq, timestep, cq)\n",
    "        out.sample = self.dequant(out.sample)\n",
    "        return out\n",
    "\n",
    "model = QuantWrapper(fp32)\n",
    "\n",
    "# 3) Insert observers/fake-quant\n",
    "model.qconfig = get_default_qat_qconfig(\"fbgemm\")\n",
    "prepare_qat(model, inplace=True)\n",
    "\n",
    "# 4) Calibration pass (dummy data just to fill min/max)\n",
    "model.train()\n",
    "for _ in range(5):\n",
    "    _ = model(\n",
    "        torch.randn(1, 8, 64, 64),\n",
    "        torch.tensor([0]),\n",
    "        torch.randn(1, 77, 1024),\n",
    "    )\n",
    "\n",
    "# 5) Freeze observers & BN, switch to eval\n",
    "disable_observer(model)\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.eval()\n",
    "model.eval()\n",
    "\n",
    "# 6) Convert to real INT8\n",
    "int8_model = convert(model, inplace=False)\n",
    "\n",
    "# 7) Dummy inference still works now that weve patched get_time_embed\n",
    "with torch.no_grad():\n",
    "    out = int8_model(\n",
    "        torch.randn(1, 8, 64, 64),\n",
    "        torch.tensor([0]),\n",
    "        torch.randn(1, 77, 1024),\n",
    "    )\n",
    "print(\" dummy INT8 inference OK  output shape:\", out.sample.shape)\n",
    "\n",
    "# 8) Save both checkpoints\n",
    "torch.save(model.state_dict(),    \"unet_ptq_fakequant.pt\")\n",
    "torch.save(int8_model.state_dict(), \"unet_ptq_int8.pt\")\n",
    "print(\" Saved unet_ptq_fakequant.pt and unet_ptq_int8.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " observers populated  ready to convert\n",
      " convert() succeeded  INT-8 weights produced\n",
      " wrote  unet_qat_fakequant.pt  and  unet_qat_int8.pt\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#  Minimal convert-only sanity cell (no extra wrappers, no INT-8\n",
    "#  forward test).  Converts and saves without crashing.\n",
    "# ================================================================\n",
    "import torch, copy\n",
    "from torch.ao.quantization import get_default_qat_qconfig, prepare_qat, convert\n",
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "torch.backends.quantized.engine = \"fbgemm\"          # static-INT8 backend\n",
    "CKPT = \"prs-eth/marigold-depth-v1-1\"\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 0)  fresh, already-patched UNet (you wrote patch_model_for_qat)\n",
    "# ----------------------------------------------------------------\n",
    "fp32 = UNet2DConditionModel.from_pretrained(CKPT, subfolder=\"unet\").cpu()\n",
    "patch_model_for_qat(fp32)                           # GELUReLU, GNBN \n",
    "fp32.qconfig = get_default_qat_qconfig(\"fbgemm\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 1)  insert observers / fake-quant layers\n",
    "# ----------------------------------------------------------------\n",
    "qat = copy.deepcopy(fp32).train()\n",
    "prepare_qat(qat, inplace=True)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2)  *one* calibration forward  fills observer stats\n",
    "# ----------------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    latent = torch.randn(1, 8, 64, 64)\n",
    "    tstep  = torch.tensor([0])\n",
    "    cond   = torch.randn(1, 77, 1024)\n",
    "    _      = qat(latent, tstep, cond)               # fake-quant graph runs\n",
    "\n",
    "print(\" observers populated  ready to convert\")\n",
    "\n",
    "qat.eval()  # convert() requires eval mode\n",
    "\n",
    "# ----------------------\n",
    "# ------------------------------------------\n",
    "# 3)  bake INT-8 kernels\n",
    "# ----------------------------------------------------------------\n",
    "int8_net = convert(qat, inplace=False)              # success == sanitised\n",
    "\n",
    "print(\" convert() succeeded  INT-8 weights produced\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 4)  save both checkpoints\n",
    "# ----------------------------------------------------------------\n",
    "torch.save(qat.state_dict(),  \"unet_qat_fakequant.pt\")  # resume-training point\n",
    "torch.save(int8_net.state_dict(), \"unet_qat_int8.pt\")   # deploy-time weights\n",
    "print(\" wrote  unet_qat_fakequant.pt  and  unet_qat_int8.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08df87c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Quantized linear op works\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.quantized as nnq\n",
    "\n",
    "# Should NOT raise an error\n",
    "m = nnq.Linear(4, 4)\n",
    "x = torch.quantize_per_tensor(torch.randn(1, 4), scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "out = m(x)\n",
    "print(\" Quantized linear op works\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marigold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
